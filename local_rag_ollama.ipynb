{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Offline RAG with Ollama\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates building a **completely offline RAG (Retrieval-Augmented Generation)** system using **Ollama** for local LLMs and embeddings.\n",
    "\n",
    "### üöÄ Benefits of Local RAG:\n",
    "- **100% Offline**: No internet required after setup\n",
    "- **Privacy First**: Your documents never leave your machine\n",
    "- **No API Costs**: Free to run unlimited queries\n",
    "- **Fast**: No network latency\n",
    "- **Full Control**: Customize models and parameters\n",
    "\n",
    "### üìã Architecture:\n",
    "```\n",
    "PDF Documents ‚Üí Load ‚Üí Split ‚Üí Local Embeddings (Ollama) ‚Üí ChromaDB\n",
    "                                                                  ‚Üì\n",
    "User Query ‚Üí Retrieve Similar Chunks ‚Üí Local LLM (Ollama) ‚Üí Answer\n",
    "```\n",
    "\n",
    "### üõ†Ô∏è Components:\n",
    "- **Document Loader**: PyPDFLoader\n",
    "- **Text Splitter**: RecursiveCharacterTextSplitter\n",
    "- **Embeddings**: Ollama with nomic-embed-text (or embeddinggemma)\n",
    "- **Vector Store**: ChromaDB (persistent, local)\n",
    "- **LLM**: Ollama with gemma3:1b\n",
    "- **Chain**: LangChain Expression Language (LCEL)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prerequisites & Installation\n",
    "\n",
    "### Required Software:\n",
    "1. **Ollama**: Download from https://ollama.ai\n",
    "2. **Python 3.9+**: Recommended 3.11 or 3.13\n",
    "\n",
    "### Install Python Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this once)\n",
    "# !pip install langchain langchain-core langchain-community langchain-text-splitters\n",
    "# !pip install langchain-ollama langchain-chroma chromadb\n",
    "# !pip install pypdf tiktoken\n",
    "\n",
    "# Or install from requirements.txt with additional packages:\n",
    "# !pip install -r requirements.txt\n",
    "# !pip install langchain-ollama langchain-chroma chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Ollama Models:\n",
    "\n",
    "Run these commands in your terminal (if you haven't already):\n",
    "\n",
    "```bash\n",
    "# Embedding model (choose one or both)\n",
    "ollama pull nomic-embed-text    # Recommended: 274 MB\n",
    "ollama pull embeddinggemma      # Alternative: 621 MB\n",
    "\n",
    "# LLM for generation\n",
    "ollama pull gemma3:1b          # Small & fast: 815 MB\n",
    "```\n",
    "\n",
    "**Note**: You already have these models downloaded! ‚úì"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All imports successful!\n",
      "‚úì Ready for local offline RAG!\n",
      "\n",
      "Python version: 3.13.2 (main, Mar 17 2025, 21:26:38) [Clang 20.1.0 ]\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# LangChain Document Loaders\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# LangChain Text Splitters\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Ollama Integration\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "\n",
    "# ChromaDB Vector Store\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# LangChain Core Components\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "print(\"‚úì All imports successful!\")\n",
    "print(\"‚úì Ready for local offline RAG!\")\n",
    "print(f\"\\nPython version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Ollama Installation\n",
    "\n",
    "Let's check that Ollama is running and our models are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       ID              SIZE      MODIFIED       \n",
      "gemma3:1b                  8648f39daa8f    815 MB    29 minutes ago    \n",
      "embeddinggemma:latest      85462619ee72    621 MB    32 minutes ago    \n",
      "nomic-embed-text:latest    0a109f422b47    274 MB    7 months ago      \n",
      "deepseek-r1:latest         0a8c26691023    4.7 GB    7 months ago      \n",
      "llama3.2:latest            a80c4f17acd5    2.0 GB    7 months ago      \n"
     ]
    }
   ],
   "source": [
    "# Check Ollama is running and list available models\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Ollama connection...\n",
      "\n",
      "‚úì Ollama is working!\n",
      "Response: Hello! I am running locally on your machine! üòä\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Ollama connection with a simple query\n",
    "print(\"Testing Ollama connection...\\n\")\n",
    "\n",
    "try:\n",
    "    test_llm = ChatOllama(model=\"gemma3:1b\", temperature=0)\n",
    "    response = test_llm.invoke(\"Say 'Hello! I am running locally on your machine!'\")\n",
    "    \n",
    "    print(\"‚úì Ollama is working!\")\n",
    "    print(f\"Response: {response.content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error connecting to Ollama: {e}\")\n",
    "    print(\"\\nMake sure Ollama is running. Try: ollama serve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load PDF Documents\n",
    "\n",
    "Load your PDF documents for the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 15 pages from 'attention.pdf'\n",
      "\n",
      "--- First Page Preview ---\n",
      "Content (first 300 chars): Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani‚àó\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer‚àó\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Par...\n",
      "\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Total characters: 39,587\n"
     ]
    }
   ],
   "source": [
    "# ===== CONFIGURATION: Update this path to your PDF file =====\n",
    "pdf_path = \"attention.pdf\"  # Change this to your PDF file path\n",
    "# =============================================================\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"‚ö†Ô∏è  ERROR: File '{pdf_path}' not found!\")\n",
    "    print(\"Please update the pdf_path variable with your PDF file location.\")\n",
    "else:\n",
    "    # Load the PDF\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Display information\n",
    "    print(f\"‚úì Loaded {len(documents)} pages from '{pdf_path}'\")\n",
    "    print(f\"\\n--- First Page Preview ---\")\n",
    "    print(f\"Content (first 300 chars): {documents[0].page_content[:300]}...\")\n",
    "    print(f\"\\nMetadata: {documents[0].metadata}\")\n",
    "    print(f\"\\nTotal characters: {sum(len(doc.page_content) for doc in documents):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Load Multiple PDFs from a Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to load multiple PDFs from a directory\n",
    "\n",
    "# pdf_directory = \"./pdfs\"\n",
    "# all_documents = []\n",
    "\n",
    "# if os.path.exists(pdf_directory):\n",
    "#     pdf_files = list(Path(pdf_directory).glob(\"*.pdf\"))\n",
    "#     print(f\"Found {len(pdf_files)} PDF files\\n\")\n",
    "    \n",
    "#     for pdf_file in pdf_files:\n",
    "#         loader = PyPDFLoader(str(pdf_file))\n",
    "#         docs = loader.load()\n",
    "#         all_documents.extend(docs)\n",
    "#         print(f\"  ‚úì Loaded {len(docs)} pages from {pdf_file.name}\")\n",
    "    \n",
    "#     print(f\"\\nTotal pages loaded: {len(all_documents)}\")\n",
    "#     documents = all_documents  # Use this for the rest of the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Split Documents into Chunks\n",
    "\n",
    "Break documents into smaller chunks for better retrieval precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Split 15 documents into 49 chunks\n",
      "\n",
      "Average chunk size: 873 characters\n",
      "\n",
      "--- Chunk Examples ---\n",
      "\n",
      "Chunk 1 (length: 986 chars):\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "...\n",
      "\n",
      "Chunk 2 (length: 944 chars):\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more pa...\n",
      "\n",
      "Chunk 3 (length: 986 chars):\n",
      "‚àóEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Tra...\n"
     ]
    }
   ],
   "source": [
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,        # Characters per chunk\n",
    "    chunk_overlap=128,      # Overlap to maintain context\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Split on paragraphs, then lines, etc.\n",
    ")\n",
    "\n",
    "# Split documents\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Display results\n",
    "avg_chunk_size = sum(len(chunk.page_content) for chunk in chunks) / len(chunks)\n",
    "\n",
    "print(f\"‚úì Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "print(f\"\\nAverage chunk size: {avg_chunk_size:.0f} characters\")\n",
    "\n",
    "# Preview chunks\n",
    "print(f\"\\n--- Chunk Examples ---\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1} (length: {len(chunk.page_content)} chars):\")\n",
    "    print(f\"{chunk.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Embeddings (Primary: Nomic-Embed-Text)\n",
    "\n",
    "### About Nomic-Embed-Text:\n",
    "- **Size**: 274 MB\n",
    "- **Dimensions**: 768\n",
    "- **Performance**: State-of-the-art for local embeddings\n",
    "- **Speed**: Fast inference\n",
    "- **License**: Open source (Apache 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing nomic-embed-text embeddings...\n",
      "\n",
      "‚úì Embeddings model: nomic-embed-text\n",
      "‚úì Embedding dimension: 768\n",
      "‚úì Sample embedding (first 10 values): [0.032493647, 0.060827848, -0.16611777, -0.08213917, 0.04330868, -0.026053637, 0.051593572, -0.0151964305, -0.008287022, -0.028351676]\n",
      "\n",
      "‚ÑπÔ∏è  Each chunk will be converted to a 768-dimensional vector\n",
      "‚ÑπÔ∏è  All processing happens locally on your machine!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ollama Embeddings with nomic-embed-text\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",\n",
    "    # base_url=\"http://localhost:11434\"  # Default Ollama URL\n",
    ")\n",
    "\n",
    "# Test embeddings\n",
    "print(\"Testing nomic-embed-text embeddings...\\n\")\n",
    "sample_text = \"This is a test sentence for embeddings.\"\n",
    "sample_embedding = embeddings.embed_query(sample_text)\n",
    "\n",
    "print(f\"‚úì Embeddings model: nomic-embed-text\")\n",
    "print(f\"‚úì Embedding dimension: {len(sample_embedding)}\")\n",
    "print(f\"‚úì Sample embedding (first 10 values): {sample_embedding[:10]}\")\n",
    "print(f\"\\n‚ÑπÔ∏è  Each chunk will be converted to a {len(sample_embedding)}-dimensional vector\")\n",
    "print(f\"‚ÑπÔ∏è  All processing happens locally on your machine!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Alternative: EmbeddingGemma (Optional)\n",
    "\n",
    "### About EmbeddingGemma:\n",
    "- **Size**: 621 MB (larger than nomic)\n",
    "- **Dimensions**: 768\n",
    "- **Optimized for**: Google Gemma models\n",
    "- **Use case**: Better alignment with Gemma LLMs\n",
    "\n",
    "**Uncomment the code below to use embeddinggemma instead:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Alternative: Use embeddinggemma instead\n",
    "# embeddings = OllamaEmbeddings(\n",
    "#     model=\"embeddinggemma:latest\"\n",
    "# )\n",
    "\n",
    "# # Test embeddings\n",
    "# print(\"Testing embeddinggemma embeddings...\\n\")\n",
    "# sample_text = \"This is a test sentence for embeddings.\"\n",
    "# sample_embedding = embeddings.embed_query(sample_text)\n",
    "\n",
    "# print(f\"‚úì Embeddings model: embeddinggemma\")\n",
    "# print(f\"‚úì Embedding dimension: {len(sample_embedding)}\")\n",
    "# print(f\"‚úì Sample embedding (first 10 values): {sample_embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create ChromaDB Vector Store\n",
    "\n",
    "### Why ChromaDB?\n",
    "- **Local & Persistent**: Stores vectors on disk\n",
    "- **Python 3.13 Compatible**: Works with latest Python\n",
    "- **Easy to Use**: Simple API\n",
    "- **Open Source**: Free and fully featured\n",
    "\n",
    "**Note**: This step may take a minute as it processes all chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ChromaDB vector store from 49 chunks...\n",
      "This may take a minute...\n",
      "\n",
      "‚úì ChromaDB vector store created successfully!\n",
      "‚úì Indexed 49 document chunks\n",
      "‚úì Stored at: ./chroma_db\n",
      "\n",
      "‚ÑπÔ∏è  Vector store persisted to disk - you can reload it later!\n"
     ]
    }
   ],
   "source": [
    "# Create ChromaDB vector store\n",
    "print(f\"Creating ChromaDB vector store from {len(chunks)} chunks...\")\n",
    "print(\"This may take a minute...\\n\")\n",
    "\n",
    "# Set persistent directory\n",
    "persist_directory = \"./chroma_db\"\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory,\n",
    "    collection_name=\"local_rag_collection\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì ChromaDB vector store created successfully!\")\n",
    "print(f\"‚úì Indexed {len(chunks)} document chunks\")\n",
    "print(f\"‚úì Stored at: {persist_directory}\")\n",
    "print(f\"\\n‚ÑπÔ∏è  Vector store persisted to disk - you can reload it later!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Existing Vector Store (Optional)\n",
    "\n",
    "If you've already created the vector store, you can load it instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to load existing vector store\n",
    "# persist_directory = \"./chroma_db\"\n",
    "\n",
    "# vectorstore = Chroma(\n",
    "#     persist_directory=persist_directory,\n",
    "#     embedding_function=embeddings,\n",
    "#     collection_name=\"local_rag_collection\"\n",
    "# )\n",
    "\n",
    "# print(f\"‚úì Loaded existing vector store from '{persist_directory}'\")\n",
    "# print(f\"‚úì Collection: local_rag_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Retriever and Test\n",
    "\n",
    "The retriever finds the most relevant chunks for a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Retriever configured successfully\n",
      "  - Search type: similarity\n",
      "  - Number of documents to retrieve (k): 4\n",
      "\n",
      "--- Retriever Test ---\n",
      "Query: 'What is the main topic of this document?'\n",
      "\n",
      "Retrieved 4 documents:\n",
      "\n",
      "Document 1:\n",
      "  Content preview: (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\n",
      "remained unchanged from the English-to-German base...\n",
      "  Source: Page 8\n",
      "\n",
      "Document 2:\n",
      "  Content preview: Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "...\n",
      "  Source: Page 13\n",
      "\n",
      "Document 3:\n",
      "  Content preview: Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
      "for different layer types. n is the sequence length, d...\n",
      "  Source: Page 5\n",
      "\n",
      "Document 4:\n",
      "  Content preview: Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "re...\n",
      "  Source: Page 12\n"
     ]
    }
   ],
   "source": [
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",    # Use cosine similarity\n",
    "    search_kwargs={\"k\": 4}        # Retrieve top 4 most relevant chunks\n",
    ")\n",
    "\n",
    "print(\"‚úì Retriever configured successfully\")\n",
    "print(f\"  - Search type: similarity\")\n",
    "print(f\"  - Number of documents to retrieve (k): 4\")\n",
    "\n",
    "# Test retrieval\n",
    "test_query = \"What is the main topic of this document?\"\n",
    "print(f\"\\n--- Retriever Test ---\")\n",
    "print(f\"Query: '{test_query}'\")\n",
    "\n",
    "retrieved_docs = retriever.invoke(test_query)\n",
    "\n",
    "print(f\"\\nRetrieved {len(retrieved_docs)} documents:\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Content preview: {doc.page_content[:150]}...\")\n",
    "    print(f\"  Source: Page {doc.metadata.get('page', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Configure Ollama LLM (Gemma3:1b)\n",
    "\n",
    "### About Gemma3:1b:\n",
    "- **Size**: 815 MB\n",
    "- **Parameters**: 1 billion\n",
    "- **Speed**: Very fast inference\n",
    "- **Quality**: Good for most Q&A tasks\n",
    "- **Memory**: Low RAM usage\n",
    "\n",
    "**Alternatives**: You can also use llama3.2 (2GB) or deepseek-r1 (4.7GB) for better quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LLM configured successfully\n",
      "  - Model: gemma3:1b (local)\n",
      "  - Temperature: 0 (deterministic)\n",
      "\n",
      "LLM Test Response: Hello! I am Gemma running locally! üòä\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ollama LLM\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma3:1b\",\n",
    "    temperature=0,          # Deterministic responses (0 = focused, 1 = creative)\n",
    "    # num_predict=2000,     # Max tokens to generate\n",
    "    # top_k=40,             # Top-k sampling\n",
    "    # top_p=0.9,            # Top-p (nucleus) sampling\n",
    ")\n",
    "\n",
    "print(\"‚úì LLM configured successfully\")\n",
    "print(f\"  - Model: gemma3:1b (local)\")\n",
    "print(f\"  - Temperature: 0 (deterministic)\")\n",
    "\n",
    "# Test LLM\n",
    "test_response = llm.invoke(\"Say 'Hello! I am Gemma running locally!'\")\n",
    "print(f\"\\nLLM Test Response: {test_response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Other Local Models (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Alternative: Use llama3.2 for better quality\n",
    "# llm = ChatOllama(\n",
    "#     model=\"llama3.2:latest\",\n",
    "#     temperature=0\n",
    "# )\n",
    "\n",
    "# # Alternative: Use deepseek-r1 for reasoning tasks\n",
    "# llm = ChatOllama(\n",
    "#     model=\"deepseek-r1:latest\",\n",
    "#     temperature=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Build RAG Chain (LangChain Expression Language)\n",
    "\n",
    "Combine retrieval and generation into a single pipeline using LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì RAG chain created successfully using LCEL!\n",
      "\n",
      "RAG Pipeline Flow:\n",
      "  1. User provides a query\n",
      "  2. Retriever finds top 4 relevant chunks (local ChromaDB)\n",
      "  3. Chunks are formatted as context\n",
      "  4. Context + question formatted with prompt template\n",
      "  5. Local LLM (gemma3:1b) generates answer\n",
      "  6. Answer parsed and returned\n",
      "\n",
      "üîí Everything runs locally on your machine!\n"
     ]
    }
   ],
   "source": [
    "# Define prompt template\n",
    "system_prompt = (\n",
    "    \"You are a helpful assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer based on the context, say that you don't know. \"\n",
    "    \"Keep the answer concise and accurate.\\n\\n\"\n",
    "    \"Context: {context}\\n\\n\"\n",
    "    \"Question: {question}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(system_prompt)\n",
    "\n",
    "# Helper function to format documents\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format retrieved documents into a single string.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build RAG chain using LCEL\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,  # Retrieve and format docs\n",
    "        \"question\": RunnablePassthrough()      # Pass through the question\n",
    "    }\n",
    "    | prompt           # Format with prompt template\n",
    "    | llm              # Generate answer with local LLM\n",
    "    | StrOutputParser() # Parse output to string\n",
    ")\n",
    "\n",
    "print(\"‚úì RAG chain created successfully using LCEL!\")\n",
    "print(\"\\nRAG Pipeline Flow:\")\n",
    "print(\"  1. User provides a query\")\n",
    "print(\"  2. Retriever finds top 4 relevant chunks (local ChromaDB)\")\n",
    "print(\"  3. Chunks are formatted as context\")\n",
    "print(\"  4. Context + question formatted with prompt template\")\n",
    "print(\"  5. Local LLM (gemma3:1b) generates answer\")\n",
    "print(\"  6. Answer parsed and returned\")\n",
    "print(\"\\nüîí Everything runs locally on your machine!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test RAG Pipeline with Example Queries\n",
    "\n",
    "Let's test our complete local RAG system!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the main topic or contribution of this document?\n",
      "\n",
      "Processing locally...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "The document discusses the parameters used during the development of the Section 22 development set, specifically focusing on learning rates and beam size.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "SOURCE DOCUMENTS USED:\n",
      "================================================================================\n",
      "\n",
      "Document 1:\n",
      "  Page: 8\n",
      "  Content: (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\n",
      "remained unchanged from the English-to-German base translation model. During inference, we\n",
      "9...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 2:\n",
      "  Page: 12\n",
      "  Content: Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 3:\n",
      "  Page: 13\n",
      "  Content: Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "sh...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 4:\n",
      "  Page: 2\n",
      "  Content: Decoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\n",
      "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
      "att...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example Query 1: General question\n",
    "query1 = \"What is the main topic or contribution of this document?\"\n",
    "\n",
    "print(f\"Query: {query1}\")\n",
    "print(\"\\nProcessing locally...\\n\")\n",
    "\n",
    "answer = rag_chain.invoke(query1)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Show source documents\n",
    "print(\"\\nSOURCE DOCUMENTS USED:\")\n",
    "print(\"=\" * 80)\n",
    "retrieved_docs = retriever.invoke(query1)\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Page: {doc.metadata.get('page', 'N/A')}\")\n",
    "    print(f\"  Content: {doc.page_content[:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Can you summarize the key technical contributions or innovations mentioned?\n",
      "\n",
      "Processing locally...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "Here‚Äôs a summary of the key technical contributions and innovations mentioned in the text:\n",
      "\n",
      "*   **Transformer Model Development:** Ashish, with Illia, designed and implemented the first Transformer models.\n",
      "*   **Parameter-Free Position Representation:** Niki designed, implemented, tuned, and evaluated numerous model variants incorporating parameter-free position representation.\n",
      "*   **Scaling Attention:** Llion experimented with novel model variants, focusing on scaled dot-product attention and multi-head attention.\n",
      "*   **Tensor2Tensor Replacement:** Lukasz and Aidan replaced the earlier codebase with tensor2tensor, significantly improving results and accelerating research.\n",
      "*   **Evaluation of Training Time:** The text estimates the number of floating-point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU.\n",
      "*   **Variations in Model Architectures:** Different model variations were tested to evaluate the importance of different components.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example Query 2: Specific information extraction\n",
    "query2 = \"Can you summarize the key technical contributions or innovations mentioned?\"\n",
    "\n",
    "print(f\"Query: {query2}\")\n",
    "print(\"\\nProcessing locally...\\n\")\n",
    "\n",
    "answer = rag_chain.invoke(query2)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What specific details are mentioned about the methodology or approach?\n",
      "\n",
      "Processing locally...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "The context describes that the authors used a ‚Äúdecomposable attention model‚Äù during inference, averaging attention-weighted positions, and that they used ‚ÄúSelf-attention‚Äù as a successful technique in various tasks.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example Query 3: Your custom question\n",
    "custom_query = \"What specific details are mentioned about the methodology or approach?\"\n",
    "\n",
    "print(f\"Query: {custom_query}\")\n",
    "print(\"\\nProcessing locally...\\n\")\n",
    "\n",
    "answer = rag_chain.invoke(custom_query)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Interactive Q&A Session\n",
    "\n",
    "Ask your own questions to the RAG system!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Question: What are the main findings or results?\n",
      "================================================================================\n",
      "\n",
      "Answer: The text describes research focused on Transformer architecture, specifically during the NIPS 2017 conference. It details variations on the Transformer architecture, including different training sets and metrics. The key findings are:\n",
      "\n",
      "*   **Beam search was used for inference.**\n",
      "*   **The model used a byte-pair encoding for perplexity.**\n",
      "*   **The model's performance on the English-to-German translation development set was improved in the Dev set.**\n",
      "*   **The model's performance on the English-to-German translation development set was improved in the Dev set.**\n",
      "*   **The model's performance on the newstest2013 development set was improved.**\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The text describes research focused on Transformer architecture, specifically during the NIPS 2017 conference. It details variations on the Transformer architecture, including different training sets and metrics. The key findings are:\\n\\n*   **Beam search was used for inference.**\\n*   **The model used a byte-pair encoding for perplexity.**\\n*   **The model's performance on the English-to-German translation development set was improved in the Dev set.**\\n*   **The model's performance on the English-to-German translation development set was improved in the Dev set.**\\n*   **The model's performance on the newstest2013 development set was improved.**\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interactive Q&A\n",
    "def ask_question(question):\n",
    "    \"\"\"Ask a question to the RAG system.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    answer = rag_chain.invoke(question)\n",
    "    \n",
    "    print(f\"\\nAnswer: {answer}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Try it out!\n",
    "# Change the question below to ask anything about your document\n",
    "my_question = \"What are the main findings or results?\"\n",
    "ask_question(my_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Bonus: Compare Embedding Models (Optional)\n",
    "\n",
    "Compare retrieval results between nomic-embed-text and embeddinggemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to compare embedding models\n",
    "\n",
    "# print(\"Comparing embedding models...\\n\")\n",
    "\n",
    "# test_query = \"What is attention mechanism?\"\n",
    "\n",
    "# # Test with nomic-embed-text\n",
    "# print(\"=\" * 80)\n",
    "# print(\"Using nomic-embed-text:\")\n",
    "# print(\"=\" * 80)\n",
    "# embeddings_nomic = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "# vectorstore_nomic = Chroma.from_documents(\n",
    "#     documents=chunks[:10],  # Use first 10 chunks for quick test\n",
    "#     embedding=embeddings_nomic,\n",
    "#     collection_name=\"test_nomic\"\n",
    "# )\n",
    "# retriever_nomic = vectorstore_nomic.as_retriever(search_kwargs={\"k\": 2})\n",
    "# docs_nomic = retriever_nomic.invoke(test_query)\n",
    "\n",
    "# print(f\"\\nTop retrieved document:\")\n",
    "# print(f\"{docs_nomic[0].page_content[:200]}...\\n\")\n",
    "\n",
    "# # Test with embeddinggemma\n",
    "# print(\"=\" * 80)\n",
    "# print(\"Using embeddinggemma:\")\n",
    "# print(\"=\" * 80)\n",
    "# embeddings_gemma = OllamaEmbeddings(model=\"embeddinggemma:latest\")\n",
    "# vectorstore_gemma = Chroma.from_documents(\n",
    "#     documents=chunks[:10],\n",
    "#     embedding=embeddings_gemma,\n",
    "#     collection_name=\"test_gemma\"\n",
    "# )\n",
    "# retriever_gemma = vectorstore_gemma.as_retriever(search_kwargs={\"k\": 2})\n",
    "# docs_gemma = retriever_gemma.invoke(test_query)\n",
    "\n",
    "# print(f\"\\nTop retrieved document:\")\n",
    "# print(f\"{docs_gemma[0].page_content[:200]}...\\n\")\n",
    "\n",
    "# print(\"=\" * 80)\n",
    "# print(\"\\n‚ÑπÔ∏è  Both models perform well. Choose based on your preference!\")\n",
    "# print(\"   - nomic-embed-text: Smaller (274MB), general-purpose\")\n",
    "# print(\"   - embeddinggemma: Larger (621MB), optimized for Gemma models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Performance Tips & Next Steps\n",
    "\n",
    "### üöÄ Performance Optimization:\n",
    "1. **Chunk Size**: Experiment with different sizes (512, 1024, 2048)\n",
    "2. **Retrieval Count (k)**: Try k=3, 4, 5, 6 based on your needs\n",
    "3. **Model Selection**: \n",
    "   - Fast: gemma3:1b\n",
    "   - Balanced: llama3.2\n",
    "   - Best quality: deepseek-r1\n",
    "4. **Temperature**: 0 for factual, 0.3-0.7 for creative\n",
    "\n",
    "### üíæ Persistence:\n",
    "- Vector store is saved at `./chroma_db`\n",
    "- You can reload it without re-embedding documents\n",
    "- Delete the directory to start fresh\n",
    "\n",
    "### üîß Troubleshooting:\n",
    "- **Slow responses**: Use smaller model (gemma3:1b)\n",
    "- **Out of memory**: Reduce chunk count or use smaller model\n",
    "- **Ollama not found**: Make sure `ollama serve` is running\n",
    "\n",
    "### üìö Next Steps:\n",
    "1. Try different documents and PDFs\n",
    "2. Experiment with other Ollama models\n",
    "3. Add custom preprocessing or post-processing\n",
    "4. Build a simple UI with Gradio or Streamlit\n",
    "5. Compare with cloud-based RAG (OpenAI, etc.)\n",
    "\n",
    "### üéâ Congratulations!\n",
    "You now have a fully functional **local, offline RAG system** running on your machine!\n",
    "\n",
    "---\n",
    "\n",
    "**Created with LangChain + Ollama + ChromaDB**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
