{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 10: RAG Evaluation with Ragas Framework\n",
    "\n",
    "**Module 3: Traditional RAG Implementation**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š What You'll Learn\n",
    "\n",
    "This notebook covers the complete lifecycle of building and evaluating a production-quality RAG system:\n",
    "\n",
    "1. âœ… **Basic RAG Pipeline** - Document loading, chunking, embeddings, retrieval, generation\n",
    "2. âœ… **Building RAG Applications** - Prompt engineering, response formatting, handling edge cases\n",
    "3. âœ… **Evaluation and Testing** - Comprehensive evaluation with Ragas framework\n",
    "\n",
    "### Why Evaluation Matters\n",
    "\n",
    "Building a RAG system is only half the battle. Without proper evaluation, you can't:\n",
    "- Know if your system is producing accurate answers\n",
    "- Compare different configuration choices\n",
    "- Identify areas for improvement\n",
    "- Ensure production-ready quality\n",
    "\n",
    "**This notebook introduces the Ragas evaluation framework** - the industry-standard tool for measuring RAG system performance.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "- Understand the importance of RAG evaluation\n",
    "- Implement a complete RAG pipeline from scratch\n",
    "- Integrate the Ragas evaluation framework\n",
    "- Create comprehensive test datasets with ground truth\n",
    "- **Evaluate RAG systems across all 6 key non-multimodal metrics**\n",
    "- Compare multiple RAG configurations systematically\n",
    "- Visualize and interpret evaluation results\n",
    "- Apply production best practices\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“– Table of Contents\n",
    "\n",
    "1. **Introduction & Setup** (ðŸ”° BEGINNER)\n",
    "2. **Custom Dummy Data Creation** (ðŸ”°/ðŸŽ“ BEGINNER/INTERMEDIATE)\n",
    "3. **Basic RAG Pipeline Implementation** (ðŸŽ“ INTERMEDIATE)\n",
    "4. **Introduction to Ragas Framework** (ðŸ”° BEGINNER)\n",
    "5. **Creating Evaluation Dataset** (ðŸŽ“ INTERMEDIATE)\n",
    "6. **Ragas Metrics Implementation - All 6 Metrics** (ðŸŽ“/ðŸš€ INTERMEDIATE/ADVANCED)\n",
    "7. **Running Comprehensive Evaluation** (ðŸŽ“ INTERMEDIATE)\n",
    "8. **Advanced Evaluation - Multiple Configurations** (ðŸš€ ADVANCED)\n",
    "9. **Results Analysis & Visualization** (ðŸŽ“/ðŸš€ INTERMEDIATE/ADVANCED)\n",
    "10. **Best Practices & Summary** (ðŸ”°/ðŸŽ“ BEGINNER/INTERMEDIATE)\n",
    "\n",
    "---\n",
    "\n",
    "**Difficulty Level:** ðŸŽ“ Intermediate (assumes familiarity with RAG basics from Notebooks 01-07)\n",
    "\n",
    "**Estimated Time:** 90-120 minutes\n",
    "\n",
    "**Prerequisites:**\n",
    "- Completed Notebooks 01-07 (or equivalent RAG knowledge)\n",
    "- OpenAI API key\n",
    "- Python 3.9+\n",
    "\n",
    "Let's begin! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Introduction & Setup\n",
    "\n",
    "**Difficulty:** ðŸ”° BEGINNER\n",
    "\n",
    "In this section, we'll:\n",
    "1. Install the Ragas evaluation framework\n",
    "2. Import all required libraries\n",
    "3. Load environment variables\n",
    "4. Verify our setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Install Ragas and Dependencies\n",
    "\n",
    "Ragas (Retrieval Augmented Generation Assessment) is the industry-standard framework for evaluating RAG systems.\n",
    "\n",
    "**New packages we're installing:**\n",
    "- `ragas==0.3.9` - RAG evaluation framework (latest stable version)\n",
    "- `datasets>=2.14.0` - Required by Ragas for dataset handling\n",
    "- `seaborn>=0.12.0` - Enhanced visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ragas and required dependencies\n",
    "# Note: Uncomment the lines below if you haven't installed these packages yet\n",
    "\n",
    "# !pip install ragas==0.3.7 datasets>=2.14.0 seaborn>=0.12.0 -q\n",
    "# !pip install opik --quiet  # OPIK for experiment tracking and observability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Import Required Libraries\n",
    "\n",
    "We'll import all necessary libraries for our RAG pipeline and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ OPIK available for experiment tracking\n",
      "âœ“ All imports successful!\n",
      "âœ“ Using LangChain 1.0+ with LCEL patterns\n",
      "âœ“ Ragas 0.3.7 evaluation framework loaded\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain core imports (LCEL patterns)\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# LangChain components\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Ragas evaluation framework (updated for 0.3.7)\n",
    "from ragas import evaluate, EvaluationDataset, SingleTurnSample\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    AnswerRelevancy,\n",
    "    ContextPrecision,\n",
    "    ContextRecall,\n",
    "    ContextEntityRecall,\n",
    "    NoiseSensitivity,\n",
    ")\n",
    "# Ragas LangChain integration wrappers (required for 0.3.9)\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "# HuggingFace datasets for Ragas\n",
    "from datasets import Dataset\n",
    "\n",
    "# OPIK for experiment tracking and observability\n",
    "try:\n",
    "    import opik\n",
    "    from opik import track, Opik\n",
    "    OPIK_AVAILABLE = True\n",
    "    print(\"âœ“ OPIK available for experiment tracking\")\n",
    "except ImportError:\n",
    "    OPIK_AVAILABLE = False\n",
    "    print(\"â„¹ OPIK not installed. Install with: pip install opik\")\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ All imports successful!\")\n",
    "print(\"âœ“ Using LangChain 1.0+ with LCEL patterns\")\n",
    "print(\"âœ“ Ragas 0.3.7 evaluation framework loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Load Environment Variables\n",
    "\n",
    "Load your OpenAI API key from the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ OpenAI API Key loaded: sk-proj-...\n",
      "âœ“ Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify OpenAI API key is loaded\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"âŒ WARNING: OPENAI_API_KEY not found!\")\n",
    "    print(\"Please set it in .env file or export it as an environment variable\")\n",
    "else:\n",
    "    print(f\"âœ“ OpenAI API Key loaded: {os.getenv('OPENAI_API_KEY')[:8]}...\")\n",
    "    print(\"âœ“ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Configure OPIK (Optional)\n",
    "\n",
    "OPIK provides experiment tracking and observability for your RAG evaluations.\n",
    "\n",
    "**Setup Steps:**\n",
    "1. Sign up at https://www.comet.com/signup (free tier available)\n",
    "2. Get your API key from https://www.comet.com/opik/settings\n",
    "3. Set `OPIK_API_KEY` in your `.env` file or run `opik configure` in terminal\n",
    "4. OPIK will automatically track all evaluation runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which Opik deployment do you want to log your traces to?\n",
      ") - Opik Cloud (default \n",
      "2 - Self-hosted Comet platform\n",
      "3 - Local deployment\n",
      "\n",
      "> \n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "#!opik configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ OPIK configured successfully!\n",
      "  Project: RAG_Evaluation_Ragas\n",
      "  Dashboard: https://www.comet.com/opik/projects/RAG_Evaluation_Ragas\n",
      "\n",
      "ðŸ“Š All evaluation results will be logged to OPIK\n"
     ]
    }
   ],
   "source": [
    "# Configure OPIK for experiment tracking\n",
    "if OPIK_AVAILABLE:\n",
    "    try:\n",
    "        # Initialize OPIK client\n",
    "        opik_client = Opik()\n",
    "        \n",
    "        # Create a project for this notebook\n",
    "        project_name = \"RAG_Evaluation_Ragas\"\n",
    "        \n",
    "        print(\"âœ“ OPIK configured successfully!\")\n",
    "        print(f\"  Project: {project_name}\")\n",
    "        print(f\"  Dashboard: https://www.comet.com/opik/projects/{project_name}\")\n",
    "        print(\"\\nðŸ“Š All evaluation results will be logged to OPIK\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ OPIK configuration failed: {e}\")\n",
    "        print(\"  Continuing without OPIK tracking...\")\n",
    "        OPIK_AVAILABLE = False\n",
    "else:\n",
    "    print(\"â„¹ OPIK not available - skipping experiment tracking\")\n",
    "    print(\"  To enable: pip install opik && opik configure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Custom Dummy Data Creation\n",
    "\n",
    "**Difficulty:** ðŸ”°/ðŸŽ“ BEGINNER/INTERMEDIATE\n",
    "\n",
    "We'll create a comprehensive set of documentation for a fictional cloud platform called **CloudFlow**.\n",
    "\n",
    "## Why CloudFlow Platform?\n",
    "\n",
    "- Technical enough for meaningful Q&A evaluation\n",
    "- Clear documentation structure supports various question types\n",
    "- Realistic enterprise scenario\n",
    "- Includes specific facts and numbers for testing accuracy\n",
    "\n",
    "## Document Categories (18 documents total)\n",
    "\n",
    "1. **Architecture** (3 docs) - System design, components, scaling\n",
    "2. **API Documentation** (4 docs) - Authentication, endpoints, rate limits, errors\n",
    "3. **Security** (2 docs) - Features and compliance\n",
    "4. **Pricing** (2 docs) - Tiers and billing\n",
    "5. **Best Practices** (3 docs) - Optimization, monitoring, DR\n",
    "6. **Troubleshooting** (3 docs) - Common errors and solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 17 CloudFlow documentation documents\n",
      "\n",
      "Document breakdown by category:\n",
      "  - Architecture: 3 documents\n",
      "  - Api: 4 documents\n",
      "  - Security: 2 documents\n",
      "  - Pricing: 2 documents\n",
      "  - Best_Practices: 3 documents\n",
      "  - Troubleshooting: 3 documents\n"
     ]
    }
   ],
   "source": [
    "# Create CloudFlow Platform Documentation\n",
    "# Each document is 200-400 words with specific facts for evaluation\n",
    "\n",
    "cloudflow_docs = [\n",
    "    # ============================================================================\n",
    "    # ARCHITECTURE DOCUMENTS (3)\n",
    "    # ============================================================================\n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Architecture Overview\n",
    "\n",
    "CloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\n",
    "\n",
    "The API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\n",
    "\n",
    "The Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\n",
    "\n",
    "The Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand.\n",
    "\n",
    "CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics.\"\"\",\n",
    "        metadata={\"source\": \"architecture_overview\", \"topic\": \"architecture\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Scaling Mechanisms\n",
    "\n",
    "CloudFlow implements sophisticated auto-scaling mechanisms to handle varying workloads efficiently. The platform monitors real-time metrics to make intelligent scaling decisions.\n",
    "\n",
    "Horizontal Pod Autoscaling (HPA) adjusts the number of pod replicas based on CPU utilization (target: 70%) and memory usage (target: 80%). When these thresholds are exceeded for more than 3 consecutive minutes, the system automatically provisions additional pods.\n",
    "\n",
    "Vertical scaling adjusts resource allocation for individual services. CloudFlow can increase or decrease CPU and memory limits without downtime, using Kubernetes resource management capabilities.\n",
    "\n",
    "The platform supports bursting to handle sudden traffic spikes. During burst periods, CloudFlow can temporarily scale up to 500% of baseline capacity for up to 15 minutes before triggering permanent scaling.\n",
    "\n",
    "Load balancing distributes traffic across all available pods using a weighted round-robin algorithm. Health checks run every 10 seconds, and unhealthy pods are automatically removed from the rotation within 30 seconds.\"\"\",\n",
    "        metadata={\"source\": \"scaling_guide\", \"topic\": \"architecture\", \"difficulty\": \"advanced\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow System Components\n",
    "\n",
    "CloudFlow's architecture comprises several key components that work in harmony to deliver reliable cloud services.\n",
    "\n",
    "The Control Plane manages the overall system state, including service registration, configuration management, and orchestration. It runs on a dedicated cluster with five replicas for high availability.\n",
    "\n",
    "The Data Plane handles actual request processing and data flow. It consists of worker nodes that execute application workloads and process user requests. Each data plane node has 16 CPU cores and 64GB RAM.\n",
    "\n",
    "The Observability Stack includes Prometheus for metrics collection, Grafana for visualization, and ELK (Elasticsearch, Logstash, Kibana) for log aggregation. Metrics are collected every 15 seconds and retained for 90 days.\n",
    "\n",
    "The Service Registry maintains a real-time directory of all available services and their endpoints. It uses etcd for distributed consensus and supports automatic service discovery with DNS-based lookups.\n",
    "\n",
    "The Message Queue system, based on Apache Kafka, handles asynchronous communication between services with guaranteed message delivery and ordering.\"\"\",\n",
    "        metadata={\"source\": \"system_components\", \"topic\": \"architecture\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    # ============================================================================\n",
    "    # API DOCUMENTATION (4)\n",
    "    # ============================================================================\n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow API Authentication\n",
    "\n",
    "CloudFlow APIs support two authentication methods: OAuth 2.0 and API Keys. Both methods provide secure access to platform resources.\n",
    "\n",
    "OAuth 2.0 is recommended for user-facing applications. It supports the Authorization Code flow and provides access tokens valid for 1 hour and refresh tokens valid for 30 days. To implement OAuth 2.0, direct users to the authorization endpoint at https://auth.cloudflow.io/oauth/authorize with your client_id and redirect_uri parameters.\n",
    "\n",
    "API Keys are ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters. API keys never expire unless explicitly revoked.\n",
    "\n",
    "To authenticate requests, include your API key in the Authorization header: \"Authorization: Bearer YOUR_API_KEY\". All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error.\n",
    "\n",
    "API keys can be scoped to specific permissions (read, write, admin) and restricted to specific IP addresses for enhanced security. You can manage your API keys through the CloudFlow dashboard or the /api/v1/keys endpoint.\"\"\",\n",
    "        metadata={\"source\": \"api_authentication\", \"topic\": \"api\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow REST API Endpoints\n",
    "\n",
    "CloudFlow provides a comprehensive REST API with endpoints organized by resource type. All endpoints follow RESTful conventions and return JSON responses.\n",
    "\n",
    "Base URL: https://api.cloudflow.io/v1\n",
    "\n",
    "Resources endpoint: GET /api/v1/resources - List all resources with pagination (max 100 per page). Supports filtering by type, status, and creation date.\n",
    "\n",
    "Resource creation: POST /api/v1/resources - Create a new resource. Required fields: name (string), type (string), config (object). Returns 201 Created on success.\n",
    "\n",
    "Resource details: GET /api/v1/resources/{id} - Retrieve detailed information about a specific resource by ID.\n",
    "\n",
    "Resource update: PUT /api/v1/resources/{id} - Update an existing resource. Supports partial updates with PATCH /api/v1/resources/{id}.\n",
    "\n",
    "Resource deletion: DELETE /api/v1/resources/{id} - Delete a resource. Returns 204 No Content on success. Deleted resources are soft-deleted and can be recovered within 30 days.\n",
    "\n",
    "All list endpoints support query parameters: limit (default: 25, max: 100), offset (default: 0), sort (default: created_at), order (asc|desc).\"\"\",\n",
    "        metadata={\"source\": \"api_endpoints\", \"topic\": \"api\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow API Rate Limiting\n",
    "\n",
    "CloudFlow implements rate limiting to ensure fair usage and platform stability. Rate limits vary by pricing tier and authentication method.\n",
    "\n",
    "Standard Tier: 1,000 requests per hour per API key. Burst capacity allows up to 100 requests per minute. Exceeding limits returns HTTP 429 (Too Many Requests).\n",
    "\n",
    "Premium Tier: 10,000 requests per hour per API key with burst capacity of 500 requests per minute. Premium tier also includes priority request processing.\n",
    "\n",
    "Enterprise Tier: Custom rate limits negotiated based on usage patterns. Typically starts at 100,000 requests per hour with dedicated infrastructure.\n",
    "\n",
    "Rate limit headers are included in every response:\n",
    "- X-RateLimit-Limit: Maximum requests per hour\n",
    "- X-RateLimit-Remaining: Remaining requests in current window\n",
    "- X-RateLimit-Reset: Unix timestamp when the limit resets\n",
    "\n",
    "When rate limited, the Retry-After header indicates how many seconds to wait before retrying. Implement exponential backoff: wait 1s, then 2s, then 4s, etc.\n",
    "\n",
    "OAuth 2.0 authenticated requests have separate, higher limits: 5,000 requests per hour for Standard tier.\"\"\",\n",
    "        metadata={\"source\": \"api_rate_limits\", \"topic\": \"api\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow API Error Codes\n",
    "\n",
    "CloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\n",
    "\n",
    "Authentication Errors:\n",
    "- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\n",
    "- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\n",
    "\n",
    "Client Errors:\n",
    "- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what's wrong.\n",
    "- 404 Not Found: Requested resource doesn't exist. Verify the resource ID.\n",
    "- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\n",
    "- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\n",
    "- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header.\n",
    "\n",
    "Server Errors:\n",
    "- 500 Internal Server Error: Unexpected server error. CloudFlow team is automatically notified.\n",
    "- 502 Bad Gateway: Temporary issue with upstream services. Retry after a few seconds.\n",
    "- 503 Service Unavailable: Scheduled maintenance or system overload. Check status.cloudflow.io.\n",
    "\n",
    "Error Response Format: {\"error\": {\"code\": \"error_code\", \"message\": \"Human-readable message\", \"details\": {...}}}\"\"\",\n",
    "        metadata={\"source\": \"api_error_codes\", \"topic\": \"api\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    \n",
    "    # ============================================================================\n",
    "    # SECURITY DOCUMENTATION (2)\n",
    "    # ============================================================================\n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Security Features\n",
    "\n",
    "Security is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\n",
    "\n",
    "Encryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\n",
    "\n",
    "Network Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\n",
    "\n",
    "Access Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect.\n",
    "\n",
    "Audit Logging: Every API call is logged with timestamp, user identity, IP address, and action taken. Audit logs are immutable and retained for 2 years. You can access logs via the /api/v1/audit-logs endpoint.\n",
    "\n",
    "Vulnerability Management: CloudFlow undergoes quarterly penetration testing by independent security firms. We maintain a bug bounty program and respond to security reports within 24 hours.\"\"\",\n",
    "        metadata={\"source\": \"security_features\", \"topic\": \"security\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Compliance Standards\n",
    "\n",
    "CloudFlow maintains compliance with major industry standards and regulations to ensure your data is handled responsibly.\n",
    "\n",
    "SOC 2 Type II: CloudFlow is SOC 2 Type II certified, demonstrating our commitment to security, availability, and confidentiality. Audit reports are available to enterprise customers under NDA.\n",
    "\n",
    "GDPR Compliance: CloudFlow is fully compliant with the European Union's General Data Protection Regulation. We support data residency requirements, right to erasure, data portability, and provide Data Processing Agreements (DPA) to all customers.\n",
    "\n",
    "HIPAA: For healthcare customers, CloudFlow offers HIPAA-compliant infrastructure with Business Associate Agreements (BAA). HIPAA features include enhanced audit logging, encrypted backups, and strict access controls.\n",
    "\n",
    "ISO 27001: CloudFlow's information security management system is certified to ISO 27001:2013 standards. We maintain comprehensive security policies and undergo annual recertification audits.\n",
    "\n",
    "PCI DSS: For customers processing payment card data, CloudFlow provides PCI DSS Level 1 certified infrastructure. However, we recommend using dedicated payment processors rather than storing card data.\n",
    "\n",
    "Data Residency: CloudFlow supports data residency in US, EU, UK, and APAC regions to meet local regulatory requirements.\"\"\",\n",
    "        metadata={\"source\": \"compliance_standards\", \"topic\": \"security\", \"difficulty\": \"advanced\"}\n",
    "    ),\n",
    "    \n",
    "    # ============================================================================\n",
    "    # PRICING DOCUMENTATION (2)\n",
    "    # ============================================================================\n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Pricing Tiers\n",
    "\n",
    "CloudFlow offers three pricing tiers designed to meet the needs of individuals, teams, and enterprises.\n",
    "\n",
    "Standard Tier ($99/month):\n",
    "- 1,000 API requests per hour\n",
    "- 100 GB storage included\n",
    "- 10 GB bandwidth per month\n",
    "- Community support via forums\n",
    "- 99.9% uptime SLA\n",
    "- Up to 5 team members\n",
    "\n",
    "Premium Tier ($499/month):\n",
    "- 10,000 API requests per hour\n",
    "- 1 TB storage included\n",
    "- 100 GB bandwidth per month\n",
    "- Email support with 24-hour response time\n",
    "- 99.95% uptime SLA\n",
    "- Up to 25 team members\n",
    "- Advanced monitoring and alerting\n",
    "- Custom domain support\n",
    "\n",
    "Enterprise Tier (Custom pricing):\n",
    "- Custom API rate limits (100,000+ requests/hour)\n",
    "- Unlimited storage and bandwidth\n",
    "- 24/7 phone and email support with 1-hour response time\n",
    "- 99.99% uptime SLA with service credits\n",
    "- Unlimited team members\n",
    "- Dedicated account manager\n",
    "- Custom integrations and professional services\n",
    "- Private cloud deployment options\n",
    "\n",
    "All tiers include: SSL certificates, daily backups, API access, and dashboard analytics. Annual billing provides 15% discount.\"\"\",\n",
    "        metadata={\"source\": \"pricing_tiers\", \"topic\": \"pricing\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Billing Information\n",
    "\n",
    "Understanding CloudFlow's billing model helps you manage costs effectively and avoid unexpected charges.\n",
    "\n",
    "Billing Cycle: Subscriptions are billed monthly on the date you signed up. Annual subscriptions are billed upfront with a 15% discount. Billing date can be changed once per year.\n",
    "\n",
    "Usage-Based Charges: Beyond included quotas, additional usage is billed at:\n",
    "- API requests: $0.01 per 1,000 requests\n",
    "- Storage: $0.10 per GB per month\n",
    "- Bandwidth: $0.08 per GB\n",
    "- Backup retention (beyond 30 days): $0.05 per GB per month\n",
    "\n",
    "Payment Methods: CloudFlow accepts credit cards (Visa, Mastercard, Amex), ACH transfers (US only), and wire transfers for invoices over $1,000. Cryptocurrency payments available for annual plans.\n",
    "\n",
    "Invoicing: Invoices are emailed on the billing date and available in the dashboard. Enterprise customers receive consolidated monthly invoices with 30-day payment terms.\n",
    "\n",
    "Upgrades and Downgrades: Upgrade anytime to immediately access higher tier features. Downgrades take effect at the next billing cycle. Prorated credits are applied to your account balance.\n",
    "\n",
    "Free Trial: New customers get 14-day free trial on Premium tier with no credit card required. Trial includes 1,000 API requests and 10 GB storage.\"\"\",\n",
    "        metadata={\"source\": \"billing_info\", \"topic\": \"pricing\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    \n",
    "    # ============================================================================\n",
    "    # BEST PRACTICES DOCUMENTATION (3)\n",
    "    # ============================================================================\n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Performance Optimization\n",
    "\n",
    "Following these best practices will help you achieve optimal performance from your CloudFlow applications.\n",
    "\n",
    "Caching Strategy: Implement caching at multiple levels. Use CloudFlow's built-in Redis cache for frequently accessed data with TTL between 5-60 minutes. Cache API responses on the client side and respect Cache-Control headers.\n",
    "\n",
    "Request Optimization: Batch multiple operations into single API calls when possible. Use pagination for large result sets (recommended page size: 50-100 items). Implement request compression using gzip to reduce bandwidth.\n",
    "\n",
    "Connection Management: Reuse HTTP connections with keep-alive headers. Maintain a connection pool with 5-10 concurrent connections per API key. Set appropriate timeouts: connection timeout 10s, read timeout 30s.\n",
    "\n",
    "Query Efficiency: Use field filtering to request only required data: /resources?fields=id,name,status. Leverage server-side filtering instead of retrieving all data and filtering locally.\n",
    "\n",
    "Asynchronous Processing: For long-running operations, use CloudFlow's async API endpoints. Poll for results using the returned job_id rather than blocking on the initial request.\n",
    "\n",
    "CDN Usage: Serve static assets through CloudFlow's global CDN with 150+ edge locations. Configure appropriate cache headers for optimal performance: max-age=3600 for semi-static content.\"\"\",\n",
    "        metadata={\"source\": \"performance_optimization\", \"topic\": \"best_practices\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Monitoring and Observability\n",
    "\n",
    "Effective monitoring ensures your CloudFlow applications remain healthy and performant.\n",
    "\n",
    "Metrics Collection: CloudFlow automatically collects key metrics including request rate, error rate, latency (p50, p95, p99), and resource utilization. Access metrics via the dashboard or Metrics API at /api/v1/metrics.\n",
    "\n",
    "Custom Metrics: Send custom application metrics using the StatsD protocol. CloudFlow aggregates custom metrics every 60 seconds and retains them for 90 days.\n",
    "\n",
    "Alerting: Configure alerts for critical conditions like error rate >5%, latency >500ms, or approaching rate limits. CloudFlow supports alerting via email, SMS, Slack, PagerDuty, and webhooks.\n",
    "\n",
    "Distributed Tracing: Enable distributed tracing to track requests across services. CloudFlow supports OpenTelemetry and provides trace visualization in the dashboard. Sample rate: 10% of requests (configurable up to 100%).\n",
    "\n",
    "Log Management: CloudFlow retains logs for 7 days by default (30 days for Premium, 90 days for Enterprise). Use structured logging with JSON format for better searchability. Maximum log line length: 32KB.\n",
    "\n",
    "Dashboard Widgets: Create custom dashboards with real-time metrics, SLA compliance, and cost tracking. Share dashboards with team members or embed in external tools using iframe integration.\"\"\",\n",
    "        metadata={\"source\": \"monitoring_observability\", \"topic\": \"best_practices\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Disaster Recovery\n",
    "\n",
    "CloudFlow implements comprehensive disaster recovery capabilities to protect your data and ensure business continuity.\n",
    "\n",
    "Backup Strategy: CloudFlow performs automatic daily backups of all data at 2 AM UTC. Backups are encrypted and stored in geographically diverse locations. Retention: 30 days for Standard tier, 90 days for Premium, 1 year for Enterprise.\n",
    "\n",
    "Point-in-Time Recovery: Enterprise customers can restore data to any point within the retention period with 5-minute granularity. Recovery operations typically complete within 15-30 minutes.\n",
    "\n",
    "Multi-Region Replication: Enable multi-region replication for critical data. Data is asynchronously replicated to a secondary region within 60 seconds. Failover to secondary region is automatic and takes approximately 5 minutes.\n",
    "\n",
    "Backup Verification: CloudFlow performs monthly backup restoration tests to ensure data recoverability. Test results are available in your compliance dashboard.\n",
    "\n",
    "Export Capabilities: Export your data anytime in JSON, CSV, or Parquet format. Full exports are available via the /api/v1/export endpoint. Large exports (>10 GB) are delivered to your S3 bucket.\n",
    "\n",
    "RTO and RPO: CloudFlow guarantees Recovery Time Objective (RTO) of 4 hours and Recovery Point Objective (RPO) of 1 hour for Enterprise tier. Contact support to initiate disaster recovery procedures.\"\"\",\n",
    "        metadata={\"source\": \"disaster_recovery\", \"topic\": \"best_practices\", \"difficulty\": \"advanced\"}\n",
    "    ),\n",
    "    \n",
    "    # ============================================================================\n",
    "    # TROUBLESHOOTING DOCUMENTATION (3)\n",
    "    # ============================================================================\n",
    "    Document(\n",
    "        page_content=\"\"\"Common CloudFlow Errors and Solutions\n",
    "\n",
    "This guide covers the most common errors encountered when using CloudFlow and their solutions.\n",
    "\n",
    "Error: \"Invalid API Key\" (401)\n",
    "Solution: Verify your API key format starts with \"cf_live_\" and is exactly 40 characters. Check for extra spaces or newlines. Generate a new API key if the issue persists. API keys are case-sensitive.\n",
    "\n",
    "Error: \"Rate Limit Exceeded\" (429)\n",
    "Solution: Implement exponential backoff in your retry logic. Check X-RateLimit-Reset header to know when limits reset. Consider upgrading to a higher tier if you consistently hit limits. Use batch endpoints to reduce request count.\n",
    "\n",
    "Error: \"Resource Not Found\" (404)\n",
    "Solution: Verify the resource ID is correct and the resource hasn't been deleted. Use the /api/v1/resources endpoint to list available resources. Check if you're using the correct API version (/v1).\n",
    "\n",
    "Error: \"Timeout\" (504)\n",
    "Solution: Increase client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Check CloudFlow status page for any service degradation.\n",
    "\n",
    "Error: \"Validation Error\" (422)\n",
    "Solution: Review the error details field for specific validation failures. Common issues: missing required fields, invalid data types, values outside allowed ranges. Consult API documentation for correct request format.\"\"\",\n",
    "        metadata={\"source\": \"common_errors\", \"topic\": \"troubleshooting\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Debugging Guide\n",
    "\n",
    "When troubleshooting issues with CloudFlow, follow this systematic debugging approach.\n",
    "\n",
    "Step 1 - Check Service Status: Visit status.cloudflow.io to verify all systems are operational. Subscribe to status updates to receive notifications about incidents and maintenance.\n",
    "\n",
    "Step 2 - Review API Logs: Access detailed API logs in the CloudFlow dashboard under Analytics > API Logs. Filter by time range, status code, and endpoint. Look for patterns in failed requests.\n",
    "\n",
    "Step 3 - Enable Debug Mode: Add X-CloudFlow-Debug: true header to requests to receive detailed debug information in responses. Debug mode provides request ID, processing time breakdown, and backend service information.\n",
    "\n",
    "Step 4 - Test with curl: Isolate issues by testing with curl commands. Example: curl -H \"Authorization: Bearer YOUR_API_KEY\" -H \"X-CloudFlow-Debug: true\" https://api.cloudflow.io/v1/resources\n",
    "\n",
    "Step 5 - Check Network Connectivity: Ensure your network allows outbound HTTPS traffic to *.cloudflow.io on port 443. Verify DNS resolution is working correctly.\n",
    "\n",
    "Step 6 - Verify SDK Version: If using CloudFlow SDK, ensure you're running the latest version. Outdated SDKs may not support new API features or may have known bugs.\n",
    "\n",
    "Step 7 - Contact Support: If issues persist, contact CloudFlow support with the request ID from failed requests. Support responds within 24 hours for Standard tier, 4 hours for Premium, 1 hour for Enterprise.\"\"\",\n",
    "        metadata={\"source\": \"debugging_guide\", \"topic\": \"troubleshooting\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Support Escalation Process\n",
    "\n",
    "Understanding CloudFlow's support escalation process ensures your issues are resolved efficiently.\n",
    "\n",
    "Support Channels:\n",
    "- Community Forums (All tiers): community.cloudflow.io - Best for general questions, feature requests, and sharing knowledge\n",
    "- Email Support (Premium & Enterprise): support@cloudflow.io - Include account ID and request ID in subject line\n",
    "- Phone Support (Enterprise only): +1-888-CLOUDFLOW - Available 24/7 for critical issues\n",
    "- Slack Channel (Enterprise only): Direct access to engineering team\n",
    "\n",
    "Issue Severity Levels:\n",
    "- P0 (Critical): Complete service outage affecting production. Response time: 1 hour for Enterprise, 4 hours for Premium\n",
    "- P1 (High): Major functionality impaired but workarounds available. Response time: 4 hours for Enterprise, 8 hours for Premium\n",
    "- P2 (Medium): Minor functionality issues with workarounds. Response time: 24 hours\n",
    "- P3 (Low): Questions, feature requests, documentation issues. Response time: 48 hours\n",
    "\n",
    "Escalation Path: If your issue isn't resolved within SLA, it automatically escalates to the next support tier. Enterprise customers can request immediate escalation to engineering team.\n",
    "\n",
    "Required Information: Include account ID, request ID, error messages, timestamps, steps to reproduce, and expected vs actual behavior. Screenshots and API logs are helpful.\"\"\",\n",
    "        metadata={\"source\": \"support_escalation\", \"topic\": \"troubleshooting\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"âœ“ Created {len(cloudflow_docs)} CloudFlow documentation documents\")\n",
    "print(\"\\nDocument breakdown by category:\")\n",
    "for topic in [\"architecture\", \"api\", \"security\", \"pricing\", \"best_practices\", \"troubleshooting\"]:\n",
    "    count = len([doc for doc in cloudflow_docs if doc.metadata[\"topic\"] == topic])\n",
    "    print(f\"  - {topic.title()}: {count} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Preview Sample Documents\n",
    "\n",
    "Let's look at a sample document to understand the structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Document:\n",
      "================================================================================\n",
      "Content preview (first 300 chars):\n",
      "CloudFlow Architecture Overview\n",
      "\n",
      "CloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\n",
      "\n",
      "The API Gateway layer handles all incoming requests using OAuth 2.0 authenticat...\n",
      "\n",
      "Metadata: {'source': 'architecture_overview', 'topic': 'architecture', 'difficulty': 'intermediate'}\n",
      "================================================================================\n",
      "\n",
      "Dataset statistics:\n",
      "  Total documents: 17\n",
      "  Total characters: 20,987\n",
      "  Average document length: 1234 characters\n"
     ]
    }
   ],
   "source": [
    "# Preview first document\n",
    "sample_doc = cloudflow_docs[0]\n",
    "\n",
    "print(\"Sample Document:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Content preview (first 300 chars):\\n{sample_doc.page_content[:300]}...\\n\")\n",
    "print(f\"Metadata: {sample_doc.metadata}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate total characters\n",
    "total_chars = sum(len(doc.page_content) for doc in cloudflow_docs)\n",
    "avg_chars = total_chars / len(cloudflow_docs)\n",
    "\n",
    "print(f\"\\nDataset statistics:\")\n",
    "print(f\"  Total documents: {len(cloudflow_docs)}\")\n",
    "print(f\"  Total characters: {total_chars:,}\")\n",
    "print(f\"  Average document length: {int(avg_chars)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: Basic RAG Pipeline Implementation\n",
    "\n",
    "**Difficulty:** ðŸŽ“ INTERMEDIATE\n",
    "\n",
    "Now we'll implement a complete RAG pipeline following LangChain 1.0+ best practices. This includes:\n",
    "\n",
    "1. Text splitting with optimal chunking\n",
    "2. Embeddings generation\n",
    "3. Vector store creation and persistence\n",
    "4. Retriever configuration\n",
    "5. LLM setup\n",
    "6. Prompt engineering\n",
    "7. LCEL chain construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Text Splitting\n",
    "\n",
    "We'll use `RecursiveCharacterTextSplitter` - the recommended splitter for most use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Split 17 documents into 34 chunks\n",
      "\n",
      "Chunk statistics:\n",
      "  Average chunk size: 616 characters\n",
      "  Min chunk size: 105 characters\n",
      "  Max chunk size: 1012 characters\n",
      "\n",
      "Sample chunk:\n",
      "Content: CloudFlow Architecture Overview\n",
      "\n",
      "CloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scala...\n",
      "Metadata: {'source': 'architecture_overview', 'topic': 'architecture', 'difficulty': 'intermediate'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize text splitter with optimal parameters\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,           # ~200-250 tokens, optimal for most Q&A tasks\n",
    "    chunk_overlap=128,         # 12.5% overlap maintains context continuity\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Prioritize paragraph > line > word boundaries\n",
    ")\n",
    "\n",
    "# Split all CloudFlow documents into chunks\n",
    "chunks = text_splitter.split_documents(cloudflow_docs)\n",
    "\n",
    "print(f\"âœ“ Split {len(cloudflow_docs)} documents into {len(chunks)} chunks\")\n",
    "print(f\"\\nChunk statistics:\")\n",
    "chunk_lengths = [len(chunk.page_content) for chunk in chunks]\n",
    "print(f\"  Average chunk size: {int(np.mean(chunk_lengths))} characters\")\n",
    "print(f\"  Min chunk size: {min(chunk_lengths)} characters\")\n",
    "print(f\"  Max chunk size: {max(chunk_lengths)} characters\")\n",
    "\n",
    "# Preview a chunk\n",
    "print(f\"\\nSample chunk:\")\n",
    "print(f\"Content: {chunks[0].page_content[:200]}...\")\n",
    "print(f\"Metadata: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Create Embeddings and Vector Store\n",
    "\n",
    "We'll use OpenAI's `text-embedding-3-small` model - cost-effective and performant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS vector store...\n",
      "  Embedding 34 chunks...\n",
      "âœ“ FAISS vector store created and saved to './ragas_evaluation_faiss'\n",
      "âœ“ Indexed 34 document chunks\n",
      "\n",
      "Vector store can be reloaded with:\n",
      "  vectorstore = FAISS.load_local('./ragas_evaluation_faiss', embeddings, allow_dangerous_deserialization=True)\n"
     ]
    }
   ],
   "source": [
    "# Initialize embeddings model\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # 1536 dimensions\n",
    "    # Cost: $0.00002 per 1K tokens\n",
    ")\n",
    "\n",
    "print(\"Creating FAISS vector store...\")\n",
    "print(f\"  Embedding {len(chunks)} chunks...\")\n",
    "\n",
    "# Create FAISS vector store from chunks\n",
    "# This may take 30-60 seconds depending on number of chunks\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Save vector store to disk for reusability\n",
    "vectorstore_path = \"./ragas_evaluation_faiss\"\n",
    "vectorstore.save_local(vectorstore_path)\n",
    "\n",
    "print(f\"âœ“ FAISS vector store created and saved to '{vectorstore_path}'\")\n",
    "print(f\"âœ“ Indexed {len(chunks)} document chunks\")\n",
    "print(\"\\nVector store can be reloaded with:\")\n",
    "print(f\"  vectorstore = FAISS.load_local('{vectorstore_path}', embeddings, allow_dangerous_deserialization=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Create Retriever\n",
    "\n",
    "Configure the retriever with similarity search and k=4 (returns top 4 most relevant chunks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Retriever configured successfully\n",
      "  Search type: similarity\n",
      "  Number of results (k): 4\n",
      "\n",
      "Testing retriever with query: 'What is CloudFlow's uptime SLA?'\n",
      "âœ“ Retrieved 4 documents\n",
      "\n",
      "First retrieved chunk preview:\n",
      "CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics....\n"
     ]
    }
   ],
   "source": [
    "# Create retriever from vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",  # Simple similarity search (vs MMR for diversity)\n",
    "    search_kwargs={\"k\": 4}     # Retrieve top 4 most relevant chunks\n",
    ")\n",
    "\n",
    "print(\"âœ“ Retriever configured successfully\")\n",
    "print(\"  Search type: similarity\")\n",
    "print(\"  Number of results (k): 4\")\n",
    "\n",
    "# Test retriever with a sample query\n",
    "test_query = \"What is CloudFlow's uptime SLA?\"\n",
    "print(f\"\\nTesting retriever with query: '{test_query}'\")\n",
    "\n",
    "retrieved_docs = retriever.invoke(test_query)\n",
    "print(f\"âœ“ Retrieved {len(retrieved_docs)} documents\")\n",
    "print(f\"\\nFirst retrieved chunk preview:\")\n",
    "print(f\"{retrieved_docs[0].page_content[:250]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Initialize Language Model\n",
    "\n",
    "We'll use GPT-4o-mini for cost-effective, high-quality responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Language model configured\n",
      "  Model: gpt-4o-mini\n",
      "  Temperature: 0 (deterministic)\n",
      "  Max tokens: 500\n",
      "\n",
      "LLM test: LLM is ready!\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChatOpenAI with GPT-4o-mini\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,        # Deterministic responses for consistent evaluation\n",
    "    max_tokens=500       # Reasonable limit for Q&A responses\n",
    ")\n",
    "\n",
    "print(\"âœ“ Language model configured\")\n",
    "print(\"  Model: gpt-4o-mini\")\n",
    "print(\"  Temperature: 0 (deterministic)\")\n",
    "print(\"  Max tokens: 500\")\n",
    "\n",
    "# Quick test\n",
    "test_response = llm.invoke(\"Say 'LLM is ready!'\")\n",
    "print(f\"\\nLLM test: {test_response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Create Prompt Template\n",
    "\n",
    "Design a prompt that instructs the LLM to answer based on context and handle cases where information is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Prompt template created\n",
      "\n",
      "Prompt structure:\n",
      "  1. System instruction (CloudFlow assistant role)\n",
      "  2. Context from retrieved documents\n",
      "  3. User question\n",
      "  4. Answer placeholder\n"
     ]
    }
   ],
   "source": [
    "# Define prompt template for CloudFlow assistant\n",
    "template = \"\"\"You are a helpful assistant for CloudFlow Platform documentation.\n",
    "Answer the question based on the following context. If you cannot answer based on\n",
    "the context, say \"I don't have enough information to answer that question.\"\n",
    "\n",
    "Be concise and accurate. Include specific details like numbers, limits, and technical\n",
    "specifications when available in the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Create ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "print(\"âœ“ Prompt template created\")\n",
    "print(\"\\nPrompt structure:\")\n",
    "print(\"  1. System instruction (CloudFlow assistant role)\")\n",
    "print(\"  2. Context from retrieved documents\")\n",
    "print(\"  3. User question\")\n",
    "print(\"  4. Answer placeholder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Build RAG Chain with LCEL\n",
    "\n",
    "Now we'll combine all components using LangChain Expression Language (LCEL) - the modern way to build chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ RAG chain created using LCEL\n",
      "\n",
      "Chain flow:\n",
      "  1. Question â†’ Retriever â†’ Get relevant documents\n",
      "  2. Documents â†’ format_docs â†’ Create context string\n",
      "  3. Context + Question â†’ Prompt template\n",
      "  4. Prompt â†’ LLM â†’ Generate answer\n",
      "  5. LLM output â†’ StrOutputParser â†’ Extract text\n"
     ]
    }
   ],
   "source": [
    "# Helper function to format retrieved documents\n",
    "def format_docs(docs):\n",
    "    \"\"\"Combine retrieved document chunks into a single context string.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build RAG chain using LCEL (pipe operator)\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"âœ“ RAG chain created using LCEL\")\n",
    "print(\"\\nChain flow:\")\n",
    "print(\"  1. Question â†’ Retriever â†’ Get relevant documents\")\n",
    "print(\"  2. Documents â†’ format_docs â†’ Create context string\")\n",
    "print(\"  3. Context + Question â†’ Prompt template\")\n",
    "print(\"  4. Prompt â†’ LLM â†’ Generate answer\")\n",
    "print(\"  5. LLM output â†’ StrOutputParser â†’ Extract text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Test the RAG Pipeline\n",
    "\n",
    "Let's verify our RAG pipeline works correctly before evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAG pipeline with sample questions:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Question 1: What is CloudFlow's uptime SLA?\n",
      "Answer: CloudFlow guarantees a 99.99% uptime SLA for the Enterprise Tier, 99.95% for the Premium Tier, and 99.9% for the Standard Tier.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question 2: How do I authenticate with CloudFlow APIs?\n",
      "Answer: You can authenticate with CloudFlow APIs using two methods: OAuth 2.0 and API Keys.\n",
      "\n",
      "1. **OAuth 2.0**: This method is recommended for user-facing applications. To implement it, direct users to the authorization endpoint at `https://auth.cloudflow.io/oauth/authorize` with your `client_id` and `redirect_uri` parameters. Access tokens are valid for 1 hour, and refresh tokens are valid for 30 days.\n",
      "\n",
      "2. **API Keys**: This method is ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters and never expires unless explicitly revoked. To authenticate requests, include your API key in the Authorization header as follows: `Authorization: Bearer YOUR_API_KEY`. \n",
      "\n",
      "All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question 3: What are the pricing tiers?\n",
      "Answer: CloudFlow offers three pricing tiers:\n",
      "\n",
      "1. **Standard Tier** ($99/month):\n",
      "   - 1,000 API requests per hour\n",
      "   - 100 GB storage included\n",
      "   - 10 GB bandwidth per month\n",
      "   - Community support via forums\n",
      "   - 99.9% uptime SLA\n",
      "   - Up to 5 team members\n",
      "\n",
      "2. **Premium Tier** ($499/month):\n",
      "   - 10,000 API requests per hour\n",
      "   - 1 TB storage included\n",
      "   - 100 GB bandwidth per month\n",
      "   - Email support with 24-hour response time\n",
      "   - 99.95% uptime SLA\n",
      "   - Up to 25 team members\n",
      "   - Advanced monitoring and alerting\n",
      "   - Custom domain support\n",
      "\n",
      "3. **Enterprise Tier** (Custom pricing):\n",
      "   - Custom API rate limits (100,000+ requests/hour)\n",
      "   - Unlimited storage and bandwidth\n",
      "   - 24/7 phone and email support with 1-hour response time\n",
      "   - 99.99% uptime SLA with service credits\n",
      "   - Unlimited team members\n",
      "   - Dedicated account manager\n",
      "   - Custom integrations and professional services\n",
      "   - Private cloud deployment options\n",
      "\n",
      "All tiers include SSL certificates, daily backups, API access, and dashboard analytics. Annual billing provides a 15% discount.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "âœ“ RAG pipeline is working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Test questions to verify RAG pipeline\n",
    "test_questions = [\n",
    "    \"What is CloudFlow's uptime SLA?\",\n",
    "    \"How do I authenticate with CloudFlow APIs?\",\n",
    "    \"What are the pricing tiers?\"\n",
    "]\n",
    "\n",
    "print(\"Testing RAG pipeline with sample questions:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\nQuestion {i}: {question}\")\n",
    "    answer = rag_chain.invoke(question)\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nâœ“ RAG pipeline is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4: Introduction to Ragas Framework\n",
    "\n",
    "**Difficulty:** ðŸ”° BEGINNER\n",
    "\n",
    "## What is Ragas?\n",
    "\n",
    "**Ragas** (Retrieval Augmented Generation Assessment) is the industry-standard framework for evaluating RAG systems. It provides:\n",
    "\n",
    "- Automated evaluation metrics for RAG quality\n",
    "- LLM-based and traditional metrics\n",
    "- Integration with popular frameworks (LangChain, LlamaIndex)\n",
    "- Comprehensive evaluation across retrieval and generation\n",
    "\n",
    "## Why Evaluate RAG Systems?\n",
    "\n",
    "Without evaluation, you can't:\n",
    "- âŒ Know if answers are accurate and grounded\n",
    "- âŒ Compare different configurations (chunk size, k value, etc.)\n",
    "- âŒ Identify failure patterns\n",
    "- âŒ Optimize for production deployment\n",
    "- âŒ Track quality over time\n",
    "\n",
    "## Core Ragas Metrics (6 Non-Multimodal Metrics)\n",
    "\n",
    "We'll implement all 6 core non-multimodal metrics in this notebook:\n",
    "\n",
    "### 1. **Faithfulness** ðŸ”\n",
    "- **What it measures**: Are the answers grounded in the retrieved context?\n",
    "- **How it works**: Uses LLM to verify if answer statements can be inferred from context\n",
    "- **Why it matters**: Prevents hallucinations\n",
    "- **Target score**: >0.7\n",
    "- **Speed**: Slow (~2-3s per question, requires LLM calls)\n",
    "\n",
    "### 2. **Answer Relevancy (Response Relevancy)** ðŸ“Š\n",
    "- **What it measures**: Is the answer relevant to the question?\n",
    "- **How it works**: Uses embeddings to measure semantic relevance\n",
    "- **Why it matters**: Ensures on-topic responses\n",
    "- **Target score**: >0.7\n",
    "- **Speed**: Fast (~0.5s per question)\n",
    "\n",
    "### 3. **Context Precision** ðŸŽ¯\n",
    "- **What it measures**: Are the most relevant chunks ranked highest?\n",
    "- **How it works**: Checks if relevant chunks appear early in retrieval results\n",
    "- **Why it matters**: Evaluates retrieval algorithm quality\n",
    "- **Target score**: >0.6\n",
    "- **Speed**: Medium (~1s per question)\n",
    "\n",
    "### 4. **Context Recall** âœ…\n",
    "- **What it measures**: Was all necessary information retrieved?\n",
    "- **How it works**: Compares retrieved contexts against ground truth\n",
    "- **Why it matters**: Ensures complete information coverage\n",
    "- **Target score**: >0.6\n",
    "- **Speed**: Medium (~1s per question)\n",
    "\n",
    "### 5. **Context Entity Recall** ðŸ·ï¸\n",
    "- **What it measures**: Are key entities from ground truth present in retrieved contexts?\n",
    "- **How it works**: Extracts entities from ground truth and checks presence in contexts\n",
    "- **Why it matters**: Ensures important named entities are retrieved\n",
    "- **Target score**: >0.6\n",
    "- **Speed**: Medium (~1-2s per question)\n",
    "\n",
    "### 6. **Noise Sensitivity** ðŸŽšï¸\n",
    "- **What it measures**: Is the system robust to irrelevant/noisy context?\n",
    "- **How it works**: Tests if system can ignore distractors in retrieved contexts\n",
    "- **Why it matters**: Evaluates robustness in real-world scenarios with imperfect retrieval\n",
    "- **Target score**: >0.7\n",
    "- **Speed**: Slow (~2-3s per question)\n",
    "\n",
    "## Metric Types\n",
    "\n",
    "**LLM-Based Metrics** (faithfulness, context_precision, context_recall, context_entity_recall, noise_sensitivity):\n",
    "- More accurate, closer to human evaluation\n",
    "- Slower and more expensive (requires LLM calls)\n",
    "- Slightly non-deterministic\n",
    "\n",
    "**Embedding-Based Metrics** (answer_relevancy):\n",
    "- Faster and cheaper\n",
    "- Deterministic\n",
    "- Good for rapid iteration\n",
    "\n",
    "**Note**: We're excluding multimodal metrics (Multimodal Faithfulness, Multimodal Relevance) as they're for image/video content evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Ragas Architecture Overview\n",
    "\n",
    "Here's how Ragas fits into the evaluation workflow:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Test Questions â”‚\n",
    "â”‚  + Ground Truth â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   RAG Pipeline  â”‚ â† Your system\n",
    "â”‚  (retrieval +   â”‚\n",
    "â”‚   generation)   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â”‚ Generates: answers + contexts\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Ragas Evaluator â”‚\n",
    "â”‚  6 Metrics      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Evaluation      â”‚\n",
    "â”‚ Scores          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 5: Creating Evaluation Dataset\n",
    "\n",
    "**Difficulty:** ðŸŽ“ INTERMEDIATE\n",
    "\n",
    "To evaluate our RAG system, we need:\n",
    "1. **Test questions** - Representative user queries\n",
    "2. **Ground truth answers** - Reference answers for comparison\n",
    "3. **Generated answers** - From our RAG pipeline\n",
    "4. **Retrieved contexts** - The chunks used to generate answers\n",
    "\n",
    "## 5.1 Create Test Questions with Ground Truth\n",
    "\n",
    "We'll create 18 diverse questions covering different categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 18 test questions with ground truth answers\n",
      "\n",
      "Question breakdown by category:\n",
      "  - Simple Factual: 5 questions\n",
      "  - Multi-Fact: 4 questions\n",
      "  - Procedural: 3 questions\n",
      "  - Comparison: 2 questions\n",
      "  - Troubleshooting: 2 questions\n",
      "  - Edge Cases: 2 questions\n"
     ]
    }
   ],
   "source": [
    "# Test cases: question + ground truth answer pairs\n",
    "test_cases = [\n",
    "    # ========== SIMPLE FACTUAL (5) ==========\n",
    "    {\n",
    "        \"question\": \"What is CloudFlow's uptime SLA?\",\n",
    "        \"ground_truth\": \"CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What authentication protocol does CloudFlow use?\",\n",
    "        \"ground_truth\": \"CloudFlow uses OAuth 2.0 for user-facing applications and API keys for server-to-server communication.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the service mesh technology used by CloudFlow?\",\n",
    "        \"ground_truth\": \"CloudFlow uses Istio as the service mesh technology to orchestrate communication between microservices.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What compliance standards does CloudFlow support?\",\n",
    "        \"ground_truth\": \"CloudFlow supports SOC 2 Type II, GDPR, HIPAA, ISO 27001, and PCI DSS Level 1 compliance standards.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How long are CloudFlow audit logs retained?\",\n",
    "        \"ground_truth\": \"CloudFlow audit logs are immutable and retained for 2 years.\"\n",
    "    },\n",
    "    \n",
    "    # ========== MULTI-FACT (4) ==========\n",
    "    {\n",
    "        \"question\": \"What are the three main layers of CloudFlow architecture?\",\n",
    "        \"ground_truth\": \"The three main layers are: API Gateway layer (handles authentication and routing), Service Mesh layer (orchestrates microservices), and Data Storage layer (distributed database with replication).\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are CloudFlow's pricing tiers and their API rate limits?\",\n",
    "        \"ground_truth\": \"Standard tier costs $99/month with 1,000 requests/hour, Premium tier costs $499/month with 10,000 requests/hour, and Enterprise tier has custom pricing with 100,000+ requests/hour.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What HTTP status codes indicate authentication failures in CloudFlow API?\",\n",
    "        \"ground_truth\": \"401 Unauthorized indicates missing or invalid API key, and 403 Forbidden indicates valid API key but insufficient permissions.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What auto-scaling metrics does CloudFlow monitor?\",\n",
    "        \"ground_truth\": \"CloudFlow monitors CPU utilization (target 70%), memory usage (target 80%), and triggers scaling when thresholds are exceeded for more than 3 consecutive minutes.\"\n",
    "    },\n",
    "    \n",
    "    # ========== PROCEDURAL (3) ==========\n",
    "    {\n",
    "        \"question\": \"How do I authenticate with CloudFlow APIs using an API key?\",\n",
    "        \"ground_truth\": \"Include your API key in the Authorization header as 'Authorization: Bearer YOUR_API_KEY'. All requests must be made over HTTPS, and API keys have the format 'cf_live_' followed by 32 alphanumeric characters.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do I handle rate limit errors in CloudFlow?\",\n",
    "        \"ground_truth\": \"When you receive a 429 error, implement exponential backoff in retry logic, check the X-RateLimit-Reset header to know when limits reset, and use the Retry-After header to determine wait time (1s, then 2s, then 4s, etc.).\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What steps should I follow to optimize CloudFlow API performance?\",\n",
    "        \"ground_truth\": \"Implement caching with Redis (TTL 5-60 minutes), batch multiple operations into single API calls, use pagination for large result sets (50-100 items), enable request compression with gzip, and maintain a connection pool with 5-10 concurrent connections.\"\n",
    "    },\n",
    "    \n",
    "    # ========== COMPARISON (2) ==========\n",
    "    {\n",
    "        \"question\": \"What's the difference between Standard and Premium tier rate limits?\",\n",
    "        \"ground_truth\": \"Standard tier allows 1,000 requests per hour with 100 requests per minute burst, while Premium tier allows 10,000 requests per hour with 500 requests per minute burst. Premium also includes priority request processing.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does OAuth 2.0 authentication differ from API key authentication in CloudFlow?\",\n",
    "        \"ground_truth\": \"OAuth 2.0 is recommended for user-facing applications with access tokens valid for 1 hour and provides the Authorization Code flow, while API keys are ideal for server-to-server communication, never expire unless revoked, and have a simpler implementation.\"\n",
    "    },\n",
    "    \n",
    "    # ========== TROUBLESHOOTING (2) ==========\n",
    "    {\n",
    "        \"question\": \"What should I do if I receive a 504 timeout error?\",\n",
    "        \"ground_truth\": \"Increase client timeout to at least 30 seconds, use async endpoints for long-running operations and poll for results, and check the CloudFlow status page for any service degradation.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do I debug slow API response times in CloudFlow?\",\n",
    "        \"ground_truth\": \"Add X-CloudFlow-Debug: true header to requests for detailed debug information, review API logs in the dashboard under Analytics > API Logs, test with curl commands, and verify network connectivity to *.cloudflow.io on port 443.\"\n",
    "    },\n",
    "    \n",
    "    # ========== EDGE CASES (2) ==========\n",
    "    {\n",
    "        \"question\": \"What happens if I use an expired OAuth token?\",\n",
    "        \"ground_truth\": \"If you use an expired access token, you'll receive a 401 Unauthorized error. You should use your refresh token to obtain a new access token. Access tokens are valid for 1 hour and refresh tokens are valid for 30 days.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Does CloudFlow support blockchain integration?\",\n",
    "        \"ground_truth\": \"I don't have enough information to answer that question.\"  # Tests 'I don't know' handling\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"âœ“ Created {len(test_cases)} test questions with ground truth answers\\n\")\n",
    "print(\"Question breakdown by category:\")\n",
    "print(\"  - Simple Factual: 5 questions\")\n",
    "print(\"  - Multi-Fact: 4 questions\")\n",
    "print(\"  - Procedural: 3 questions\")\n",
    "print(\"  - Comparison: 2 questions\")\n",
    "print(\"  - Troubleshooting: 2 questions\")\n",
    "print(\"  - Edge Cases: 2 questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Generate Answers and Capture Contexts\n",
    "\n",
    "Now we'll run our RAG pipeline on each question and capture:\n",
    "- The generated answer\n",
    "- The retrieved context chunks used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answers and capturing contexts for all test questions...\n",
      "\n",
      "[1/18] Processing: What is CloudFlow's uptime SLA?...\n",
      "[2/18] Processing: What authentication protocol does CloudFlow use?...\n",
      "[3/18] Processing: What is the service mesh technology used by CloudFlow?...\n",
      "[4/18] Processing: What compliance standards does CloudFlow support?...\n",
      "[5/18] Processing: How long are CloudFlow audit logs retained?...\n",
      "[6/18] Processing: What are the three main layers of CloudFlow architecture?...\n",
      "[7/18] Processing: What are CloudFlow's pricing tiers and their API rate limits...\n",
      "[8/18] Processing: What HTTP status codes indicate authentication failures in C...\n",
      "[9/18] Processing: What auto-scaling metrics does CloudFlow monitor?...\n",
      "[10/18] Processing: How do I authenticate with CloudFlow APIs using an API key?...\n",
      "[11/18] Processing: How do I handle rate limit errors in CloudFlow?...\n",
      "[12/18] Processing: What steps should I follow to optimize CloudFlow API perform...\n",
      "[13/18] Processing: What's the difference between Standard and Premium tier rate...\n",
      "[14/18] Processing: How does OAuth 2.0 authentication differ from API key authen...\n",
      "[15/18] Processing: What should I do if I receive a 504 timeout error?...\n",
      "[16/18] Processing: How do I debug slow API response times in CloudFlow?...\n",
      "[17/18] Processing: What happens if I use an expired OAuth token?...\n",
      "[18/18] Processing: Does CloudFlow support blockchain integration?...\n",
      "\n",
      "âœ“ Generated 18 answer-context pairs\n",
      "\n",
      "Dataset structure:\n",
      "  - user_input: 18 questions\n",
      "  - reference: 18 ground truth answers\n",
      "  - response: 18 RAG-generated answers\n",
      "  - retrieved_contexts: 18 context lists\n"
     ]
    }
   ],
   "source": [
    "# Generate evaluation dataset by running RAG pipeline\n",
    "print(\"Generating answers and capturing contexts for all test questions...\\n\")\n",
    "\n",
    "# Prepare dataset structure (required by Ragas)\n",
    "evaluation_data = {\n",
    "    \"user_input\": [],        # Questions\n",
    "    \"reference\": [],         # Ground truth answers\n",
    "    \"response\": [],          # RAG-generated answers\n",
    "    \"retrieved_contexts\": [] # Retrieved chunks (List[List[str]])\n",
    "}\n",
    "\n",
    "# Process each test case\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    question = test_case[\"question\"]\n",
    "    ground_truth = test_case[\"ground_truth\"]\n",
    "    \n",
    "    print(f\"[{i}/{len(test_cases)}] Processing: {question[:60]}...\")\n",
    "    \n",
    "    # Generate answer using RAG chain\n",
    "    answer = rag_chain.invoke(question)\n",
    "    \n",
    "    # Get retrieved contexts (same retriever as in RAG chain)\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    contexts = [doc.page_content for doc in retrieved_docs]\n",
    "    \n",
    "    # Append to dataset\n",
    "    evaluation_data[\"user_input\"].append(question)\n",
    "    evaluation_data[\"reference\"].append(ground_truth)\n",
    "    evaluation_data[\"response\"].append(answer)\n",
    "    evaluation_data[\"retrieved_contexts\"].append(contexts)\n",
    "\n",
    "print(f\"\\nâœ“ Generated {len(evaluation_data['user_input'])} answer-context pairs\")\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(f\"  - user_input: {len(evaluation_data['user_input'])} questions\")\n",
    "print(f\"  - reference: {len(evaluation_data['reference'])} ground truth answers\")\n",
    "print(f\"  - response: {len(evaluation_data['response'])} RAG-generated answers\")\n",
    "print(f\"  - retrieved_contexts: {len(evaluation_data['retrieved_contexts'])} context lists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Convert to Ragas Format\n",
    "\n",
    "Ragas requires data in `EvaluationDataset` format with `SingleTurnSample` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Converted to Ragas EvaluationDataset format\n",
      "  Total samples: 18\n",
      "\n",
      "Sample preview:\n",
      "  Question: What is CloudFlow's uptime SLA?\n",
      "  Answer: CloudFlow guarantees a 99.99% uptime SLA for the Enterprise Tier, 99.95% for the Premium Tier, and 9...\n",
      "  Contexts: 4 chunks retrieved\n"
     ]
    }
   ],
   "source": [
    "# Convert to Ragas EvaluationDataset format\n",
    "#import pickle\n",
    "\n",
    "samples = [\n",
    "    SingleTurnSample(\n",
    "        user_input=evaluation_data[\"user_input\"][i],\n",
    "        response=evaluation_data[\"response\"][i],\n",
    "        retrieved_contexts=evaluation_data[\"retrieved_contexts\"][i],\n",
    "        reference=evaluation_data[\"reference\"][i]\n",
    "    )\n",
    "    for i in range(len(evaluation_data[\"user_input\"]))\n",
    "]\n",
    "\n",
    "eval_dataset = EvaluationDataset(samples=samples)\n",
    "# Save to CSV\n",
    "#eval_dataset.to_csv(\"my_eval_dataset.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# # After creating eval_dataset\n",
    "# with open(\"my_eval_dataset.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(eval_dataset, f)\n",
    "\n",
    "# print(\"âœ… Dataset saved as pickle\")\n",
    "\n",
    "# Load from CSV\n",
    "#loaded_dataset = EvaluationDataset.from_csv(\"my_eval_dataset.csv\")\n",
    "\n",
    "print(\"âœ“ Converted to Ragas EvaluationDataset format\")\n",
    "print(f\"  Total samples: {len(eval_dataset)}\")\n",
    "print(f\"\\nSample preview:\")\n",
    "print(f\"  Question: {samples[0].user_input}\")\n",
    "print(f\"  Answer: {samples[0].response[:100]}...\")\n",
    "print(f\"  Contexts: {len(samples[0].retrieved_contexts)} chunks retrieved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 6: Ragas Metrics Implementation\n",
    "\n",
    "**Difficulty:** ðŸŽ“/ðŸš€ INTERMEDIATE/ADVANCED\n",
    "\n",
    "Now we'll configure and explain all 6 non-multimodal Ragas metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Configure Metrics\n",
    "\n",
    "We'll set up all 6 non-multimodal metrics with proper LLM and embeddings configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ragas.llms import LangchainLLMWrapper\n",
    "#from langchain_openai import ChatOpenAI\n",
    "\n",
    "#llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=100))\n",
    "#print(ragas_llm.langchain_llm.max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "# evaluator_llm = ChatOpenAI(\n",
    "#       model=\"gpt-4o-mini\",\n",
    "#       temperature=0,\n",
    "#       max_tokens=2000,      # âœ… Increased from 500 to 2000\n",
    "#       request_timeout=120   # âœ… Prevent timeouts (2 minutes)\n",
    "# )\n",
    "# ragas_llm = LangchainLLMWrapper(evaluator_llm)\n",
    "# print(ragas_llm.langchain_llm.max_tokens)\n",
    "# #ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "\n",
    "# import openai\n",
    "\n",
    "# openai_client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# ragas_embeddings = embedding_factory(\n",
    "#     'openai', \n",
    "#     model='text-embedding-3-small', \n",
    "#     client=openai_client\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ragas.llms import llm_factory\n",
    "# from ragas.embeddings import embedding_factory\n",
    "# import openai\n",
    "\n",
    "# # Create OpenAI client\n",
    "# openai_client = openai.OpenAI(api_key=\"your-api-key\")\n",
    "\n",
    "# # Create LLM using llm_factory\n",
    "# ragas_llm = llm_factory(\n",
    "#     'gpt-4o-mini', \n",
    "#     client=openai_client\n",
    "# )\n",
    "\n",
    "# # Create embeddings using embedding_factory\n",
    "# ragas_embeddings = embedding_factory(\n",
    "#     'openai',\n",
    "#     model='text-embedding-3-small',\n",
    "#     client=openai_client\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ragas.llms import llm_factory\n",
    "# from ragas.embeddings import embedding_factory\n",
    "# import openai\n",
    "\n",
    "# # Create OpenAI client\n",
    "# openai_client = openai.OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "# # Create LLM using llm_factory\n",
    "# ragas_llm = llm_factory(\n",
    "#     'gpt-4o-mini', \n",
    "#     client=openai_client\n",
    "# )\n",
    "\n",
    "# # Create embeddings using embedding_factory\n",
    "# ragas_embeddings = embedding_factory(\n",
    "#     'openai',\n",
    "#     model='text-embedding-3-small',\n",
    "#     client=openai_client\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONFIGURING LLM & EMBEDDINGS (WITH FIX)\n",
      "================================================================================\n",
      "\n",
      "ðŸ”§ Creating SEPARATE evaluator LLM with increased limits...\n",
      "âœ… Evaluator LLM configured:\n",
      "   Model: gpt-4o-mini\n",
      "   Temperature: 0 (deterministic)\n",
      "   Max tokens: 2000 (sufficient for Ragas evaluations)\n",
      "   Request timeout: 120 seconds\n",
      "\n",
      "ðŸ”§ Creating embeddings model...\n",
      "âœ… Embeddings configured:\n",
      "   Model: text-embedding-3-small\n",
      "   Dimensions: 1536\n",
      "\n",
      "ðŸ”§ Wrapping LLM and embeddings for Ragas...\n",
      "âœ… Wrappers created successfully\n",
      "âœ“ Configured 6 Ragas metrics (v0.3.9) - All non-multimodal metrics\n",
      "\n",
      "Metric Details:\n",
      "================================================================================\n",
      "\n",
      "1. FAITHFULNESS\n",
      "   - Measures: Answer groundedness in retrieved context\n",
      "   - Type: LLM-based (uses GPT-4o-mini for verification)\n",
      "   - Initialization: Faithfulness(llm=ragas_llm)\n",
      "   - Target: >0.7\n",
      "   - Speed: ~2-3s per question\n",
      "   - Why: Prevents hallucinations and ensures factual accuracy\n",
      "\n",
      "2. ANSWER RELEVANCY (Response Relevancy)\n",
      "   - Measures: Semantic relevance of answer to question\n",
      "   - Type: Embedding-based (uses cosine similarity)\n",
      "   - Initialization: AnswerRelevancy(embeddings=ragas_embeddings)\n",
      "   - Target: >0.7\n",
      "   - Speed: ~0.5s per question\n",
      "   - Why: Ensures on-topic, focused responses\n",
      "\n",
      "3. CONTEXT PRECISION\n",
      "   - Measures: Ranking quality of retrieved chunks\n",
      "   - Type: LLM-based ground truth comparison\n",
      "   - Initialization: ContextPrecision(llm=ragas_llm)\n",
      "   - Target: >0.6\n",
      "   - Speed: ~1s per question\n",
      "   - Why: Evaluates retrieval algorithm effectiveness\n",
      "\n",
      "4. CONTEXT RECALL\n",
      "   - Measures: Completeness of retrieved information\n",
      "   - Type: LLM-based ground truth comparison\n",
      "   - Initialization: ContextRecall(llm=ragas_llm)\n",
      "   - Target: >0.6\n",
      "   - Speed: ~1s per question\n",
      "   - Why: Ensures no critical information is missed\n",
      "\n",
      "5. CONTEXT ENTITY RECALL\n",
      "   - Measures: Key entities from ground truth present in retrieved contexts\n",
      "   - Type: LLM-based entity extraction and matching\n",
      "   - Initialization: ContextEntityRecall(llm=ragas_llm)\n",
      "   - Target: >0.6\n",
      "   - Speed: ~1-2s per question\n",
      "   - Why: Ensures important named entities are retrieved\n",
      "\n",
      "6. NOISE SENSITIVITY\n",
      "   - Measures: Robustness to irrelevant/noisy context\n",
      "   - Type: LLM-based evaluation with added noise\n",
      "   - Initialization: NoiseSensitivity(llm=ragas_llm)\n",
      "   - Target: >0.7\n",
      "   - Speed: ~2-3s per question\n",
      "   - Why: Tests if system can ignore distractors\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Œ Note on Ragas 0.3.9:\n",
      "   All 6 non-multimodal metrics are now configured.\n",
      "   Metrics are initialized as classes with wrapped LLM/embeddings.\n",
      "   LangChain components must be wrapped using Ragas wrapper classes.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFIGURING LLM & EMBEDDINGS (WITH FIX)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ”§ Creating SEPARATE evaluator LLM with increased limits...\")\n",
    "\n",
    "# âœ… FIX: Create separate evaluator LLM with higher max_tokens\n",
    "evaluator_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,           # Deterministic for consistent evaluation\n",
    "    max_tokens=3000,         # âœ… INCREASED from 500 to 2000\n",
    "    request_timeout=480,      # âœ… ADDED 2-minute timeout to prevent hangs\n",
    "    n=3                       # âœ… ADDED for multiple responses\n",
    ")\n",
    "\n",
    "print(\"âœ… Evaluator LLM configured:\")\n",
    "print(f\"   Model: gpt-4o-mini\")\n",
    "print(f\"   Temperature: 0 (deterministic)\")\n",
    "print(f\"   Max tokens: 2000 (sufficient for Ragas evaluations)\")\n",
    "print(f\"   Request timeout: 120 seconds\")\n",
    "\n",
    "# Create embeddings\n",
    "print(\"\\nðŸ”§ Creating embeddings model...\")\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # 1536 dimensions, cost-effective\n",
    ")\n",
    "print(\"âœ… Embeddings configured:\")\n",
    "print(f\"   Model: text-embedding-3-small\")\n",
    "print(f\"   Dimensions: 1536\")\n",
    "\n",
    "# âœ… Wrap for Ragas compatibility\n",
    "print(\"\\nðŸ”§ Wrapping LLM and embeddings for Ragas...\")\n",
    "ragas_llm = LangchainLLMWrapper(evaluator_llm)\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "print(\"âœ… Wrappers created successfully\")\n",
    "\n",
    "# Initialize ALL 6 non-multimodal metrics with wrapped evaluator LLM and embeddings\n",
    "# Note: In Ragas 0.3.9, metrics are classes that need LLM/embeddings initialization\n",
    "\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    AnswerRelevancy,\n",
    "    ContextPrecision,\n",
    "    ContextRecall,\n",
    "    ContextEntityRecall,\n",
    "    NoiseSensitivity,\n",
    ")\n",
    "metrics = [\n",
    "    Faithfulness(llm=ragas_llm),                         # Are answers grounded in context?\n",
    "    AnswerRelevancy(llm=ragas_llm, embeddings=ragas_embeddings),        # Are answers relevant to questions?\n",
    "    ContextPrecision(llm=ragas_llm),                     # Are relevant chunks ranked high?\n",
    "    ContextRecall(llm=ragas_llm),                        # Was all needed info retrieved?\n",
    "    ContextEntityRecall(llm=ragas_llm),                  # Are key entities from ground truth retrieved?\n",
    "    NoiseSensitivity(llm=ragas_llm),                     # Is system robust to irrelevant context?\n",
    "]\n",
    "\n",
    "print(\"âœ“ Configured 6 Ragas metrics (v0.3.9) - All non-multimodal metrics\\n\")\n",
    "print(\"Metric Details:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n1. FAITHFULNESS\")\n",
    "print(\"   - Measures: Answer groundedness in retrieved context\")\n",
    "print(\"   - Type: LLM-based (uses GPT-4o-mini for verification)\")\n",
    "print(\"   - Initialization: Faithfulness(llm=ragas_llm)\")\n",
    "print(\"   - Target: >0.7\")\n",
    "print(\"   - Speed: ~2-3s per question\")\n",
    "print(\"   - Why: Prevents hallucinations and ensures factual accuracy\")\n",
    "\n",
    "print(\"\\n2. ANSWER RELEVANCY (Response Relevancy)\")\n",
    "print(\"   - Measures: Semantic relevance of answer to question\")\n",
    "print(\"   - Type: Embedding-based (uses cosine similarity)\")\n",
    "print(\"   - Initialization: AnswerRelevancy(embeddings=ragas_embeddings)\")\n",
    "print(\"   - Target: >0.7\")\n",
    "print(\"   - Speed: ~0.5s per question\")\n",
    "print(\"   - Why: Ensures on-topic, focused responses\")\n",
    "\n",
    "print(\"\\n3. CONTEXT PRECISION\")\n",
    "print(\"   - Measures: Ranking quality of retrieved chunks\")\n",
    "print(\"   - Type: LLM-based ground truth comparison\")\n",
    "print(\"   - Initialization: ContextPrecision(llm=ragas_llm)\")\n",
    "print(\"   - Target: >0.6\")\n",
    "print(\"   - Speed: ~1s per question\")\n",
    "print(\"   - Why: Evaluates retrieval algorithm effectiveness\")\n",
    "\n",
    "print(\"\\n4. CONTEXT RECALL\")\n",
    "print(\"   - Measures: Completeness of retrieved information\")\n",
    "print(\"   - Type: LLM-based ground truth comparison\")\n",
    "print(\"   - Initialization: ContextRecall(llm=ragas_llm)\")\n",
    "print(\"   - Target: >0.6\")\n",
    "print(\"   - Speed: ~1s per question\")\n",
    "print(\"   - Why: Ensures no critical information is missed\")\n",
    "\n",
    "print(\"\\n5. CONTEXT ENTITY RECALL\")\n",
    "print(\"   - Measures: Key entities from ground truth present in retrieved contexts\")\n",
    "print(\"   - Type: LLM-based entity extraction and matching\")\n",
    "print(\"   - Initialization: ContextEntityRecall(llm=ragas_llm)\")\n",
    "print(\"   - Target: >0.6\")\n",
    "print(\"   - Speed: ~1-2s per question\")\n",
    "print(\"   - Why: Ensures important named entities are retrieved\")\n",
    "\n",
    "print(\"\\n6. NOISE SENSITIVITY\")\n",
    "print(\"   - Measures: Robustness to irrelevant/noisy context\")\n",
    "print(\"   - Type: LLM-based evaluation with added noise\")\n",
    "print(\"   - Initialization: NoiseSensitivity(llm=ragas_llm)\")\n",
    "print(\"   - Target: >0.7\")\n",
    "print(\"   - Speed: ~2-3s per question\")\n",
    "print(\"   - Why: Tests if system can ignore distractors\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ“Œ Note on Ragas 0.3.9:\")\n",
    "print(\"   All 6 non-multimodal metrics are now configured.\")\n",
    "print(\"   Metrics are initialized as classes with wrapped LLM/embeddings.\")\n",
    "print(\"   LangChain components must be wrapped using Ragas wrapper classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Understanding Metric Scores\n",
    "\n",
    "**Score Interpretation:**\n",
    "\n",
    "| Score Range | Interpretation | Action |\n",
    "|-------------|----------------|--------|\n",
    "| 0.8 - 1.0 | Excellent | Production-ready |\n",
    "| 0.7 - 0.8 | Good | Minor tuning recommended |\n",
    "| 0.6 - 0.7 | Acceptable | Needs optimization |\n",
    "| 0.5 - 0.6 | Poor | Significant issues to address |\n",
    "| < 0.5 | Very Poor | Major redesign needed |\n",
    "\n",
    "**What to optimize if scores are low:**\n",
    "\n",
    "- **Low Faithfulness**: Improve prompt to emphasize context adherence, reduce temperature\n",
    "- **Low Answer Relevancy**: Improve prompt focus, filter irrelevant retrieved chunks\n",
    "- **Low Context Precision**: Optimize chunk size, adjust k value, improve metadata\n",
    "- **Low Context Recall**: Increase k value, improve chunking strategy, enhance retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 7: Running Comprehensive Evaluation\n",
    "\n",
    "**Difficulty:** ðŸŽ“ INTERMEDIATE\n",
    "\n",
    "Now we'll run Ragas evaluation on our baseline RAG configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Run Ragas Evaluation\n",
    "\n",
    "This will take ~7-12 minutes depending on the number of questions and metrics (6 metrics total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Ragas evaluation...\n",
      "  Dataset: 18 questions\n",
      "  Metrics: 6 (all 6 non-multimodal metrics)\n",
      "  Estimated time: 7-12 minutes\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9861dbeb9b47cfaefb0424e6f9a006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[5]: TimeoutError()\n",
      "Exception raised in Job[11]: TimeoutError()\n",
      "Exception raised in Job[17]: TimeoutError()\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[23]: TimeoutError()\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[29]: TimeoutError()\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[35]: TimeoutError()\n",
      "Exception raised in Job[41]: TimeoutError()\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[47]: TimeoutError()\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[53]: TimeoutError()\n",
      "Exception raised in Job[59]: TimeoutError()\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[65]: TimeoutError()\n",
      "Exception raised in Job[71]: TimeoutError()\n",
      "Exception raised in Job[77]: TimeoutError()\n",
      "Exception raised in Job[83]: TimeoutError()\n",
      "Exception raised in Job[89]: TimeoutError()\n",
      "Exception raised in Job[95]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Evaluation completed!\n",
      "\n",
      "================================================================================\n",
      "RAGAS EVALUATION RESULTS (6 Metrics)\n",
      "================================================================================\n",
      "{'faithfulness': 0.9029, 'answer_relevancy': 0.8278, 'context_precision': 0.8565, 'context_recall': 0.8333, 'context_entity_recall': 0.3859, 'noise_sensitivity(mode=relevant)': 0.5000}\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Ragas evaluation...\")\n",
    "print(f\"  Dataset: {len(eval_dataset)} questions\")\n",
    "print(f\"  Metrics: {len(metrics)} (all 6 non-multimodal metrics)\")\n",
    "print(f\"  Estimated time: 7-12 minutes\\n\")\n",
    "\n",
    "# Run evaluation (Ragas 0.3.7)\n",
    "# Note: In Ragas 0.3+, LLM and embeddings are configured on metrics directly,\n",
    "# so we don't need to pass them to evaluate()\n",
    "result = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Evaluation completed!\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RAGAS EVALUATION RESULTS (6 Metrics)\")\n",
    "print(\"=\" * 80)\n",
    "print(result)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Log Results to OPIK\n",
    "\n",
    "**Optional:** If OPIK is configured, log the evaluation results for experiment tracking and analysis.\n",
    "\n",
    "**What gets logged:**\n",
    "- All 6 metric scores (mean values)\n",
    "- Configuration parameters (chunk_size, k, search_type, etc.)\n",
    "- Model information (LLM, embeddings)\n",
    "- Dataset metadata (num_questions, num_documents)\n",
    "- Overall score (average of all metrics)\n",
    "\n",
    "**Benefits:**\n",
    "- Track experiments over time\n",
    "- Compare different RAG configurations\n",
    "- Share results with team\n",
    "- Visualize metric trends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Log Baseline Evaluation to OPIK ===\n",
    "if OPIK_AVAILABLE:\n",
    "    try:\n",
    "        # Convert result to DataFrame\n",
    "        results_df = result.to_pandas()\n",
    "        \n",
    "        # Calculate mean scores for all 6 metrics\n",
    "        metric_scores = {\n",
    "            'faithfulness': float(results_df['faithfulness'].mean()),\n",
    "            'answer_relevancy': float(results_df['answer_relevancy'].mean()),\n",
    "            'context_precision': float(results_df['context_precision'].mean()),\n",
    "            'context_recall': float(results_df['context_recall'].mean()),\n",
    "            'context_entity_recall': float(results_df['context_entity_recall'].mean()),\n",
    "            'noise_sensitivity': float(results_df['noise_sensitivity'].mean()),\n",
    "        }\n",
    "        \n",
    "        # Calculate overall average\n",
    "        overall_score = sum(metric_scores.values()) / len(metric_scores)\n",
    "        \n",
    "        # Log experiment to OPIK\n",
    "        opik_client.log_traces(\n",
    "            project_name=project_name,\n",
    "            name=\"Baseline_RAG_Evaluation\",\n",
    "            input={\"configuration\": \"baseline\", \"num_questions\": len(eval_dataset)},\n",
    "            output=metric_scores,\n",
    "            metadata={\n",
    "                \"ragas_version\": \"0.3.9\",\n",
    "                \"chunk_size\": 1024,\n",
    "                \"chunk_overlap\": 200,\n",
    "                \"retriever_k\": 4,\n",
    "                \"search_type\": \"similarity\",\n",
    "                \"embedding_model\": \"text-embedding-3-small\",\n",
    "                \"llm_model\": \"gpt-4o-mini\",\n",
    "                \"num_documents\": 18,\n",
    "                \"num_questions\": len(eval_dataset),\n",
    "                \"evaluation_date\": datetime.now().isoformat(),\n",
    "                \"overall_score\": overall_score,\n",
    "            },\n",
    "            tags=[\"baseline\", \"ragas\", \"rag_evaluation\", \"all_6_metrics\"]\n",
    "        )\n",
    "        \n",
    "        print(\"\\nâœ“ Logged baseline evaluation to OPIK\")\n",
    "        print(f\"  Overall Score: {overall_score:.4f}\")\n",
    "        print(f\"  Dashboard: https://www.comet.com/opik/projects/{project_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ OPIK logging failed: {e}\")\n",
    "else:\n",
    "    print(\"â„¹ OPIK not available - skipping logging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Analyze Results\n",
    "\n",
    "Let's convert results to a DataFrame for better analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Results by Question:\n",
      "\n",
      "                                                                            user_input                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           retrieved_contexts                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           response                                                                                                                                                                                                                                                         reference  faithfulness  answer_relevancy  context_precision  context_recall  context_entity_recall  noise_sensitivity(mode=relevant)\n",
      "0                                                      What is CloudFlow's uptime SLA?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         [CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics., CloudFlow Pricing Tiers\\n\\nCloudFlow offers three pricing tiers designed to meet the needs of individuals, teams, and enterprises.\\n\\nStandard Tier ($99/month):\\n- 1,000 API requests per hour\\n- 100 GB storage included\\n- 10 GB bandwidth per month\\n- Community support via forums\\n- 99.9% uptime SLA\\n- Up to 5 team members\\n\\nPremium Tier ($499/month):\\n- 10,000 API requests per hour\\n- 1 TB storage included\\n- 100 GB bandwidth per month\\n- Email support with 24-hour response time\\n- 99.95% uptime SLA\\n- Up to 25 team members\\n- Advanced monitoring and alerting\\n- Custom domain support\\n\\nEnterprise Tier (Custom pricing):\\n- Custom API rate limits (100,000+ requests/hour)\\n- Unlimited storage and bandwidth\\n- 24/7 phone and email support with 1-hour response time\\n- 99.99% uptime SLA with service credits\\n- Unlimited team members\\n- Dedicated account manager\\n- Custom integrations and professional services\\n- Private cloud deployment options, CloudFlow Disaster Recovery\\n\\nCloudFlow implements comprehensive disaster recovery capabilities to protect your data and ensure business continuity.\\n\\nBackup Strategy: CloudFlow performs automatic daily backups of all data at 2 AM UTC. Backups are encrypted and stored in geographically diverse locations. Retention: 30 days for Standard tier, 90 days for Premium, 1 year for Enterprise.\\n\\nPoint-in-Time Recovery: Enterprise customers can restore data to any point within the retention period with 5-minute granularity. Recovery operations typically complete within 15-30 minutes.\\n\\nMulti-Region Replication: Enable multi-region replication for critical data. Data is asynchronously replicated to a secondary region within 60 seconds. Failover to secondary region is automatic and takes approximately 5 minutes.\\n\\nBackup Verification: CloudFlow performs monthly backup restoration tests to ensure data recoverability. Test results are available in your compliance dashboard., CloudFlow Support Escalation Process\\n\\nUnderstanding CloudFlow's support escalation process ensures your issues are resolved efficiently.\\n\\nSupport Channels:\\n- Community Forums (All tiers): community.cloudflow.io - Best for general questions, feature requests, and sharing knowledge\\n- Email Support (Premium & Enterprise): support@cloudflow.io - Include account ID and request ID in subject line\\n- Phone Support (Enterprise only): +1-888-CLOUDFLOW - Available 24/7 for critical issues\\n- Slack Channel (Enterprise only): Direct access to engineering team\\n\\nIssue Severity Levels:\\n- P0 (Critical): Complete service outage affecting production. Response time: 1 hour for Enterprise, 4 hours for Premium\\n- P1 (High): Major functionality impaired but workarounds available. Response time: 4 hours for Enterprise, 8 hours for Premium\\n- P2 (Medium): Minor functionality issues with workarounds. Response time: 24 hours\\n- P3 (Low): Questions, feature requests, documentation issues. Response time: 48 hours]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    CloudFlow guarantees a 99.99% uptime SLA for the Enterprise Tier, 99.95% for the Premium Tier, and 99.9% for the Standard Tier.                                                                                                                                                                          CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones.      1.000000               NaN           1.000000        1.000000                    NaN                               NaN\n",
      "1                                     What authentication protocol does CloudFlow use?                                                                                                                                        [CloudFlow API Authentication\\n\\nCloudFlow APIs support two authentication methods: OAuth 2.0 and API Keys. Both methods provide secure access to platform resources.\\n\\nOAuth 2.0 is recommended for user-facing applications. It supports the Authorization Code flow and provides access tokens valid for 1 hour and refresh tokens valid for 30 days. To implement OAuth 2.0, direct users to the authorization endpoint at https://auth.cloudflow.io/oauth/authorize with your client_id and redirect_uri parameters.\\n\\nAPI Keys are ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters. API keys never expire unless explicitly revoked.\\n\\nTo authenticate requests, include your API key in the Authorization header: \"Authorization: Bearer YOUR_API_KEY\". All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error., CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect., CloudFlow Architecture Overview\\n\\nCloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\\n\\nThe API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\\n\\nThe Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\nThe Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand., CloudFlow Compliance Standards\\n\\nCloudFlow maintains compliance with major industry standards and regulations to ensure your data is handled responsibly.\\n\\nSOC 2 Type II: CloudFlow is SOC 2 Type II certified, demonstrating our commitment to security, availability, and confidentiality. Audit reports are available to enterprise customers under NDA.\\n\\nGDPR Compliance: CloudFlow is fully compliant with the European Union's General Data Protection Regulation. We support data residency requirements, right to erasure, data portability, and provide Data Processing Agreements (DPA) to all customers.\\n\\nHIPAA: For healthcare customers, CloudFlow offers HIPAA-compliant infrastructure with Business Associate Agreements (BAA). HIPAA features include enhanced audit logging, encrypted backups, and strict access controls.\\n\\nISO 27001: CloudFlow's information security management system is certified to ISO 27001:2013 standards. We maintain comprehensive security policies and undergo annual recertification audits.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         CloudFlow uses two authentication methods: OAuth 2.0 and API Keys. OAuth 2.0 is recommended for user-facing applications, while API Keys are ideal for server-to-server communication and background jobs.                                                                                                                                                            CloudFlow uses OAuth 2.0 for user-facing applications and API keys for server-to-server communication.      1.000000               NaN           0.833333        1.000000               0.666667                               NaN\n",
      "2                               What is the service mesh technology used by CloudFlow?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [CloudFlow Architecture Overview\\n\\nCloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\\n\\nThe API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\\n\\nThe Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\nThe Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand., CloudFlow System Components\\n\\nCloudFlow's architecture comprises several key components that work in harmony to deliver reliable cloud services.\\n\\nThe Control Plane manages the overall system state, including service registration, configuration management, and orchestration. It runs on a dedicated cluster with five replicas for high availability.\\n\\nThe Data Plane handles actual request processing and data flow. It consists of worker nodes that execute application workloads and process user requests. Each data plane node has 16 CPU cores and 64GB RAM.\\n\\nThe Observability Stack includes Prometheus for metrics collection, Grafana for visualization, and ELK (Elasticsearch, Logstash, Kibana) for log aggregation. Metrics are collected every 15 seconds and retained for 90 days.\\n\\nThe Service Registry maintains a real-time directory of all available services and their endpoints. It uses etcd for distributed consensus and supports automatic service discovery with DNS-based lookups., CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect., CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 The service mesh technology used by CloudFlow is powered by Istio.                                                                                                                                                           CloudFlow uses Istio as the service mesh technology to orchestrate communication between microservices.      1.000000               NaN           1.000000        1.000000               0.500000                               NaN\n",
      "3                                    What compliance standards does CloudFlow support?                                                               [CloudFlow Compliance Standards\\n\\nCloudFlow maintains compliance with major industry standards and regulations to ensure your data is handled responsibly.\\n\\nSOC 2 Type II: CloudFlow is SOC 2 Type II certified, demonstrating our commitment to security, availability, and confidentiality. Audit reports are available to enterprise customers under NDA.\\n\\nGDPR Compliance: CloudFlow is fully compliant with the European Union's General Data Protection Regulation. We support data residency requirements, right to erasure, data portability, and provide Data Processing Agreements (DPA) to all customers.\\n\\nHIPAA: For healthcare customers, CloudFlow offers HIPAA-compliant infrastructure with Business Associate Agreements (BAA). HIPAA features include enhanced audit logging, encrypted backups, and strict access controls.\\n\\nISO 27001: CloudFlow's information security management system is certified to ISO 27001:2013 standards. We maintain comprehensive security policies and undergo annual recertification audits., CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect., CloudFlow Architecture Overview\\n\\nCloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\\n\\nThe API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\\n\\nThe Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\nThe Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand., CloudFlow System Components\\n\\nCloudFlow's architecture comprises several key components that work in harmony to deliver reliable cloud services.\\n\\nThe Control Plane manages the overall system state, including service registration, configuration management, and orchestration. It runs on a dedicated cluster with five replicas for high availability.\\n\\nThe Data Plane handles actual request processing and data flow. It consists of worker nodes that execute application workloads and process user requests. Each data plane node has 16 CPU cores and 64GB RAM.\\n\\nThe Observability Stack includes Prometheus for metrics collection, Grafana for visualization, and ELK (Elasticsearch, Logstash, Kibana) for log aggregation. Metrics are collected every 15 seconds and retained for 90 days.\\n\\nThe Service Registry maintains a real-time directory of all available services and their endpoints. It uses etcd for distributed consensus and supports automatic service discovery with DNS-based lookups.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  CloudFlow supports the following compliance standards:\\n\\n1. **SOC 2 Type II**: Certified, demonstrating commitment to security, availability, and confidentiality.\\n2. **GDPR Compliance**: Fully compliant with the EU's General Data Protection Regulation, supporting data residency, right to erasure, data portability, and providing Data Processing Agreements (DPA).\\n3. **HIPAA**: Offers HIPAA-compliant infrastructure with Business Associate Agreements (BAA), including enhanced audit logging, encrypted backups, and strict access controls.\\n4. **ISO 27001**: Certified to ISO 27001:2013 standards, maintaining comprehensive security policies and undergoing annual recertification audits.                                                                                                                                                               CloudFlow supports SOC 2 Type II, GDPR, HIPAA, ISO 27001, and PCI DSS Level 1 compliance standards.           NaN               NaN           0.000000        0.000000               0.666667                               NaN\n",
      "4                                          How long are CloudFlow audit logs retained?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              [Audit Logging: Every API call is logged with timestamp, user identity, IP address, and action taken. Audit logs are immutable and retained for 2 years. You can access logs via the /api/v1/audit-logs endpoint.\\n\\nVulnerability Management: CloudFlow undergoes quarterly penetration testing by independent security firms. We maintain a bug bounty program and respond to security reports within 24 hours., Log Management: CloudFlow retains logs for 7 days by default (30 days for Premium, 90 days for Enterprise). Use structured logging with JSON format for better searchability. Maximum log line length: 32KB.\\n\\nDashboard Widgets: Create custom dashboards with real-time metrics, SLA compliance, and cost tracking. Share dashboards with team members or embed in external tools using iframe integration., CloudFlow Disaster Recovery\\n\\nCloudFlow implements comprehensive disaster recovery capabilities to protect your data and ensure business continuity.\\n\\nBackup Strategy: CloudFlow performs automatic daily backups of all data at 2 AM UTC. Backups are encrypted and stored in geographically diverse locations. Retention: 30 days for Standard tier, 90 days for Premium, 1 year for Enterprise.\\n\\nPoint-in-Time Recovery: Enterprise customers can restore data to any point within the retention period with 5-minute granularity. Recovery operations typically complete within 15-30 minutes.\\n\\nMulti-Region Replication: Enable multi-region replication for critical data. Data is asynchronously replicated to a secondary region within 60 seconds. Failover to secondary region is automatic and takes approximately 5 minutes.\\n\\nBackup Verification: CloudFlow performs monthly backup restoration tests to ensure data recoverability. Test results are available in your compliance dashboard., CloudFlow Compliance Standards\\n\\nCloudFlow maintains compliance with major industry standards and regulations to ensure your data is handled responsibly.\\n\\nSOC 2 Type II: CloudFlow is SOC 2 Type II certified, demonstrating our commitment to security, availability, and confidentiality. Audit reports are available to enterprise customers under NDA.\\n\\nGDPR Compliance: CloudFlow is fully compliant with the European Union's General Data Protection Regulation. We support data residency requirements, right to erasure, data portability, and provide Data Processing Agreements (DPA) to all customers.\\n\\nHIPAA: For healthcare customers, CloudFlow offers HIPAA-compliant infrastructure with Business Associate Agreements (BAA). HIPAA features include enhanced audit logging, encrypted backups, and strict access controls.\\n\\nISO 27001: CloudFlow's information security management system is certified to ISO 27001:2013 standards. We maintain comprehensive security policies and undergo annual recertification audits.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     CloudFlow audit logs are retained for 2 years.                                                                                                                                                                                                      CloudFlow audit logs are immutable and retained for 2 years.      1.000000               NaN           1.000000        1.000000               0.000000                               NaN\n",
      "5                            What are the three main layers of CloudFlow architecture?                                                                                                                           [CloudFlow Architecture Overview\\n\\nCloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\\n\\nThe API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\\n\\nThe Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\nThe Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand., CloudFlow System Components\\n\\nCloudFlow's architecture comprises several key components that work in harmony to deliver reliable cloud services.\\n\\nThe Control Plane manages the overall system state, including service registration, configuration management, and orchestration. It runs on a dedicated cluster with five replicas for high availability.\\n\\nThe Data Plane handles actual request processing and data flow. It consists of worker nodes that execute application workloads and process user requests. Each data plane node has 16 CPU cores and 64GB RAM.\\n\\nThe Observability Stack includes Prometheus for metrics collection, Grafana for visualization, and ELK (Elasticsearch, Logstash, Kibana) for log aggregation. Metrics are collected every 15 seconds and retained for 90 days.\\n\\nThe Service Registry maintains a real-time directory of all available services and their endpoints. It uses etcd for distributed consensus and supports automatic service discovery with DNS-based lookups., CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect., CloudFlow Pricing Tiers\\n\\nCloudFlow offers three pricing tiers designed to meet the needs of individuals, teams, and enterprises.\\n\\nStandard Tier ($99/month):\\n- 1,000 API requests per hour\\n- 100 GB storage included\\n- 10 GB bandwidth per month\\n- Community support via forums\\n- 99.9% uptime SLA\\n- Up to 5 team members\\n\\nPremium Tier ($499/month):\\n- 10,000 API requests per hour\\n- 1 TB storage included\\n- 100 GB bandwidth per month\\n- Email support with 24-hour response time\\n- 99.95% uptime SLA\\n- Up to 25 team members\\n- Advanced monitoring and alerting\\n- Custom domain support\\n\\nEnterprise Tier (Custom pricing):\\n- Custom API rate limits (100,000+ requests/hour)\\n- Unlimited storage and bandwidth\\n- 24/7 phone and email support with 1-hour response time\\n- 99.99% uptime SLA with service credits\\n- Unlimited team members\\n- Dedicated account manager\\n- Custom integrations and professional services\\n- Private cloud deployment options]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The three main layers of CloudFlow architecture are:\\n\\n1. **API Gateway Layer**: Handles all incoming requests, provides OAuth 2.0 authentication, load balancing, SSL termination, and request routing.\\n\\n2. **Service Mesh Layer**: Orchestrates communication between microservices, offering service discovery, health checking, and automatic failover, utilizing Kubernetes for container orchestration.\\n\\n3. **Data Storage Layer**: Implements a distributed database system with automatic replication across three availability zones for data durability and horizontal scaling.                                                               The three main layers are: API Gateway layer (handles authentication and routing), Service Mesh layer (orchestrates microservices), and Data Storage layer (distributed database with replication).           NaN               NaN           1.000000        1.000000               0.200000                               NaN\n",
      "6                        What are CloudFlow's pricing tiers and their API rate limits?  [CloudFlow Pricing Tiers\\n\\nCloudFlow offers three pricing tiers designed to meet the needs of individuals, teams, and enterprises.\\n\\nStandard Tier ($99/month):\\n- 1,000 API requests per hour\\n- 100 GB storage included\\n- 10 GB bandwidth per month\\n- Community support via forums\\n- 99.9% uptime SLA\\n- Up to 5 team members\\n\\nPremium Tier ($499/month):\\n- 10,000 API requests per hour\\n- 1 TB storage included\\n- 100 GB bandwidth per month\\n- Email support with 24-hour response time\\n- 99.95% uptime SLA\\n- Up to 25 team members\\n- Advanced monitoring and alerting\\n- Custom domain support\\n\\nEnterprise Tier (Custom pricing):\\n- Custom API rate limits (100,000+ requests/hour)\\n- Unlimited storage and bandwidth\\n- 24/7 phone and email support with 1-hour response time\\n- 99.99% uptime SLA with service credits\\n- Unlimited team members\\n- Dedicated account manager\\n- Custom integrations and professional services\\n- Private cloud deployment options, CloudFlow API Rate Limiting\\n\\nCloudFlow implements rate limiting to ensure fair usage and platform stability. Rate limits vary by pricing tier and authentication method.\\n\\nStandard Tier: 1,000 requests per hour per API key. Burst capacity allows up to 100 requests per minute. Exceeding limits returns HTTP 429 (Too Many Requests).\\n\\nPremium Tier: 10,000 requests per hour per API key with burst capacity of 500 requests per minute. Premium tier also includes priority request processing.\\n\\nEnterprise Tier: Custom rate limits negotiated based on usage patterns. Typically starts at 100,000 requests per hour with dedicated infrastructure.\\n\\nRate limit headers are included in every response:\\n- X-RateLimit-Limit: Maximum requests per hour\\n- X-RateLimit-Remaining: Remaining requests in current window\\n- X-RateLimit-Reset: Unix timestamp when the limit resets\\n\\nWhen rate limited, the Retry-After header indicates how many seconds to wait before retrying. Implement exponential backoff: wait 1s, then 2s, then 4s, etc., CloudFlow Billing Information\\n\\nUnderstanding CloudFlow's billing model helps you manage costs effectively and avoid unexpected charges.\\n\\nBilling Cycle: Subscriptions are billed monthly on the date you signed up. Annual subscriptions are billed upfront with a 15% discount. Billing date can be changed once per year.\\n\\nUsage-Based Charges: Beyond included quotas, additional usage is billed at:\\n- API requests: $0.01 per 1,000 requests\\n- Storage: $0.10 per GB per month\\n- Bandwidth: $0.08 per GB\\n- Backup retention (beyond 30 days): $0.05 per GB per month\\n\\nPayment Methods: CloudFlow accepts credit cards (Visa, Mastercard, Amex), ACH transfers (US only), and wire transfers for invoices over $1,000. Cryptocurrency payments available for annual plans.\\n\\nInvoicing: Invoices are emailed on the billing date and available in the dashboard. Enterprise customers receive consolidated monthly invoices with 30-day payment terms., Common CloudFlow Errors and Solutions\\n\\nThis guide covers the most common errors encountered when using CloudFlow and their solutions.\\n\\nError: \"Invalid API Key\" (401)\\nSolution: Verify your API key format starts with \"cf_live_\" and is exactly 40 characters. Check for extra spaces or newlines. Generate a new API key if the issue persists. API keys are case-sensitive.\\n\\nError: \"Rate Limit Exceeded\" (429)\\nSolution: Implement exponential backoff in your retry logic. Check X-RateLimit-Reset header to know when limits reset. Consider upgrading to a higher tier if you consistently hit limits. Use batch endpoints to reduce request count.\\n\\nError: \"Resource Not Found\" (404)\\nSolution: Verify the resource ID is correct and the resource hasn't been deleted. Use the /api/v1/resources endpoint to list available resources. Check if you're using the correct API version (/v1).]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       CloudFlow offers three pricing tiers:\\n\\n1. **Standard Tier ($99/month)**:\\n   - API Rate Limit: 1,000 requests per hour per API key\\n   - Burst Capacity: Up to 100 requests per minute\\n\\n2. **Premium Tier ($499/month)**:\\n   - API Rate Limit: 10,000 requests per hour per API key\\n   - Burst Capacity: Up to 500 requests per minute\\n   - Includes priority request processing\\n\\n3. **Enterprise Tier (Custom pricing)**:\\n   - API Rate Limit: Custom limits typically starting at 100,000 requests per hour\\n   - Burst Capacity: Negotiated based on usage patterns\\n\\nEach tier has specific features and support options, with the Enterprise Tier offering the most flexibility and support.                                                                              Standard tier costs $99/month with 1,000 requests/hour, Premium tier costs $499/month with 10,000 requests/hour, and Enterprise tier has custom pricing with 100,000+ requests/hour.           NaN               NaN           1.000000        1.000000               0.000000                               NaN\n",
      "7            What HTTP status codes indicate authentication failures in CloudFlow API?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     [CloudFlow API Error Codes\\n\\nCloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\\n\\nAuthentication Errors:\\n- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\\n- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\\n\\nClient Errors:\\n- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what's wrong.\\n- 404 Not Found: Requested resource doesn't exist. Verify the resource ID.\\n- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\\n- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\\n- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header., Error: \"Timeout\" (504)\\nSolution: Increase client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Check CloudFlow status page for any service degradation.\\n\\nError: \"Validation Error\" (422)\\nSolution: Review the error details field for specific validation failures. Common issues: missing required fields, invalid data types, values outside allowed ranges. Consult API documentation for correct request format., CloudFlow API Authentication\\n\\nCloudFlow APIs support two authentication methods: OAuth 2.0 and API Keys. Both methods provide secure access to platform resources.\\n\\nOAuth 2.0 is recommended for user-facing applications. It supports the Authorization Code flow and provides access tokens valid for 1 hour and refresh tokens valid for 30 days. To implement OAuth 2.0, direct users to the authorization endpoint at https://auth.cloudflow.io/oauth/authorize with your client_id and redirect_uri parameters.\\n\\nAPI Keys are ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters. API keys never expire unless explicitly revoked.\\n\\nTo authenticate requests, include your API key in the Authorization header: \"Authorization: Bearer YOUR_API_KEY\". All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error., Step 5 - Check Network Connectivity: Ensure your network allows outbound HTTPS traffic to *.cloudflow.io on port 443. Verify DNS resolution is working correctly.\\n\\nStep 6 - Verify SDK Version: If using CloudFlow SDK, ensure you're running the latest version. Outdated SDKs may not support new API features or may have known bugs.\\n\\nStep 7 - Contact Support: If issues persist, contact CloudFlow support with the request ID from failed requests. Support responds within 24 hours for Standard tier, 4 hours for Premium, 1 hour for Enterprise.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The HTTP status codes that indicate authentication failures in CloudFlow API are:\\n\\n- **401 Unauthorized**: This indicates a missing or invalid API key. You should check the Authorization header.\\n- **403 Forbidden**: This indicates a valid API key but insufficient permissions for the requested operation.                                                                                                                                    401 Unauthorized indicates missing or invalid API key, and 403 Forbidden indicates valid API key but insufficient permissions.      1.000000               NaN           1.000000        1.000000               0.750000                               NaN\n",
      "8                                    What auto-scaling metrics does CloudFlow monitor?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [CloudFlow Scaling Mechanisms\\n\\nCloudFlow implements sophisticated auto-scaling mechanisms to handle varying workloads efficiently. The platform monitors real-time metrics to make intelligent scaling decisions.\\n\\nHorizontal Pod Autoscaling (HPA) adjusts the number of pod replicas based on CPU utilization (target: 70%) and memory usage (target: 80%). When these thresholds are exceeded for more than 3 consecutive minutes, the system automatically provisions additional pods.\\n\\nVertical scaling adjusts resource allocation for individual services. CloudFlow can increase or decrease CPU and memory limits without downtime, using Kubernetes resource management capabilities.\\n\\nThe platform supports bursting to handle sudden traffic spikes. During burst periods, CloudFlow can temporarily scale up to 500% of baseline capacity for up to 15 minutes before triggering permanent scaling., CloudFlow Monitoring and Observability\\n\\nEffective monitoring ensures your CloudFlow applications remain healthy and performant.\\n\\nMetrics Collection: CloudFlow automatically collects key metrics including request rate, error rate, latency (p50, p95, p99), and resource utilization. Access metrics via the dashboard or Metrics API at /api/v1/metrics.\\n\\nCustom Metrics: Send custom application metrics using the StatsD protocol. CloudFlow aggregates custom metrics every 60 seconds and retains them for 90 days.\\n\\nAlerting: Configure alerts for critical conditions like error rate >5%, latency >500ms, or approaching rate limits. CloudFlow supports alerting via email, SMS, Slack, PagerDuty, and webhooks.\\n\\nDistributed Tracing: Enable distributed tracing to track requests across services. CloudFlow supports OpenTelemetry and provides trace visualization in the dashboard. Sample rate: 10% of requests (configurable up to 100%)., CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics., CloudFlow System Components\\n\\nCloudFlow's architecture comprises several key components that work in harmony to deliver reliable cloud services.\\n\\nThe Control Plane manages the overall system state, including service registration, configuration management, and orchestration. It runs on a dedicated cluster with five replicas for high availability.\\n\\nThe Data Plane handles actual request processing and data flow. It consists of worker nodes that execute application workloads and process user requests. Each data plane node has 16 CPU cores and 64GB RAM.\\n\\nThe Observability Stack includes Prometheus for metrics collection, Grafana for visualization, and ELK (Elasticsearch, Logstash, Kibana) for log aggregation. Metrics are collected every 15 seconds and retained for 90 days.\\n\\nThe Service Registry maintains a real-time directory of all available services and their endpoints. It uses etcd for distributed consensus and supports automatic service discovery with DNS-based lookups.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    CloudFlow monitors CPU utilization (target: 70%) and memory usage (target: 80%) for its Horizontal Pod Autoscaling (HPA). Additionally, it collects key metrics including request rate, error rate, latency (p50, p95, p99), and resource utilization for overall monitoring and observability.                                                                                                CloudFlow monitors CPU utilization (target 70%), memory usage (target 80%), and triggers scaling when thresholds are exceeded for more than 3 consecutive minutes.      0.875000               NaN           0.833333        1.000000               0.500000                               NaN\n",
      "9                          How do I authenticate with CloudFlow APIs using an API key?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        [CloudFlow API Authentication\\n\\nCloudFlow APIs support two authentication methods: OAuth 2.0 and API Keys. Both methods provide secure access to platform resources.\\n\\nOAuth 2.0 is recommended for user-facing applications. It supports the Authorization Code flow and provides access tokens valid for 1 hour and refresh tokens valid for 30 days. To implement OAuth 2.0, direct users to the authorization endpoint at https://auth.cloudflow.io/oauth/authorize with your client_id and redirect_uri parameters.\\n\\nAPI Keys are ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters. API keys never expire unless explicitly revoked.\\n\\nTo authenticate requests, include your API key in the Authorization header: \"Authorization: Bearer YOUR_API_KEY\". All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error., API keys can be scoped to specific permissions (read, write, admin) and restricted to specific IP addresses for enhanced security. You can manage your API keys through the CloudFlow dashboard or the /api/v1/keys endpoint., CloudFlow API Error Codes\\n\\nCloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\\n\\nAuthentication Errors:\\n- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\\n- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\\n\\nClient Errors:\\n- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what's wrong.\\n- 404 Not Found: Requested resource doesn't exist. Verify the resource ID.\\n- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\\n- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\\n- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header., Common CloudFlow Errors and Solutions\\n\\nThis guide covers the most common errors encountered when using CloudFlow and their solutions.\\n\\nError: \"Invalid API Key\" (401)\\nSolution: Verify your API key format starts with \"cf_live_\" and is exactly 40 characters. Check for extra spaces or newlines. Generate a new API key if the issue persists. API keys are case-sensitive.\\n\\nError: \"Rate Limit Exceeded\" (429)\\nSolution: Implement exponential backoff in your retry logic. Check X-RateLimit-Reset header to know when limits reset. Consider upgrading to a higher tier if you consistently hit limits. Use batch endpoints to reduce request count.\\n\\nError: \"Resource Not Found\" (404)\\nSolution: Verify the resource ID is correct and the resource hasn't been deleted. Use the /api/v1/resources endpoint to list available resources. Check if you're using the correct API version (/v1).]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        To authenticate with CloudFlow APIs using an API key, include your API key in the Authorization header of your request. The format should be: \\n\\n```\\nAuthorization: Bearer YOUR_API_KEY\\n```\\n\\nMake sure your API key starts with \"cf_live_\" and is exactly 40 characters long. All API requests must be made over HTTPS, as HTTP requests will be rejected with a 403 error. You can manage your API keys through the CloudFlow dashboard or the /api/v1/keys endpoint.                                                   Include your API key in the Authorization header as 'Authorization: Bearer YOUR_API_KEY'. All requests must be made over HTTPS, and API keys have the format 'cf_live_' followed by 32 alphanumeric characters.      0.857143               NaN           1.000000        1.000000               0.500000                               NaN\n",
      "10                                     How do I handle rate limit errors in CloudFlow?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        [CloudFlow API Rate Limiting\\n\\nCloudFlow implements rate limiting to ensure fair usage and platform stability. Rate limits vary by pricing tier and authentication method.\\n\\nStandard Tier: 1,000 requests per hour per API key. Burst capacity allows up to 100 requests per minute. Exceeding limits returns HTTP 429 (Too Many Requests).\\n\\nPremium Tier: 10,000 requests per hour per API key with burst capacity of 500 requests per minute. Premium tier also includes priority request processing.\\n\\nEnterprise Tier: Custom rate limits negotiated based on usage patterns. Typically starts at 100,000 requests per hour with dedicated infrastructure.\\n\\nRate limit headers are included in every response:\\n- X-RateLimit-Limit: Maximum requests per hour\\n- X-RateLimit-Remaining: Remaining requests in current window\\n- X-RateLimit-Reset: Unix timestamp when the limit resets\\n\\nWhen rate limited, the Retry-After header indicates how many seconds to wait before retrying. Implement exponential backoff: wait 1s, then 2s, then 4s, etc., Common CloudFlow Errors and Solutions\\n\\nThis guide covers the most common errors encountered when using CloudFlow and their solutions.\\n\\nError: \"Invalid API Key\" (401)\\nSolution: Verify your API key format starts with \"cf_live_\" and is exactly 40 characters. Check for extra spaces or newlines. Generate a new API key if the issue persists. API keys are case-sensitive.\\n\\nError: \"Rate Limit Exceeded\" (429)\\nSolution: Implement exponential backoff in your retry logic. Check X-RateLimit-Reset header to know when limits reset. Consider upgrading to a higher tier if you consistently hit limits. Use batch endpoints to reduce request count.\\n\\nError: \"Resource Not Found\" (404)\\nSolution: Verify the resource ID is correct and the resource hasn't been deleted. Use the /api/v1/resources endpoint to list available resources. Check if you're using the correct API version (/v1)., Error: \"Timeout\" (504)\\nSolution: Increase client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Check CloudFlow status page for any service degradation.\\n\\nError: \"Validation Error\" (422)\\nSolution: Review the error details field for specific validation failures. Common issues: missing required fields, invalid data types, values outside allowed ranges. Consult API documentation for correct request format., CloudFlow API Error Codes\\n\\nCloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\\n\\nAuthentication Errors:\\n- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\\n- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\\n\\nClient Errors:\\n- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what's wrong.\\n- 404 Not Found: Requested resource doesn't exist. Verify the resource ID.\\n- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\\n- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\\n- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      To handle rate limit errors in CloudFlow, follow these steps:\\n\\n1. **Implement Exponential Backoff**: When you receive a \"Rate Limit Exceeded\" (HTTP 429) error, wait before retrying your request. Use an exponential backoff strategy: wait 1 second, then 2 seconds, then 4 seconds, and so on.\\n\\n2. **Check Rate Limit Headers**: In the response headers, look for:\\n   - `X-RateLimit-Limit`: Maximum requests allowed per hour.\\n   - `X-RateLimit-Remaining`: Remaining requests in the current window.\\n   - `X-RateLimit-Reset`: Unix timestamp indicating when the limit resets.\\n\\n3. **Use the Retry-After Header**: This header indicates how many seconds to wait before retrying your request.\\n\\n4. **Consider Upgrading**: If you consistently hit rate limits, consider upgrading to a higher pricing tier for increased limits.\\n\\n5. **Optimize Requests**: Use batch endpoints to reduce the number of requests you make, which can help stay within the rate limits.                                     When you receive a 429 error, implement exponential backoff in retry logic, check the X-RateLimit-Reset header to know when limits reset, and use the Retry-After header to determine wait time (1s, then 2s, then 4s, etc.).           NaN               NaN           0.916667        1.000000               0.142857                               NaN\n",
      "11                   What steps should I follow to optimize CloudFlow API performance?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     [CloudFlow Performance Optimization\\n\\nFollowing these best practices will help you achieve optimal performance from your CloudFlow applications.\\n\\nCaching Strategy: Implement caching at multiple levels. Use CloudFlow's built-in Redis cache for frequently accessed data with TTL between 5-60 minutes. Cache API responses on the client side and respect Cache-Control headers.\\n\\nRequest Optimization: Batch multiple operations into single API calls when possible. Use pagination for large result sets (recommended page size: 50-100 items). Implement request compression using gzip to reduce bandwidth.\\n\\nConnection Management: Reuse HTTP connections with keep-alive headers. Maintain a connection pool with 5-10 concurrent connections per API key. Set appropriate timeouts: connection timeout 10s, read timeout 30s.\\n\\nQuery Efficiency: Use field filtering to request only required data: /resources?fields=id,name,status. Leverage server-side filtering instead of retrieving all data and filtering locally., Asynchronous Processing: For long-running operations, use CloudFlow's async API endpoints. Poll for results using the returned job_id rather than blocking on the initial request.\\n\\nCDN Usage: Serve static assets through CloudFlow's global CDN with 150+ edge locations. Configure appropriate cache headers for optimal performance: max-age=3600 for semi-static content., CloudFlow API Rate Limiting\\n\\nCloudFlow implements rate limiting to ensure fair usage and platform stability. Rate limits vary by pricing tier and authentication method.\\n\\nStandard Tier: 1,000 requests per hour per API key. Burst capacity allows up to 100 requests per minute. Exceeding limits returns HTTP 429 (Too Many Requests).\\n\\nPremium Tier: 10,000 requests per hour per API key with burst capacity of 500 requests per minute. Premium tier also includes priority request processing.\\n\\nEnterprise Tier: Custom rate limits negotiated based on usage patterns. Typically starts at 100,000 requests per hour with dedicated infrastructure.\\n\\nRate limit headers are included in every response:\\n- X-RateLimit-Limit: Maximum requests per hour\\n- X-RateLimit-Remaining: Remaining requests in current window\\n- X-RateLimit-Reset: Unix timestamp when the limit resets\\n\\nWhen rate limited, the Retry-After header indicates how many seconds to wait before retrying. Implement exponential backoff: wait 1s, then 2s, then 4s, etc., Common CloudFlow Errors and Solutions\\n\\nThis guide covers the most common errors encountered when using CloudFlow and their solutions.\\n\\nError: \"Invalid API Key\" (401)\\nSolution: Verify your API key format starts with \"cf_live_\" and is exactly 40 characters. Check for extra spaces or newlines. Generate a new API key if the issue persists. API keys are case-sensitive.\\n\\nError: \"Rate Limit Exceeded\" (429)\\nSolution: Implement exponential backoff in your retry logic. Check X-RateLimit-Reset header to know when limits reset. Consider upgrading to a higher tier if you consistently hit limits. Use batch endpoints to reduce request count.\\n\\nError: \"Resource Not Found\" (404)\\nSolution: Verify the resource ID is correct and the resource hasn't been deleted. Use the /api/v1/resources endpoint to list available resources. Check if you're using the correct API version (/v1).]                                                                                       To optimize CloudFlow API performance, follow these steps:\\n\\n1. **Implement Caching Strategy**:\\n   - Use CloudFlow's built-in Redis cache for frequently accessed data with a TTL of 5-60 minutes.\\n   - Cache API responses on the client side and respect Cache-Control headers.\\n\\n2. **Optimize Requests**:\\n   - Batch multiple operations into single API calls when possible.\\n   - Use pagination for large result sets (recommended page size: 50-100 items).\\n   - Implement request compression using gzip to reduce bandwidth.\\n\\n3. **Manage Connections**:\\n   - Reuse HTTP connections with keep-alive headers.\\n   - Maintain a connection pool with 5-10 concurrent connections per API key.\\n   - Set appropriate timeouts: connection timeout of 10 seconds and read timeout of 30 seconds.\\n\\n4. **Enhance Query Efficiency**:\\n   - Use field filtering to request only the required data (e.g., `/resources?fields=id,name,status`).\\n   - Leverage server-side filtering instead of retrieving all data and filtering locally.\\n\\n5. **Utilize Asynchronous Processing**:\\n   - For long-running operations, use CloudFlow's async API endpoints.\\n   - Poll for results using the returned job_id instead of blocking on the initial request.\\n\\n6. **Use CDN for Static Assets**:\\n   - Serve static assets through CloudFlow's global CDN with 150+ edge locations.\\n   - Configure appropriate cache headers for optimal performance (e.g., max-age=3600 for semi-static content). \\n\\nBy following these best practices, you can achieve optimal performance from your CloudFlow applications.     Implement caching with Redis (TTL 5-60 minutes), batch multiple operations into single API calls, use pagination for large result sets (50-100 items), enable request compression with gzip, and maintain a connection pool with 5-10 concurrent connections.           NaN               NaN           1.000000        1.000000               0.222222                               NaN\n",
      "12                What's the difference between Standard and Premium tier rate limits?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             [OAuth 2.0 authenticated requests have separate, higher limits: 5,000 requests per hour for Standard tier., CloudFlow API Rate Limiting\\n\\nCloudFlow implements rate limiting to ensure fair usage and platform stability. Rate limits vary by pricing tier and authentication method.\\n\\nStandard Tier: 1,000 requests per hour per API key. Burst capacity allows up to 100 requests per minute. Exceeding limits returns HTTP 429 (Too Many Requests).\\n\\nPremium Tier: 10,000 requests per hour per API key with burst capacity of 500 requests per minute. Premium tier also includes priority request processing.\\n\\nEnterprise Tier: Custom rate limits negotiated based on usage patterns. Typically starts at 100,000 requests per hour with dedicated infrastructure.\\n\\nRate limit headers are included in every response:\\n- X-RateLimit-Limit: Maximum requests per hour\\n- X-RateLimit-Remaining: Remaining requests in current window\\n- X-RateLimit-Reset: Unix timestamp when the limit resets\\n\\nWhen rate limited, the Retry-After header indicates how many seconds to wait before retrying. Implement exponential backoff: wait 1s, then 2s, then 4s, etc., CloudFlow Pricing Tiers\\n\\nCloudFlow offers three pricing tiers designed to meet the needs of individuals, teams, and enterprises.\\n\\nStandard Tier ($99/month):\\n- 1,000 API requests per hour\\n- 100 GB storage included\\n- 10 GB bandwidth per month\\n- Community support via forums\\n- 99.9% uptime SLA\\n- Up to 5 team members\\n\\nPremium Tier ($499/month):\\n- 10,000 API requests per hour\\n- 1 TB storage included\\n- 100 GB bandwidth per month\\n- Email support with 24-hour response time\\n- 99.95% uptime SLA\\n- Up to 25 team members\\n- Advanced monitoring and alerting\\n- Custom domain support\\n\\nEnterprise Tier (Custom pricing):\\n- Custom API rate limits (100,000+ requests/hour)\\n- Unlimited storage and bandwidth\\n- 24/7 phone and email support with 1-hour response time\\n- 99.99% uptime SLA with service credits\\n- Unlimited team members\\n- Dedicated account manager\\n- Custom integrations and professional services\\n- Private cloud deployment options, Upgrades and Downgrades: Upgrade anytime to immediately access higher tier features. Downgrades take effect at the next billing cycle. Prorated credits are applied to your account balance.\\n\\nFree Trial: New customers get 14-day free trial on Premium tier with no credit card required. Trial includes 1,000 API requests and 10 GB storage.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The Standard tier has a rate limit of 1,000 requests per hour per API key, with a burst capacity of up to 100 requests per minute. In contrast, the Premium tier allows for 10,000 requests per hour per API key, with a burst capacity of 500 requests per minute.                                        Standard tier allows 1,000 requests per hour with 100 requests per minute burst, while Premium tier allows 10,000 requests per hour with 500 requests per minute burst. Premium also includes priority request processing.      1.000000               NaN           0.583333        1.000000               0.285714                               NaN\n",
      "13  How does OAuth 2.0 authentication differ from API key authentication in CloudFlow?                                                                                                                                                                                                                                                                                                                                    [CloudFlow API Authentication\\n\\nCloudFlow APIs support two authentication methods: OAuth 2.0 and API Keys. Both methods provide secure access to platform resources.\\n\\nOAuth 2.0 is recommended for user-facing applications. It supports the Authorization Code flow and provides access tokens valid for 1 hour and refresh tokens valid for 30 days. To implement OAuth 2.0, direct users to the authorization endpoint at https://auth.cloudflow.io/oauth/authorize with your client_id and redirect_uri parameters.\\n\\nAPI Keys are ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters. API keys never expire unless explicitly revoked.\\n\\nTo authenticate requests, include your API key in the Authorization header: \"Authorization: Bearer YOUR_API_KEY\". All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error., CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect., CloudFlow API Error Codes\\n\\nCloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\\n\\nAuthentication Errors:\\n- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\\n- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\\n\\nClient Errors:\\n- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what's wrong.\\n- 404 Not Found: Requested resource doesn't exist. Verify the resource ID.\\n- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\\n- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\\n- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header., CloudFlow Architecture Overview\\n\\nCloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\\n\\nThe API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\\n\\nThe Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\nThe Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               OAuth 2.0 authentication is recommended for user-facing applications and supports the Authorization Code flow, providing access tokens valid for 1 hour and refresh tokens valid for 30 days. Users are directed to an authorization endpoint with parameters like client_id and redirect_uri.\\n\\nIn contrast, API key authentication is ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters and never expires unless revoked. API keys are included in the Authorization header as \"Authorization: Bearer YOUR_API_KEY\".\\n\\nAdditionally, all API requests must be made over HTTPS, and while OAuth 2.0 is more suited for user interactions, API keys are simpler for automated processes.  OAuth 2.0 is recommended for user-facing applications with access tokens valid for 1 hour and provides the Authorization Code flow, while API keys are ideal for server-to-server communication, never expire unless revoked, and have a simpler implementation.           NaN               NaN           1.000000        0.500000               0.750000                               NaN\n",
      "14                                  What should I do if I receive a 504 timeout error?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             [Error: \"Timeout\" (504)\\nSolution: Increase client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Check CloudFlow status page for any service degradation.\\n\\nError: \"Validation Error\" (422)\\nSolution: Review the error details field for specific validation failures. Common issues: missing required fields, invalid data types, values outside allowed ranges. Consult API documentation for correct request format., Server Errors:\\n- 500 Internal Server Error: Unexpected server error. CloudFlow team is automatically notified.\\n- 502 Bad Gateway: Temporary issue with upstream services. Retry after a few seconds.\\n- 503 Service Unavailable: Scheduled maintenance or system overload. Check status.cloudflow.io.\\n\\nError Response Format: {\"error\": {\"code\": \"error_code\", \"message\": \"Human-readable message\", \"details\": {...}}}, Step 5 - Check Network Connectivity: Ensure your network allows outbound HTTPS traffic to *.cloudflow.io on port 443. Verify DNS resolution is working correctly.\\n\\nStep 6 - Verify SDK Version: If using CloudFlow SDK, ensure you're running the latest version. Outdated SDKs may not support new API features or may have known bugs.\\n\\nStep 7 - Contact Support: If issues persist, contact CloudFlow support with the request ID from failed requests. Support responds within 24 hours for Standard tier, 4 hours for Premium, 1 hour for Enterprise., Common CloudFlow Errors and Solutions\\n\\nThis guide covers the most common errors encountered when using CloudFlow and their solutions.\\n\\nError: \"Invalid API Key\" (401)\\nSolution: Verify your API key format starts with \"cf_live_\" and is exactly 40 characters. Check for extra spaces or newlines. Generate a new API key if the issue persists. API keys are case-sensitive.\\n\\nError: \"Rate Limit Exceeded\" (429)\\nSolution: Implement exponential backoff in your retry logic. Check X-RateLimit-Reset header to know when limits reset. Consider upgrading to a higher tier if you consistently hit limits. Use batch endpoints to reduce request count.\\n\\nError: \"Resource Not Found\" (404)\\nSolution: Verify the resource ID is correct and the resource hasn't been deleted. Use the /api/v1/resources endpoint to list available resources. Check if you're using the correct API version (/v1).]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          If you receive a 504 timeout error, increase the client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Additionally, check the CloudFlow status page for any service degradation.                                                                            Increase client timeout to at least 30 seconds, use async endpoints for long-running operations and poll for results, and check the CloudFlow status page for any service degradation.      1.000000               NaN           1.000000        1.000000               0.333333                               0.0\n",
      "15                                How do I debug slow API response times in CloudFlow?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [CloudFlow Debugging Guide\\n\\nWhen troubleshooting issues with CloudFlow, follow this systematic debugging approach.\\n\\nStep 1 - Check Service Status: Visit status.cloudflow.io to verify all systems are operational. Subscribe to status updates to receive notifications about incidents and maintenance.\\n\\nStep 2 - Review API Logs: Access detailed API logs in the CloudFlow dashboard under Analytics > API Logs. Filter by time range, status code, and endpoint. Look for patterns in failed requests.\\n\\nStep 3 - Enable Debug Mode: Add X-CloudFlow-Debug: true header to requests to receive detailed debug information in responses. Debug mode provides request ID, processing time breakdown, and backend service information.\\n\\nStep 4 - Test with curl: Isolate issues by testing with curl commands. Example: curl -H \"Authorization: Bearer YOUR_API_KEY\" -H \"X-CloudFlow-Debug: true\" https://api.cloudflow.io/v1/resources, Error: \"Timeout\" (504)\\nSolution: Increase client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Check CloudFlow status page for any service degradation.\\n\\nError: \"Validation Error\" (422)\\nSolution: Review the error details field for specific validation failures. Common issues: missing required fields, invalid data types, values outside allowed ranges. Consult API documentation for correct request format., CloudFlow Performance Optimization\\n\\nFollowing these best practices will help you achieve optimal performance from your CloudFlow applications.\\n\\nCaching Strategy: Implement caching at multiple levels. Use CloudFlow's built-in Redis cache for frequently accessed data with TTL between 5-60 minutes. Cache API responses on the client side and respect Cache-Control headers.\\n\\nRequest Optimization: Batch multiple operations into single API calls when possible. Use pagination for large result sets (recommended page size: 50-100 items). Implement request compression using gzip to reduce bandwidth.\\n\\nConnection Management: Reuse HTTP connections with keep-alive headers. Maintain a connection pool with 5-10 concurrent connections per API key. Set appropriate timeouts: connection timeout 10s, read timeout 30s.\\n\\nQuery Efficiency: Use field filtering to request only required data: /resources?fields=id,name,status. Leverage server-side filtering instead of retrieving all data and filtering locally., Step 5 - Check Network Connectivity: Ensure your network allows outbound HTTPS traffic to *.cloudflow.io on port 443. Verify DNS resolution is working correctly.\\n\\nStep 6 - Verify SDK Version: If using CloudFlow SDK, ensure you're running the latest version. Outdated SDKs may not support new API features or may have known bugs.\\n\\nStep 7 - Contact Support: If issues persist, contact CloudFlow support with the request ID from failed requests. Support responds within 24 hours for Standard tier, 4 hours for Premium, 1 hour for Enterprise.]  To debug slow API response times in CloudFlow, follow these steps:\\n\\n1. **Check Service Status**: Visit status.cloudflow.io to ensure all systems are operational and there are no ongoing incidents affecting performance.\\n\\n2. **Review API Logs**: Access the API logs in the CloudFlow dashboard under Analytics > API Logs. Look for patterns in response times and identify any endpoints that are consistently slow.\\n\\n3. **Enable Debug Mode**: Add the header `X-CloudFlow-Debug: true` to your requests. This will provide detailed debug information in the responses, including request ID and processing time breakdown.\\n\\n4. **Test with curl**: Use curl commands to isolate the issue. For example:\\n   ```\\n   curl -H \"Authorization: Bearer YOUR_API_KEY\" -H \"X-CloudFlow-Debug: true\" https://api.cloudflow.io/v1/resources\\n   ```\\n   This can help you determine if the slowness is related to specific requests or endpoints.\\n\\n5. **Optimize Requests**: Implement request optimization strategies such as batching multiple operations into single API calls, using pagination for large result sets (recommended page size: 50-100 items), and enabling request compression with gzip.\\n\\n6. **Check Network Connectivity**: Ensure your network allows outbound HTTPS traffic to *.cloudflow.io on port 443 and verify that DNS resolution is functioning correctly.\\n\\n7. **Verify SDK Version**: If you are using the CloudFlow SDK, ensure it is updated to the latest version, as outdated SDKs may not support new features or could have performance issues.\\n\\nBy following these steps, you can identify and address the causes of slow API response times in CloudFlow.                               Add X-CloudFlow-Debug: true header to requests for detailed debug information, review API logs in the dashboard under Analytics > API Logs, test with curl commands, and verify network connectivity to *.cloudflow.io on port 443.           NaN               NaN           1.000000        1.000000               0.285714                               NaN\n",
      "16                                       What happens if I use an expired OAuth token?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         [OAuth 2.0 authenticated requests have separate, higher limits: 5,000 requests per hour for Standard tier., CloudFlow API Authentication\\n\\nCloudFlow APIs support two authentication methods: OAuth 2.0 and API Keys. Both methods provide secure access to platform resources.\\n\\nOAuth 2.0 is recommended for user-facing applications. It supports the Authorization Code flow and provides access tokens valid for 1 hour and refresh tokens valid for 30 days. To implement OAuth 2.0, direct users to the authorization endpoint at https://auth.cloudflow.io/oauth/authorize with your client_id and redirect_uri parameters.\\n\\nAPI Keys are ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters. API keys never expire unless explicitly revoked.\\n\\nTo authenticate requests, include your API key in the Authorization header: \"Authorization: Bearer YOUR_API_KEY\". All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error., CloudFlow API Error Codes\\n\\nCloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\\n\\nAuthentication Errors:\\n- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\\n- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\\n\\nClient Errors:\\n- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what's wrong.\\n- 404 Not Found: Requested resource doesn't exist. Verify the resource ID.\\n- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\\n- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\\n- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header., Upgrades and Downgrades: Upgrade anytime to immediately access higher tier features. Downgrades take effect at the next billing cycle. Prorated credits are applied to your account balance.\\n\\nFree Trial: New customers get 14-day free trial on Premium tier with no credit card required. Trial includes 1,000 API requests and 10 GB storage.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           I don't have enough information to answer that question.                                         If you use an expired access token, you'll receive a 401 Unauthorized error. You should use your refresh token to obtain a new access token. Access tokens are valid for 1 hour and refresh tokens are valid for 30 days.      0.000000               NaN           0.500000        0.333333               0.333333                               0.0\n",
      "17                                      Does CloudFlow support blockchain integration?                                                                                                                           [CloudFlow Architecture Overview\\n\\nCloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\\n\\nThe API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\\n\\nThe Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\nThe Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand., CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect., CloudFlow System Components\\n\\nCloudFlow's architecture comprises several key components that work in harmony to deliver reliable cloud services.\\n\\nThe Control Plane manages the overall system state, including service registration, configuration management, and orchestration. It runs on a dedicated cluster with five replicas for high availability.\\n\\nThe Data Plane handles actual request processing and data flow. It consists of worker nodes that execute application workloads and process user requests. Each data plane node has 16 CPU cores and 64GB RAM.\\n\\nThe Observability Stack includes Prometheus for metrics collection, Grafana for visualization, and ELK (Elasticsearch, Logstash, Kibana) for log aggregation. Metrics are collected every 15 seconds and retained for 90 days.\\n\\nThe Service Registry maintains a real-time directory of all available services and their endpoints. It uses etcd for distributed consensus and supports automatic service discovery with DNS-based lookups., CloudFlow Pricing Tiers\\n\\nCloudFlow offers three pricing tiers designed to meet the needs of individuals, teams, and enterprises.\\n\\nStandard Tier ($99/month):\\n- 1,000 API requests per hour\\n- 100 GB storage included\\n- 10 GB bandwidth per month\\n- Community support via forums\\n- 99.9% uptime SLA\\n- Up to 5 team members\\n\\nPremium Tier ($499/month):\\n- 10,000 API requests per hour\\n- 1 TB storage included\\n- 100 GB bandwidth per month\\n- Email support with 24-hour response time\\n- 99.95% uptime SLA\\n- Up to 25 team members\\n- Advanced monitoring and alerting\\n- Custom domain support\\n\\nEnterprise Tier (Custom pricing):\\n- Custom API rate limits (100,000+ requests/hour)\\n- Unlimited storage and bandwidth\\n- 24/7 phone and email support with 1-hour response time\\n- 99.99% uptime SLA with service credits\\n- Unlimited team members\\n- Dedicated account manager\\n- Custom integrations and professional services\\n- Private cloud deployment options]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           I don't have enough information to answer that question.                                                                                                                                                                                                          I don't have enough information to answer that question.      1.000000               NaN           0.000000        0.000000               0.000000                               1.0\n",
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n",
      "       faithfulness  answer_relevancy  context_precision  context_recall  \\\n",
      "count     11.000000               0.0          18.000000       18.000000   \n",
      "mean       0.884740               NaN           0.814815        0.824074   \n",
      "std        0.298312               NaN           0.331558        0.354579   \n",
      "min        0.000000               NaN           0.000000        0.000000   \n",
      "25%        0.937500               NaN           0.833333        1.000000   \n",
      "50%        1.000000               NaN           1.000000        1.000000   \n",
      "75%        1.000000               NaN           1.000000        1.000000   \n",
      "max        1.000000               NaN           1.000000        1.000000   \n",
      "\n",
      "       context_entity_recall  noise_sensitivity(mode=relevant)  \n",
      "count              17.000000                          3.000000  \n",
      "mean                0.360971                          0.333333  \n",
      "std                 0.255449                          0.577350  \n",
      "min                 0.000000                          0.000000  \n",
      "25%                 0.200000                          0.000000  \n",
      "50%                 0.333333                          0.000000  \n",
      "75%                 0.500000                          0.500000  \n",
      "max                 0.750000                          1.000000  \n",
      "\n",
      "================================================================================\n",
      "BEST & WORST PERFORMING QUESTIONS\n",
      "================================================================================\n",
      "\n",
      "BEST: What is CloudFlow's uptime SLA?\n",
      "  Average Score: 1.000\n",
      "\n",
      "WORST: What compliance standards does CloudFlow support?\n",
      "  Average Score: 0.222\n"
     ]
    }
   ],
   "source": [
    "# Convert results to pandas DataFrame\n",
    "results_df = result.to_pandas()\n",
    "\n",
    "print(\"Detailed Results by Question:\\n\")\n",
    "print(results_df.to_string())\n",
    "\n",
    "# Calculate summary statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df.describe())\n",
    "\n",
    "# Identify best and worst performing questions\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BEST & WORST PERFORMING QUESTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate average score per question (across all metrics)\n",
    "metric_cols = [col for col in results_df.columns if col not in ['user_input', 'response', 'retrieved_contexts', 'reference']]\n",
    "results_df['avg_score'] = results_df[metric_cols].mean(axis=1)\n",
    "\n",
    "# Best performing\n",
    "best_idx = results_df['avg_score'].idxmax()\n",
    "print(f\"\\nBEST: {results_df.loc[best_idx, 'user_input']}\")\n",
    "print(f\"  Average Score: {results_df.loc[best_idx, 'avg_score']:.3f}\")\n",
    "\n",
    "# Worst performing\n",
    "worst_idx = results_df['avg_score'].idxmin()\n",
    "print(f\"\\nWORST: {results_df.loc[worst_idx, 'user_input']}\")\n",
    "print(f\"  Average Score: {results_df.loc[worst_idx, 'avg_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 8: Advanced Evaluation - Multiple Configurations\n",
    "\n",
    "**Difficulty:** ðŸš€ ADVANCED\n",
    "\n",
    "Now we'll compare 4 different RAG configurations:\n",
    "\n",
    "1. **Baseline**: chunk_size=1024, k=4, similarity\n",
    "2. **Precise**: chunk_size=512, k=4, similarity (smaller chunks)\n",
    "3. **Contextual**: chunk_size=1024, k=6, similarity (more context)\n",
    "4. **Diverse**: chunk_size=1024, k=4, mmr (diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Define Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configurations to test\n",
    "configs = {\n",
    "    \"Baseline\": {\n",
    "        \"chunk_size\": 1024,\n",
    "        \"chunk_overlap\": 128,\n",
    "        \"k\": 4,\n",
    "        \"search_type\": \"similarity\",\n",
    "        \"description\": \"Standard configuration (our baseline)\"\n",
    "    },\n",
    "    \"Precise\": {\n",
    "        \"chunk_size\": 512,\n",
    "        \"chunk_overlap\": 64,\n",
    "        \"k\": 4,\n",
    "        \"search_type\": \"similarity\",\n",
    "        \"description\": \"Smaller chunks for focused answers\"\n",
    "    },\n",
    "    \"Contextual\": {\n",
    "        \"chunk_size\": 1024,\n",
    "        \"chunk_overlap\": 128,\n",
    "        \"k\": 6,\n",
    "        \"search_type\": \"similarity\",\n",
    "        \"description\": \"More context for complex questions\"\n",
    "    },\n",
    "    \"Diverse\": {\n",
    "        \"chunk_size\": 1024,\n",
    "        \"chunk_overlap\": 128,\n",
    "        \"k\": 4,\n",
    "        \"search_type\": \"mmr\",\n",
    "        \"description\": \"Maximum Marginal Relevance for diversity\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuration Matrix:\\n\")\n",
    "print(\"=\" * 80)\n",
    "for config_name, config in configs.items():\n",
    "    print(f\"\\n{config_name}:\")\n",
    "    print(f\"  Chunk Size: {config['chunk_size']}\")\n",
    "    print(f\"  Chunk Overlap: {config['chunk_overlap']}\")\n",
    "    print(f\"  k (retrieval): {config['k']}\")\n",
    "    print(f\"  Search Type: {config['search_type']}\")\n",
    "    print(f\"  Description: {config['description']}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Evaluate All Configurations\n",
    "\n",
    "**Warning**: This will take 15-25 minutes as we're evaluating 4 configurations.\n",
    "\n",
    "For faster testing, you can reduce the number of test questions or configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results for all configurations\n",
    "comparison_results = {}\n",
    "\n",
    "print(\"Starting multi-configuration evaluation...\")\n",
    "print(f\"Total configurations: {len(configs)}\")\n",
    "print(f\"Estimated total time: 20-30 minutes (6 metrics per config)\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for config_name, config in configs.items():\n",
    "    print(f\"\\nEvaluating configuration: {config_name}\")\n",
    "    print(f\"  {config['description']}\")\n",
    "    print(f\"  Progress: {list(configs.keys()).index(config_name) + 1}/{len(configs)}\")\n",
    "    \n",
    "    # Step 1: Re-split documents with new chunk_size\n",
    "    text_splitter_config = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=config[\"chunk_size\"],\n",
    "        chunk_overlap=config[\"chunk_overlap\"],\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    chunks_config = text_splitter_config.split_documents(cloudflow_docs)\n",
    "    print(f\"  âœ“ Created {len(chunks_config)} chunks\")\n",
    "    \n",
    "    # Step 2: Recreate vector store\n",
    "    vectorstore_config = FAISS.from_documents(chunks_config, embeddings)\n",
    "    print(f\"  âœ“ Vector store created\")\n",
    "    \n",
    "    # Step 3: Create retriever with config\n",
    "    if config[\"search_type\"] == \"mmr\":\n",
    "        retriever_config = vectorstore_config.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\"k\": config[\"k\"], \"fetch_k\": config[\"k\"] * 2}\n",
    "        )\n",
    "    else:\n",
    "        retriever_config = vectorstore_config.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": config[\"k\"]}\n",
    "        )\n",
    "    print(f\"  âœ“ Retriever configured\")\n",
    "    \n",
    "    # Step 4: Rebuild RAG chain (same prompt and LLM)\n",
    "    rag_chain_config = (\n",
    "        {\"context\": retriever_config | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    print(f\"  âœ“ RAG chain built\")\n",
    "    \n",
    "    # Step 5: Generate evaluation data\n",
    "    print(f\"  â–º Generating answers for {len(test_cases)} questions...\")\n",
    "    config_eval_data = {\n",
    "        \"user_input\": [],\n",
    "        \"reference\": [],\n",
    "        \"response\": [],\n",
    "        \"retrieved_contexts\": []\n",
    "    }\n",
    "    \n",
    "    for test_case in test_cases:\n",
    "        question = test_case[\"question\"]\n",
    "        ground_truth = test_case[\"ground_truth\"]\n",
    "        \n",
    "        answer = rag_chain_config.invoke(question)\n",
    "        retrieved_docs = retriever_config.invoke(question)\n",
    "        contexts = [doc.page_content for doc in retrieved_docs]\n",
    "        \n",
    "        config_eval_data[\"user_input\"].append(question)\n",
    "        config_eval_data[\"reference\"].append(ground_truth)\n",
    "        config_eval_data[\"response\"].append(answer)\n",
    "        config_eval_data[\"retrieved_contexts\"].append(contexts)\n",
    "    \n",
    "    # Convert to Ragas format\n",
    "    config_samples = [\n",
    "        SingleTurnSample(\n",
    "            user_input=config_eval_data[\"user_input\"][i],\n",
    "            response=config_eval_data[\"response\"][i],\n",
    "            retrieved_contexts=config_eval_data[\"retrieved_contexts\"][i],\n",
    "            reference=config_eval_data[\"reference\"][i]\n",
    "        )\n",
    "        for i in range(len(config_eval_data[\"user_input\"]))\n",
    "    ]\n",
    "    config_eval_dataset = EvaluationDataset(samples=config_samples)\n",
    "    \n",
    "    # Step 6: Initialize ALL 6 metrics for this configuration (with wrapped LLM/embeddings)\n",
    "    config_metrics = [\n",
    "        Faithfulness(llm=ragas_llm),\n",
    "        AnswerRelevancy(embeddings=ragas_embeddings),\n",
    "        ContextPrecision(llm=ragas_llm),\n",
    "        ContextRecall(llm=ragas_llm),\n",
    "        ContextEntityRecall(llm=ragas_llm),\n",
    "        NoiseSensitivity(llm=ragas_llm),\n",
    "    ]\n",
    "    \n",
    "    # Step 7: Run Ragas evaluation (Ragas 0.3.9 API)\n",
    "    print(f\"  â–º Running Ragas evaluation with 6 metrics (this may take 7-12 minutes)...\")\n",
    "    config_result = evaluate(\n",
    "        dataset=config_eval_dataset,\n",
    "        metrics=config_metrics,\n",
    "    )\n",
    "    \n",
    "    # Step 8: Store results\n",
    "    comparison_results[config_name] = config_result.to_pandas()\n",
    "\n",
    "    # Log to OPIK\n",
    "    if OPIK_AVAILABLE:\n",
    "        try:\n",
    "            config_df = comparison_results[config_name]\n",
    "            config_metric_scores = {\n",
    "                'faithfulness': float(config_df['faithfulness'].mean()),\n",
    "                'answer_relevancy': float(config_df['answer_relevancy'].mean()),\n",
    "                'context_precision': float(config_df['context_precision'].mean()),\n",
    "                'context_recall': float(config_df['context_recall'].mean()),\n",
    "                'context_entity_recall': float(config_df['context_entity_recall'].mean()),\n",
    "                'noise_sensitivity': float(config_df['noise_sensitivity'].mean()),\n",
    "            }\n",
    "            config_overall_score = sum(config_metric_scores.values()) / len(config_metric_scores)\n",
    "            \n",
    "            opik_client.log_traces(\n",
    "                project_name=project_name,\n",
    "                name=f\"RAG_Evaluation_{config_name}\",\n",
    "                input={\n",
    "                    \"configuration\": config_name,\n",
    "                    \"description\": config['description'],\n",
    "                    \"num_questions\": len(config_eval_dataset)\n",
    "                },\n",
    "                output=config_metric_scores,\n",
    "                metadata={\n",
    "                    \"ragas_version\": \"0.3.9\",\n",
    "                    \"chunk_size\": config[\"chunk_size\"],\n",
    "                    \"chunk_overlap\": config[\"chunk_overlap\"],\n",
    "                    \"retriever_k\": config[\"k\"],\n",
    "                    \"search_type\": config[\"search_type\"],\n",
    "                    \"embedding_model\": \"text-embedding-3-small\",\n",
    "                    \"llm_model\": \"gpt-4o-mini\",\n",
    "                    \"num_documents\": 18,\n",
    "                    \"num_questions\": len(config_eval_dataset),\n",
    "                    \"overall_score\": config_overall_score,\n",
    "                },\n",
    "                tags=[config_name, \"ragas\", \"rag_evaluation\", \"all_6_metrics\", \"multi_config\"]\n",
    "            )\n",
    "            print(f\"  âœ“ Logged {config_name} to OPIK (Overall: {config_overall_score:.4f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ OPIK logging failed for {config_name}: {e}\")\n",
    "    print(f\"  âœ“ Evaluation complete!\")\n",
    "    print(f\"  Average scores: {comparison_results[config_name][metric_cols].mean().to_dict()}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ“ ALL CONFIGURATIONS EVALUATED!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 9: Results Analysis & Visualization\n",
    "\n",
    "**Difficulty:** ðŸŽ“/ðŸš€ INTERMEDIATE/ADVANCED\n",
    "\n",
    "Now we'll create comprehensive visualizations to compare all configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Summary Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics for each configuration\n",
    "summary_stats = {}\n",
    "\n",
    "for config_name, result_df in comparison_results.items():\n",
    "    metric_scores = result_df[metric_cols].mean()\n",
    "    summary_stats[config_name] = {\n",
    "        'Mean': metric_scores.mean(),\n",
    "        'Std': metric_scores.std(),\n",
    "        'Min': metric_scores.min(),\n",
    "        'Max': metric_scores.max()\n",
    "    }\n",
    "\n",
    "summary_df = pd.DataFrame(summary_stats).T\n",
    "\n",
    "print(\"Summary Statistics Across All Metrics:\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(summary_df.round(3))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Identify best configuration\n",
    "best_config = summary_df['Mean'].idxmax()\n",
    "print(f\"\\nðŸ† BEST OVERALL CONFIGURATION: {best_config}\")\n",
    "print(f\"   Average Score: {summary_df.loc[best_config, 'Mean']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Visualization 1: Grouped Bar Chart\n",
    "\n",
    "Compare all configurations across all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for grouped bar chart\n",
    "metrics_list = metric_cols\n",
    "config_names = list(comparison_results.keys())\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "x = np.arange(len(metrics_list))\n",
    "width = 0.2  # Width of bars\n",
    "\n",
    "# Plot bars for each configuration\n",
    "for i, config in enumerate(config_names):\n",
    "    scores = [comparison_results[config][metric].mean() for metric in metrics_list]\n",
    "    ax.bar(x + i*width, scores, width, label=config, alpha=0.8)\n",
    "\n",
    "# Customize chart\n",
    "ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('RAG Configuration Comparison Across Metrics', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(metrics_list, rotation=45, ha='right')\n",
    "ax.axhline(y=0.7, color='red', linestyle='--', linewidth=1.5, label='Target (0.7)', alpha=0.7)\n",
    "ax.legend(loc='upper left', framealpha=0.9)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle=':')\n",
    "ax.set_ylim(0, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Grouped bar chart shows how each configuration performs on each metric.\")\n",
    "print(\"   Configurations are grouped by metric for easy comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Visualization 2: Radar Chart\n",
    "\n",
    "Show the strength/weakness profile of each configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create radar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "# Calculate angles for each metric\n",
    "angles = np.linspace(0, 2*np.pi, len(metrics_list), endpoint=False).tolist()\n",
    "angles += angles[:1]  # Complete the circle\n",
    "\n",
    "# Plot each configuration\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "for idx, config in enumerate(config_names):\n",
    "    scores = [comparison_results[config][metric].mean() for metric in metrics_list]\n",
    "    scores += scores[:1]  # Complete the circle\n",
    "    \n",
    "    ax.plot(angles, scores, 'o-', linewidth=2, label=config, color=colors[idx])\n",
    "    ax.fill(angles, scores, alpha=0.15, color=colors[idx])\n",
    "\n",
    "# Customize chart\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metrics_list, fontsize=10)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=9)\n",
    "ax.set_title('Configuration Performance Profile\\n(Radar Chart)', \n",
    "             fontsize=14, fontweight='bold', pad=30)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), framealpha=0.9)\n",
    "ax.grid(True, linestyle=':', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸŽ¯ Radar chart shows the overall performance profile of each configuration.\")\n",
    "print(\"   Larger filled areas indicate better overall performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 Visualization 3: Heatmap\n",
    "\n",
    "Color-coded performance grid for quick pattern identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data matrix for heatmap\n",
    "data_matrix = []\n",
    "for config in config_names:\n",
    "    scores = [comparison_results[config][metric].mean() for metric in metrics_list]\n",
    "    data_matrix.append(scores)\n",
    "\n",
    "# Create heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "sns.heatmap(\n",
    "    data_matrix,\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    cmap='RdYlGn',  # Red (low) â†’ Yellow (medium) â†’ Green (high)\n",
    "    center=0.7,      # Center colormap at target threshold\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    xticklabels=metrics_list,\n",
    "    yticklabels=config_names,\n",
    "    cbar_kws={'label': 'Score'},\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray',\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title('RAG Evaluation Heatmap: Configurations Ã— Metrics', \n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Configurations', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ”¥ Heatmap provides quick visual identification of strengths and weaknesses.\")\n",
    "print(\"   Green = Good performance, Yellow = Acceptable, Red = Needs improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 Detailed Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DETAILED ANALYSIS & INSIGHTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Analysis 1: Best configuration per metric\n",
    "print(\"\\n1. BEST CONFIGURATION PER METRIC:\")\n",
    "print(\"-\" * 80)\n",
    "for metric in metrics_list:\n",
    "    best_config_for_metric = max(config_names, \n",
    "                                  key=lambda c: comparison_results[c][metric].mean())\n",
    "    best_score = comparison_results[best_config_for_metric][metric].mean()\n",
    "    print(f\"   {metric:20s}: {best_config_for_metric:15s} (score: {best_score:.3f})\")\n",
    "\n",
    "# Analysis 2: Configuration rankings\n",
    "print(\"\\n2. OVERALL CONFIGURATION RANKINGS:\")\n",
    "print(\"-\" * 80)\n",
    "rankings = summary_df.sort_values('Mean', ascending=False)\n",
    "for rank, (config, row) in enumerate(rankings.iterrows(), 1):\n",
    "    print(f\"   #{rank}. {config:15s} - Average: {row['Mean']:.3f} (Â±{row['Std']:.3f})\")\n",
    "\n",
    "# Analysis 3: Trade-offs\n",
    "print(\"\\n3. KEY TRADE-OFFS:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   Baseline:    Balanced performance across all metrics\")\n",
    "print(\"   Precise:     Better for focused answers, may miss context\")\n",
    "print(\"   Contextual:  More complete information, potential noise\")\n",
    "print(\"   Diverse:     Variety in retrieval, may reduce precision\")\n",
    "\n",
    "# Analysis 4: Recommendations\n",
    "print(\"\\n4. RECOMMENDATIONS:\")\n",
    "print(\"-\" * 80)\n",
    "best_overall = rankings.index[0]\n",
    "print(f\"   â†’ For general use: {best_overall}\")\n",
    "print(f\"   â†’ For accuracy-critical apps: Check highest faithfulness config\")\n",
    "print(f\"   â†’ For complex questions: Contextual (k=6)\")\n",
    "print(f\"   â†’ For exploratory queries: Diverse (MMR)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 10: Best Practices & Summary\n",
    "\n",
    "**Difficulty:** ðŸ”°/ðŸŽ“ BEGINNER/INTERMEDIATE\n",
    "\n",
    "## 10.1 RAG Evaluation Best Practices\n",
    "\n",
    "### âœ… Creating Effective Test Datasets\n",
    "\n",
    "1. **Diversity is key**: Include 15-30 questions covering different:\n",
    "   - Question types (factual, procedural, comparison)\n",
    "   - Difficulty levels (simple to complex)\n",
    "   - Edge cases (missing info, ambiguous queries)\n",
    "\n",
    "2. **High-quality ground truth**:\n",
    "   - Write clear, accurate reference answers\n",
    "   - Include specific details (numbers, names, dates)\n",
    "   - Test \"I don't know\" handling with out-of-scope questions\n",
    "\n",
    "3. **Representative of production**:\n",
    "   - Use real user questions when possible\n",
    "   - Mirror actual query patterns\n",
    "   - Include common pain points\n",
    "\n",
    "### âœ… Choosing Appropriate Metrics\n",
    "\n",
    "**For accuracy-critical applications** (medical, legal, financial):\n",
    "- Prioritize: Faithfulness, Context Recall\n",
    "- Target: >0.8 on both\n",
    "\n",
    "**For customer support / Q&A**:\n",
    "- Prioritize: Answer Relevancy, Faithfulness\n",
    "- Target: >0.7 on both\n",
    "\n",
    "**For exploratory / research**:\n",
    "- Prioritize: Context Recall, Context Precision\n",
    "- Target: >0.6 on both\n",
    "\n",
    "### âœ… Optimization Strategies\n",
    "\n",
    "**If Faithfulness is low (<0.7)**:\n",
    "- Improve prompt to emphasize context adherence\n",
    "- Reduce LLM temperature (make it more deterministic)\n",
    "- Add explicit instruction: \"Only use information from context\"\n",
    "- Use stronger LLM (GPT-4 vs GPT-3.5)\n",
    "\n",
    "**If Answer Relevancy is low (<0.7)**:\n",
    "- Improve prompt focus and clarity\n",
    "- Filter retrieved chunks by relevance score\n",
    "- Add query preprocessing (spell check, expansion)\n",
    "- Tune retrieval parameters (k value)\n",
    "\n",
    "**If Context Precision is low (<0.6)**:\n",
    "- Optimize chunk size (test 512, 1024, 2048)\n",
    "- Adjust k value (test 3, 4, 5, 6)\n",
    "- Improve document metadata for filtering\n",
    "- Consider hybrid search (keyword + semantic)\n",
    "\n",
    "**If Context Recall is low (<0.6)**:\n",
    "- Increase k value (retrieve more chunks)\n",
    "- Improve chunking strategy (reduce overlap)\n",
    "- Enhance document preprocessing\n",
    "- Consider multi-query retrieval\n",
    "\n",
    "### âœ… Regular Evaluation Cycles\n",
    "\n",
    "1. **Initial development**: Evaluate after each major change\n",
    "2. **Pre-production**: Full evaluation on comprehensive test set\n",
    "3. **Production**: \n",
    "   - Weekly automated evaluation\n",
    "   - Monthly review of failed queries\n",
    "   - Quarterly full re-evaluation\n",
    "\n",
    "\n",
    "### âœ… Experiment Tracking with OPIK\n",
    "\n",
    "**Track evaluations for better insights**:\n",
    "\n",
    "1. **Log all evaluation runs**:\n",
    "   - Automatically log metric scores\n",
    "   - Track configuration parameters\n",
    "   - Record timestamps and metadata\n",
    "\n",
    "2. **Compare experiments**:\n",
    "   - Visualize metric trends over time\n",
    "   - Compare different RAG configurations\n",
    "   - Identify best-performing setups\n",
    "\n",
    "3. **Team collaboration**:\n",
    "   - Share results with teammates\n",
    "   - Comment on specific evaluations\n",
    "   - Build institutional knowledge\n",
    "\n",
    "4. **Production monitoring**:\n",
    "   - Track live RAG performance\n",
    "   - Alert on metric degradation\n",
    "   - Correlate with user feedback\n",
    "\n",
    "**Setup**: Install OPIK (`pip install opik`), configure with API key, and the notebook will automatically log all evaluations.\n",
    "\n",
    "### âœ… Cost Management\n",
    "\n",
    "**Reduce evaluation costs**:\n",
    "- Use smaller test set for rapid iteration (5-10 questions)\n",
    "- Use full test set (15-30 questions) before deployment\n",
    "- Cache evaluation results for unchanged configs\n",
    "- Use non-LLM metrics for quick checks\n",
    "\n",
    "**Estimated costs** (with GPT-4o-mini):\n",
    "- 10 questions, 4 metrics: ~$0.20-0.40\n",
    "- 20 questions, 4 metrics: ~$0.40-0.80\n",
    "- 4 configs Ã— 20 questions: ~$1.60-3.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Ragas Metrics Cheat Sheet\n",
    "\n",
    "| Metric | What It Measures | When to Use | How to Optimize If Low |\n",
    "|--------|-----------------|-------------|------------------------|\n",
    "| **Faithfulness** | Answer grounded in context | Always (prevents hallucinations) | Improve prompt, reduce temperature, use stronger LLM |\n",
    "| **Answer Relevancy** | Answer matches question | Always (ensures on-topic) | Improve prompt focus, filter contexts, preprocess queries |\n",
    "| **Context Precision** | Relevant chunks ranked high | When retrieval quality matters | Optimize chunk size, adjust k, improve metadata |\n",
    "| **Context Recall** | All needed info retrieved | When completeness matters | Increase k, improve chunking, multi-query retrieval |\n",
    "\n",
    "### Target Thresholds\n",
    "\n",
    "| Application Type | Faithfulness | Answer Relevancy | Context Precision | Context Recall |\n",
    "|-----------------|--------------|------------------|-------------------|----------------|\n",
    "| **High-stakes** (medical, legal) | >0.85 | >0.80 | >0.70 | >0.75 |\n",
    "| **Production** (customer support) | >0.75 | >0.75 | >0.65 | >0.65 |\n",
    "| **Development** (testing) | >0.65 | >0.65 | >0.55 | >0.55 |\n",
    "| **Prototype** (MVP) | >0.55 | >0.55 | >0.45 | >0.45 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Configuration Selection Guide\n",
    "\n",
    "### When to use different configurations:\n",
    "\n",
    "**Baseline (chunk_size=1024, k=4, similarity)**\n",
    "- âœ… General-purpose applications\n",
    "- âœ… Balanced performance and cost\n",
    "- âœ… Good starting point for tuning\n",
    "\n",
    "**Precise (chunk_size=512, k=4, similarity)**\n",
    "- âœ… Technical Q&A requiring specific details\n",
    "- âœ… When answer brevity is important\n",
    "- âœ… Highly structured documentation\n",
    "- âš ï¸ May miss broader context\n",
    "\n",
    "**Contextual (chunk_size=1024, k=6, similarity)**\n",
    "- âœ… Complex, multi-part questions\n",
    "- âœ… When comprehensive answers needed\n",
    "- âœ… Exploratory or research queries\n",
    "- âš ï¸ Higher cost (more tokens)\n",
    "- âš ï¸ Potential information overload\n",
    "\n",
    "**Diverse (chunk_size=1024, k=4, mmr)**\n",
    "- âœ… Broad topic exploration\n",
    "- âœ… When query intent is ambiguous\n",
    "- âœ… Multi-aspect questions\n",
    "- âš ï¸ May reduce precision for focused queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Production Deployment Checklist\n",
    "\n",
    "Before deploying your RAG system to production:\n",
    "\n",
    "### âœ… Evaluation Requirements\n",
    "- [ ] Faithfulness score >0.75 on test set\n",
    "- [ ] Answer Relevancy score >0.75 on test set\n",
    "- [ ] Context Precision score >0.65 on test set\n",
    "- [ ] Context Recall score >0.65 on test set\n",
    "- [ ] Tested on minimum 20 diverse questions\n",
    "- [ ] Edge cases handled (missing info, ambiguous queries)\n",
    "\n",
    "### âœ… Monitoring Setup\n",
    "- [ ] Logging all queries and responses\n",
    "- [ ] Tracking response times\n",
    "- [ ] Monitoring API costs\n",
    "- [ ] Collecting user feedback\n",
    "- [ ] Alerting for quality degradation\n",
    "\n",
    "### âœ… Quality Assurance\n",
    "- [ ] Manual review of sample outputs\n",
    "- [ ] A/B testing of configurations\n",
    "- [ ] Load testing for performance\n",
    "- [ ] Failover and error handling tested\n",
    "\n",
    "### âœ… Documentation\n",
    "- [ ] System architecture documented\n",
    "- [ ] Configuration parameters recorded\n",
    "- [ ] Evaluation results archived\n",
    "- [ ] Known limitations documented\n",
    "\n",
    "### âœ… Continuous Improvement\n",
    "- [ ] Weekly automated evaluation\n",
    "- [ ] Monthly review of failed queries\n",
    "- [ ] Quarterly full re-evaluation\n",
    "- [ ] Process for adding new test cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Summary and Key Takeaways\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. âœ… Created custom CloudFlow documentation (18 documents)\n",
    "2. âœ… Implemented a complete RAG pipeline with LCEL\n",
    "3. âœ… Integrated Ragas evaluation framework\n",
    "4. âœ… Created comprehensive test dataset (18 Q&A pairs)\n",
    "5. âœ… Evaluated with all 6 non-multimodal Ragas metrics\n",
    "6. âœ… Compared 4 different RAG configurations\n",
    "7. âœ… Created professional visualizations\n",
    "8. âœ… Analyzed results and identified best configuration\n",
    "9. âœ… Integrated OPIK for experiment tracking and monitoring\n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "**Evaluation is essential**:\n",
    "- You can't improve what you don't measure\n",
    "- Ragas provides standardized, reliable metrics\n",
    "- Regular evaluation catches quality issues early\n",
    "\n",
    "**Configuration matters**:\n",
    "- No one-size-fits-all solution\n",
    "- Different configs excel at different tasks\n",
    "- Test multiple approaches systematically\n",
    "\n",
    "**Balance trade-offs**:\n",
    "- Precision vs Recall\n",
    "- Cost vs Quality\n",
    "- Speed vs Accuracy\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Immediate**:\n",
    "1. Apply this evaluation approach to your own RAG system\n",
    "2. Create a test dataset for your domain\n",
    "3. Run baseline evaluation\n",
    "\n",
    "**Short-term**:\n",
    "1. Test additional configurations (temperature, prompts, models)\n",
    "2. Expand test dataset to 30-50 questions\n",
    "3. Implement automated evaluation pipeline\n",
    "\n",
    "**Long-term**:\n",
    "1. Set up continuous evaluation in production\n",
    "2. Build feedback loop with real user queries\n",
    "3. Explore advanced techniques (hybrid search, re-ranking, query transformation)\n",
    "\n",
    "### Resources for Further Learning\n",
    "\n",
    "**Ragas Documentation**:\n",
    "- Official docs: https://docs.ragas.io\n",
    "- GitHub: https://github.com/explodinggradients/ragas\n",
    "\n",
    "**LangChain Resources**:\n",
    "- RAG tutorials: https://python.langchain.com/docs/use_cases/question_answering/\n",
    "- LCEL guide: https://python.langchain.com/docs/expression_language/\n",
    "\n",
    "**RAG Best Practices**:\n",
    "- Pinecone RAG guide: https://www.pinecone.io/learn/retrieval-augmented-generation/\n",
    "- OpenAI RAG patterns: https://platform.openai.com/docs/guides/retrieval-augmented-generation\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've completed the RAG Evaluation with Ragas notebook! You now have:\n",
    "\n",
    "- âœ… Practical experience with Ragas framework\n",
    "- âœ… Understanding of key RAG evaluation metrics\n",
    "- âœ… Skills to compare configurations systematically\n",
    "- âœ… Production-ready evaluation patterns\n",
    "\n",
    "**You're now equipped to build, evaluate, and optimize production-quality RAG systems!** ðŸš€\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
