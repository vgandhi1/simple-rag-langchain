{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG (Retrieval-Augmented Generation) Implementation with LangChain 1.0+\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates a complete RAG pipeline using **LangChain 1.0+ with LCEL** (LangChain Expression Language).\n",
    "\n",
    "### What is RAG?\n",
    "RAG combines retrieval of relevant documents with generation from a Large Language Model (LLM):\n",
    "1. **Retrieval**: Find relevant information from a knowledge base\n",
    "2. **Augmentation**: Add retrieved context to the prompt\n",
    "3. **Generation**: LLM generates answers based on the context\n",
    "\n",
    "### Pipeline Flow:\n",
    "```\n",
    "PDF Documents ‚Üí Load ‚Üí Split into Chunks ‚Üí Create Embeddings ‚Üí Store in Vector DB\n",
    "                                                                         ‚Üì\n",
    "User Query ‚Üí Retrieve Similar Chunks ‚Üí Combine with Query ‚Üí LLM ‚Üí Answer\n",
    "```\n",
    "\n",
    "### Components Used:\n",
    "- **Document Loader**: PyPDFLoader (for PDF processing)\n",
    "- **Text Splitter**: RecursiveCharacterTextSplitter (smart chunking)\n",
    "- **Embeddings**: OpenAI text-embedding-3-small (vector representations)\n",
    "- **Vector Store**: FAISS (fast similarity search)\n",
    "- **LLM**: OpenAI GPT-4-Turbo or GPT-3.5-Turbo\n",
    "- **Chain Builder**: LCEL (LangChain Expression Language)\n",
    "\n",
    "### LangChain 1.0+ Features:\n",
    "- ‚úÖ Modern LCEL syntax with pipe operator `|`\n",
    "- ‚úÖ More readable and composable chains\n",
    "- ‚úÖ Better streaming support\n",
    "- ‚úÖ Type-safe operations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup\n",
    "\n",
    "First, install all required packages. Make sure you have Python 3.9+ installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Uncomment and run ONE of the following options:\n",
    "\n",
    "# Option 1: Install from requirements.txt (RECOMMENDED)\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "# Option 2: Install all packages individually\n",
    "# !pip install langchain langchain-core langchain-openai langchain-community langchain-text-splitters faiss-cpu pypdf python-dotenv tiktoken jupyter notebook\n",
    "\n",
    "# Option 3: Quick install (if you're having import issues)\n",
    "# !pip install --upgrade langchain langchain-core langchain-openai langchain-community langchain-text-splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries\n",
    "\n",
    "Import all necessary modules with explanations of what each does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Installation\n",
    "\n",
    "If you encounter import errors, run this cell first to check package versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-core langchain-openai langchain-community langchain-text-splitters faiss-cpu pypdf python-dotenv tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì langchain: 0.3.27\n",
      "‚úì langchain-core: 0.3.79\n",
      "‚úì langchain-openai: 0.3.35\n",
      "‚úì langchain-community: 0.3.31\n",
      "\n",
      "Python version: 3.13.2 (main, Mar 17 2025, 21:26:38) [Clang 20.1.0 ]\n",
      "\n",
      "If any packages are missing, run:\n",
      "pip install langchain langchain-core langchain-openai langchain-community langchain-text-splitters faiss-cpu pypdf python-dotenv tiktoken\n"
     ]
    }
   ],
   "source": [
    "# Check installed package versions\n",
    "import sys\n",
    "from importlib.metadata import version\n",
    "\n",
    "try:\n",
    "    import langchain\n",
    "    print(f\"‚úì langchain: {langchain.__version__}\")\n",
    "except:\n",
    "    print(\"‚úó langchain not installed\")\n",
    "\n",
    "try:\n",
    "    import langchain_core\n",
    "    print(f\"‚úì langchain-core: {langchain_core.__version__}\")\n",
    "except:\n",
    "    print(\"‚úó langchain-core not installed - REQUIRED!\")\n",
    "    print(\"  Run: pip install langchain-core\")\n",
    "\n",
    "try:\n",
    "    import langchain_openai\n",
    "    print(f\"‚úì langchain-openai: {version('langchain-openai')}\")\n",
    "except:\n",
    "    print(\"‚úó langchain-openai not installed\")\n",
    "\n",
    "try:\n",
    "    import langchain_community\n",
    "    print(f\"‚úì langchain-community: {langchain_community.__version__}\")\n",
    "except:\n",
    "    print(\"‚úó langchain-community not installed\")\n",
    "\n",
    "print(f\"\\nPython version: {sys.version}\")\n",
    "print(\"\\nIf any packages are missing, run:\")\n",
    "print(\"pip install langchain langchain-core langchain-openai langchain-community langchain-text-splitters faiss-cpu pypdf python-dotenv tiktoken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All imports successful!\n",
      "‚úì Compatible with LangChain 1.0+\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "from pathlib import Path\n",
    "\n",
    "# Environment variable management - for secure API key handling\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain Document Loaders - for loading PDF documents\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# LangChain Text Splitters - for breaking documents into manageable chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# OpenAI Integration - for embeddings and LLM\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "# Vector Store - FAISS for efficient similarity search\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# LangChain Core Components\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "print(\"‚úì All imports successful!\")\n",
    "print(\"‚úì Compatible with LangChain 1.0+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Environment Configuration\n",
    "\n",
    "### Setting up OpenAI API Key\n",
    "\n",
    "You have two options:\n",
    "1. **Recommended**: Create a `.env` file with `OPENAI_API_KEY=your_key_here`\n",
    "2. **Alternative**: Set it directly in code (not recommended for production)\n",
    "\n",
    "Get your API key from: https://platform.openai.com/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì OpenAI API Key loaded successfully!\n",
      "‚úì Key starts with: sk-proj-...\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key is loaded\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚ö†Ô∏è  WARNING: OPENAI_API_KEY not found!\")\n",
    "    print(\"Please set it in .env file or uncomment the line below:\")\n",
    "    # os.environ[\"OPENAI_API_KEY\"] = \"your_api_key_here\"\n",
    "else:\n",
    "    print(\"‚úì OpenAI API Key loaded successfully!\")\n",
    "    print(f\"‚úì Key starts with: {os.getenv('OPENAI_API_KEY')[:8]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Document Loading\n",
    "\n",
    "### Loading PDF Documents\n",
    "\n",
    "PyPDFLoader extracts text from PDF files page by page. Each page becomes a separate document with metadata (page number, source file).\n",
    "\n",
    "**How it works:**\n",
    "- Reads PDF files and extracts text content\n",
    "- Preserves page numbers for source tracking\n",
    "- Returns Document objects with `.page_content` and `.metadata`\n",
    "\n",
    "**Note**: Update the `pdf_path` variable to point to your PDF file(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 15 pages from 'attention.pdf'\n",
      "\n",
      "--- First Document Preview ---\n",
      "Content (first 500 chars): Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani‚àó\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer‚àó\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar‚àó\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit‚àó\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones‚àó\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez‚àó ‚Ä†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "≈Åukasz ...\n",
      "\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Total characters across all pages: 39,587\n"
     ]
    }
   ],
   "source": [
    "# ===== CONFIGURATION: Update this path to your PDF file =====\n",
    "pdf_path = \"attention.pdf\"  # Change this to your PDF file path\n",
    "# =============================================================\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"‚ö†Ô∏è  ERROR: File '{pdf_path}' not found!\")\n",
    "    print(\"Please update the pdf_path variable with your PDF file location.\")\n",
    "else:\n",
    "    # Initialize the PDF loader\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    \n",
    "    # Load all pages from the PDF\n",
    "    # Each page becomes a separate Document object\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Display information about loaded documents\n",
    "    print(f\"‚úì Loaded {len(documents)} pages from '{pdf_path}'\")\n",
    "    print(f\"\\n--- First Document Preview ---\")\n",
    "    print(f\"Content (first 500 chars): {documents[0].page_content[:500]}...\")\n",
    "    print(f\"\\nMetadata: {documents[0].metadata}\")\n",
    "    print(f\"\\nTotal characters across all pages: {sum(len(doc.page_content) for doc in documents):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Multiple PDFs (Optional)\n",
    "\n",
    "If you have multiple PDF files, you can load them all at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 PDF files\n",
      "  ‚úì Loaded 19 pages from rag.pdf\n",
      "  ‚úì Loaded 21 pages from ragsurvey.pdf\n",
      "\n",
      "Total pages loaded: 40\n"
     ]
    }
   ],
   "source": [
    "# Example: Loading multiple PDFs from a directory\n",
    "# Uncomment and modify if you want to load multiple files\n",
    "\n",
    "pdf_directory = \"./pdfs\"  # Directory containing your PDFs\n",
    "all_documents = []\n",
    "\n",
    "if os.path.exists(pdf_directory):\n",
    "    pdf_files = list(Path(pdf_directory).glob(\"*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        loader = PyPDFLoader(str(pdf_file))\n",
    "        docs = loader.load()\n",
    "        all_documents.extend(docs)\n",
    "        print(f\"  ‚úì Loaded {len(docs)} pages from {pdf_file.name}\")\n",
    "    \n",
    "    print(f\"\\nTotal pages loaded: {len(all_documents)}\")\n",
    "    documents = all_documents  # Use this for the rest of the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Splitting\n",
    "\n",
    "### Why Split Documents?\n",
    "- LLMs have token limits (e.g., 4K, 8K, 128K tokens)\n",
    "- Smaller chunks = more precise retrieval\n",
    "- Balance: chunks must be large enough to contain meaningful context but small enough to be specific\n",
    "\n",
    "### RecursiveCharacterTextSplitter\n",
    "This splitter tries to keep related text together by recursively splitting on:\n",
    "1. Paragraphs (`\\n\\n`)\n",
    "2. Lines (`\\n`)\n",
    "3. Sentences (`. `)\n",
    "4. Words (` `)\n",
    "5. Characters (as last resort)\n",
    "\n",
    "**Parameters:**\n",
    "- `chunk_size=1024`: Target size for each chunk (in characters)\n",
    "- `chunk_overlap=128`: Overlap between chunks to maintain context continuity\n",
    "- Overlap prevents important information from being split across chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Split 15 documents into 49 chunks\n",
      "\n",
      "Average chunk size: 873 characters\n",
      "\n",
      "--- Chunk Examples ---\n",
      "\n",
      "Chunk 1 (length: 986 chars):\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Chunk 2 (length: 944 chars):\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more pa...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Chunk 3 (length: 986 chars):\n",
      "‚àóEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Tra...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text splitter with recommended settings\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,        # Maximum characters per chunk (roughly 200-250 tokens)\n",
    "    chunk_overlap=128,      # Characters overlap between chunks (maintains context)\n",
    "    length_function=len,    # Function to measure chunk length\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Try to split on paragraphs first, then lines, etc.\n",
    ")\n",
    "\n",
    "# Split the documents into chunks\n",
    "# This creates smaller, manageable pieces while preserving semantic meaning\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Display splitting results\n",
    "print(f\"‚úì Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "print(f\"\\nAverage chunk size: {sum(len(chunk.page_content) for chunk in chunks) / len(chunks):.0f} characters\")\n",
    "\n",
    "# Preview a few chunks\n",
    "print(f\"\\n--- Chunk Examples ---\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1} (length: {len(chunk.page_content)} chars):\")\n",
    "    print(f\"{chunk.page_content[:200]}...\")\n",
    "    print(f\"Metadata: {chunk.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating Embeddings\n",
    "\n",
    "### What are Embeddings?\n",
    "Embeddings are vector representations of text that capture semantic meaning. Similar texts have similar vectors.\n",
    "\n",
    "**Example**: \n",
    "- \"dog\" and \"puppy\" ‚Üí similar vectors (close in vector space)\n",
    "- \"dog\" and \"spaceship\" ‚Üí different vectors (far apart)\n",
    "\n",
    "### OpenAI text-embedding-3-small\n",
    "- **Dimensions**: 1536 (each text becomes a 1536-dimensional vector)\n",
    "- **Cost**: $0.00002 per 1,000 tokens (very affordable)\n",
    "- **Performance**: 62.3% on MTEB benchmark\n",
    "- **Speed**: Fast and efficient\n",
    "\n",
    "**Alternative**: `text-embedding-3-large` for higher quality (64.6% MTEB) at higher cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embeddings model initialized: text-embedding-3-small\n",
      "‚úì Embedding dimension: 1536\n",
      "‚úì Sample embedding (first 10 values): [0.020370882004499435, -0.0031641265377402306, -0.0005454652709886432, 0.0045827641151845455, -0.015004359185695648, -0.034060992300510406, 0.0176328606903553, 0.01959054544568062, 0.0013125392142683268, 0.00596546521410346]\n",
      "\n",
      "‚ÑπÔ∏è  Each chunk will be converted to a 1536-dimensional vector for similarity search\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI Embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # Latest, cost-effective embedding model\n",
    "    # dimensions=1536\n",
    "    # Alternative: \"text-embedding-3-large\" for better quality\n",
    ")\n",
    "\n",
    "# Test the embeddings with a sample text\n",
    "sample_text = \"This is a test sentence to demonstrate embeddings.\"\n",
    "sample_embedding = embeddings.embed_query(sample_text)\n",
    "\n",
    "print(f\"‚úì Embeddings model initialized: text-embedding-3-small\")\n",
    "print(f\"‚úì Embedding dimension: {len(sample_embedding)}\")\n",
    "print(f\"‚úì Sample embedding (first 10 values): {sample_embedding[:10]}\")\n",
    "print(f\"\\n‚ÑπÔ∏è  Each chunk will be converted to a {len(sample_embedding)}-dimensional vector for similarity search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Gemini Embeddings (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the code for Gemini Embeddings:\n",
    "\n",
    "# from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# # Test the embeddings with a sample text\n",
    "# sample_text = \"This is a test sentence to demonstrate embeddings.\"\n",
    "# # sample_embedding = embeddings.embed_query(sample_text)\n",
    "\n",
    "# print(f\"‚úì Embeddings model initialized: text-embedding-3-small\")\n",
    "# print(f\"‚úì Embedding dimension: {len(sample_embedding)}\")\n",
    "# print(f\"‚úì Sample embedding (first 10 values): {sample_embedding[:10]}\")\n",
    "# print(f\"‚úì Each chunk will be converted to a {len(sample_embedding)}-dimensional vector for similarity search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Creating Vector Store (FAISS)\n",
    "\n",
    "### What is a Vector Store?\n",
    "A vector store (or vector database) stores embeddings and enables fast similarity search.\n",
    "\n",
    "### FAISS (Facebook AI Similarity Search)\n",
    "- **Fast**: Optimized for billion-scale vector search\n",
    "- **Local**: Runs on your machine, no cloud dependency\n",
    "- **Efficient**: Uses advanced indexing algorithms\n",
    "\n",
    "### How Similarity Search Works:\n",
    "1. Convert query to embedding vector\n",
    "2. Find vectors in the database most similar to query vector (using cosine similarity or Euclidean distance)\n",
    "3. Return the corresponding text chunks\n",
    "\n",
    "**This cell will:**\n",
    "1. Convert all chunks to embeddings (may take a minute for large documents)\n",
    "2. Build a FAISS index\n",
    "3. Save to disk for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS-CPU Version Compatibility Issue\n",
    "\n",
    "If you encounter issues with `faiss-cpu` installation, try:\n",
    "\n",
    "```bash\n",
    "uv pip uninstall faiss-cpu\n",
    "uv pip install faiss-cpu==1.12.0\n",
    "```\n",
    "\n",
    "Or for conda users:\n",
    "```bash\n",
    "conda install -c conda-forge faiss-cpu==1.12.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: faiss-cpu\n",
      "Version: 1.11.0\n",
      "Location: /Users/sourangshupal/Downloads/simple-rag-langchain/.venv/lib/python3.13/site-packages\n",
      "Requires: numpy, packaging\n",
      "Required-by:\n"
     ]
    }
   ],
   "source": [
    "!uv pip show faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS index from 49 chunks...\n",
      "This may take a minute depending on the number of chunks...\n",
      "‚úì FAISS vector store created successfully!\n",
      "‚úì Indexed 49 document chunks\n",
      "‚úì Vector store saved to './faiss_index'\n",
      "\n",
      "‚ÑπÔ∏è  You can reload this index later using: FAISS.load_local('./faiss_index', embeddings)\n"
     ]
    }
   ],
   "source": [
    "# Create FAISS vector store from document chunks\n",
    "# This step converts each chunk to an embedding and stores it\n",
    "print(f\"Creating FAISS index from {len(chunks)} chunks...\")\n",
    "print(\"This may take a minute depending on the number of chunks...\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=chunks,      # Our split document chunks\n",
    "    embedding=embeddings   # OpenAI embedding model\n",
    ")\n",
    "\n",
    "print(f\"‚úì FAISS vector store created successfully!\")\n",
    "print(f\"‚úì Indexed {len(chunks)} document chunks\")\n",
    "\n",
    "# Save the vector store to disk for later use\n",
    "# This allows you to reload the index without re-processing documents\n",
    "vectorstore_path = \"./faiss_index\"\n",
    "vectorstore.save_local(vectorstore_path)\n",
    "print(f\"‚úì Vector store saved to '{vectorstore_path}'\")\n",
    "print(f\"\\n‚ÑπÔ∏è  You can reload this index later using: FAISS.load_local('{vectorstore_path}', embeddings)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChromaDB Vector Store (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ChromaDB from 49 chunks...\n",
      "‚úì ChromaDB vector store created!\n"
     ]
    }
   ],
   "source": [
    "#ChromaDB has better Python 3.13 support. Replace cells 21-24 with:\n",
    "\n",
    "# Instead of FAISS, use ChromaDB\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# # Create ChromaDB vector store\n",
    "# print(f\"Creating ChromaDB from {len(chunks)} chunks...\")\n",
    "# vectorstore = Chroma.from_documents(\n",
    "#     documents=chunks,\n",
    "#     embedding=embeddings,\n",
    "#     persist_directory=\"./chroma_db\"\n",
    "# )\n",
    "# print(\"‚úì ChromaDB vector store created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a Saved Vector Store (Optional)\n",
    "\n",
    "If you've already created a vector store, you can load it instead of recreating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment to load an existing vector store instead of creating a new one\n",
    "# vectorstore_path = \"./faiss_index\"\n",
    "# vectorstore = FAISS.load_local(\n",
    "#     vectorstore_path, \n",
    "#     embeddings,\n",
    "#     allow_dangerous_deserialization=True  # Required for loading pickled data\n",
    "# )\n",
    "# print(f\"‚úì Loaded existing vector store from '{vectorstore_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Retriever configured successfully\n",
      "  - Search type: similarity\n",
      "  - Number of documents to retrieve (k): 4\n",
      "\n",
      "--- Retriever Test ---\n",
      "Query: 'What is the main topic of this document?'\n",
      "Retrieved 4 documents:\n",
      "\n",
      "Document 1:\n",
      "  Content preview: Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "re...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}\n",
      "\n",
      "Document 2:\n",
      "  Content preview: Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}\n",
      "\n",
      "Document 3:\n",
      "  Content preview: Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}\n",
      "\n",
      "Document 4:\n",
      "  Content preview: translation system: Bridging the gap between human and machine translation. arXiv preprint\n",
      "arXiv:1609.08144, 2016.\n",
      "[39] Jie Zhou, Ying Cao, Xuguang Wa...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}\n"
     ]
    }
   ],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",    # Use cosine similarity for search\n",
    "    search_kwargs={\"k\": 4}        # Retrieve top 4 most relevant chunks\n",
    ")\n",
    "\n",
    "print(\"‚úì Retriever configured successfully\")\n",
    "print(f\"  - Search type: similarity\")\n",
    "print(f\"  - Number of documents to retrieve (k): 4\")\n",
    "\n",
    "# Test the retriever with a sample query\n",
    "# Note: In LangChain 1.0+, use .invoke() instead of .get_relevant_documents()\n",
    "test_query = \"What is the main topic of this document?\"\n",
    "retrieved_docs = retriever.invoke(test_query)  # LangChain 1.0+ method\n",
    "\n",
    "print(f\"\\n--- Retriever Test ---\")\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Content preview: {doc.page_content[:150]}...\")\n",
    "    print(f\"  Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Configuring the Language Model (LLM)\n",
    "\n",
    "### LLM Selection\n",
    "The LLM generates the final answer based on retrieved context.\n",
    "\n",
    "### Available Models:\n",
    "1. **gpt-4-turbo-2025-04-09**: Most capable, best quality, slower, more expensive\n",
    "2. **gpt-4o**: Fast GPT-4 level performance, good balance\n",
    "3. **gpt-3.5-turbo**: Fast and cheap, good for simpler queries\n",
    "\n",
    "### Temperature:\n",
    "- **0**: Deterministic, focused answers (recommended for factual Q&A)\n",
    "- **0.7**: More creative, varied responses\n",
    "- **1.0**: Most creative, less predictable\n",
    "\n",
    "### Max Tokens:\n",
    "Controls the maximum length of the generated response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LLM configured successfully\n",
      "  - Model: gpt-4-turbo-2024-04-09\n",
      "  - Temperature: 0 (deterministic)\n",
      "  - Max tokens: 2000\n",
      "\n",
      "LLM Test Response: Hello, I am ready to answer questions!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ChatOpenAI model\n",
    "llm = ChatOpenAI(\n",
    "      model=\"gpt-4-turbo-2024-04-09\",  # Choose your model\n",
    "      # Alternative options:\n",
    "      # model=\"gpt-4o\",           # Faster GPT-4 performance, good \n",
    "      # balance,\n",
    "      # model=\"gpt-3.5-turbo\",    # Faster and cheaper option\n",
    "\n",
    "      temperature=0,         # 0 = deterministic, factual responses (recommended for Q&A)\n",
    "      max_tokens=2000,       # Maximum length of response\n",
    "  )\n",
    "\n",
    "print(\"‚úì LLM configured successfully\")\n",
    "print(f\"  - Model: gpt-4-turbo-2024-04-09\")\n",
    "print(f\"  - Temperature: 0 (deterministic)\")\n",
    "print(f\"  - Max tokens: 2000\")\n",
    "\n",
    "# Test the LLM with a simple query\n",
    "test_response = llm.invoke(\"Say 'Hello, I am ready to answer questions!'\")\n",
    "print(f\"\\nLLM Test Response: {test_response.content}\")\n",
    "\n",
    "#   üìù Explanation of Parameters:\n",
    "\n",
    "#   Model Selection:\n",
    "\n",
    "#   # Option 1: Best quality (slower, more expensive)\n",
    "#   llm = ChatOpenAI(model=\"gpt-4-turbo-2024-04-09\")\n",
    "\n",
    "#   # Option 2: Fast GPT-4 performance (balanced)\n",
    "#   llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "#   # Option 3: Fast and cheap (good for testing)\n",
    "#   llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "#   Temperature:\n",
    "\n",
    "#   temperature=0    # Deterministic, focused (best for factual Q&A)\n",
    "#   temperature=0.7  # More creative, varied responses\n",
    "#   temperature=1.0  # Most creative, less predictable\n",
    "\n",
    "#   Max Tokens:\n",
    "\n",
    "#   max_tokens=2000  # Controls maximum response length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Creating the RAG Chain (LangChain 1.0+ LCEL)\n",
    "\n",
    "### What is a RAG Chain?\n",
    "The RAG chain combines retrieval and generation into a single workflow:\n",
    "1. User asks a question\n",
    "2. Retriever finds relevant documents\n",
    "3. Documents are formatted as context\n",
    "4. LLM generates answer using the context\n",
    "\n",
    "### LangChain 1.0+ LCEL (LangChain Expression Language)\n",
    "LangChain 1.0+ uses LCEL, a declarative way to build chains using the pipe operator `|`.\n",
    "\n",
    "**Benefits:**\n",
    "- More intuitive and readable\n",
    "- Better streaming support\n",
    "- Easier to debug and modify\n",
    "- Type-safe and composable\n",
    "\n",
    "**Components:**\n",
    "- **RunnablePassthrough**: Passes input through unchanged\n",
    "- **Pipe operator (|)**: Chains components together\n",
    "- **StrOutputParser**: Converts LLM output to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Retriever configured successfully\n",
      "  - Search type: similarity\n",
      "  - Number of documents to retrieve (k): 4\n",
      "\n",
      "--- Retriever Test ---\n",
      "Query: 'What is the main topic of this document?'\n",
      "Retrieved 4 documents:\n",
      "\n",
      "Document 1:\n",
      "  Content preview: Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "re...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}\n",
      "\n",
      "Document 2:\n",
      "  Content preview: Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}\n",
      "\n",
      "Document 3:\n",
      "  Content preview: Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}\n",
      "\n",
      "Document 4:\n",
      "  Content preview: translation system: Bridging the gap between human and machine translation. arXiv preprint\n",
      "arXiv:1609.08144, 2016.\n",
      "[39] Jie Zhou, Ying Cao, Xuguang Wa...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Create the Retriever \n",
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "      search_type=\"similarity\",    # Use cosine similarity for search\n",
    "      search_kwargs={\"k\": 4}        # Retrieve top 4 most relevant chunks\n",
    "  )\n",
    "\n",
    "print(\"‚úì Retriever configured successfully\")\n",
    "print(f\"  - Search type: similarity\")\n",
    "print(f\"  - Number of documents to retrieve (k): 4\")\n",
    "\n",
    "  # Test the retriever with a sample query\n",
    "test_query = \"What is the main topic of this document?\"\n",
    "retrieved_docs = retriever.invoke(test_query)\n",
    "\n",
    "print(f\"\\n--- Retriever Test ---\")\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Content preview: {doc.page_content[:150]}...\")\n",
    "    print(f\"  Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì RAG chain created successfully using LangChain 1.0+ LCEL!\n",
      "\n",
      "RAG Pipeline Flow:\n",
      "  1. User provides a query\n",
      "  2. Retriever finds top 4 relevant chunks\n",
      "  3. Chunks are formatted as context\n",
      "  4. Context + question are formatted with prompt template\n",
      "  5. LLM generates answer based on context\n",
      "  6. Answer is parsed and returned to user\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt template for the RAG system\n",
    "# This tells the LLM how to use the retrieved context\n",
    "system_prompt = (\n",
    "    \"You are a helpful assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer based on the context, say that you don't know. \"\n",
    "    \"Keep the answer concise and accurate.\\n\\n\"\n",
    "    \"Context: {context}\\n\\n\"\n",
    "    \"Question: {question}\"\n",
    ")\n",
    "\n",
    "# Create the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(system_prompt)\n",
    "\n",
    "# Helper function to format documents\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format retrieved documents into a single string.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build the RAG chain using LangChain 1.0+ LCEL (LangChain Expression Language)\n",
    "# This uses the pipe operator (|) to chain components together\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,  # Retrieve docs and format them\n",
    "        \"question\": RunnablePassthrough()      # Pass through the question\n",
    "    }\n",
    "    | prompt           # Format with prompt template\n",
    "    | llm              # Generate answer with LLM\n",
    "    | StrOutputParser() # Parse output to string\n",
    ")\n",
    "\n",
    "print(\"‚úì RAG chain created successfully using LangChain 1.0+ LCEL!\")\n",
    "print(\"\\nRAG Pipeline Flow:\")\n",
    "print(\"  1. User provides a query\")\n",
    "print(\"  2. Retriever finds top 4 relevant chunks\")\n",
    "print(\"  3. Chunks are formatted as context\")\n",
    "print(\"  4. Context + question are formatted with prompt template\")\n",
    "print(\"  5. LLM generates answer based on context\")\n",
    "print(\"  6. Answer is parsed and returned to user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the main topic or subject of this document?\n",
      "\n",
      "Processing...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "The main topic of the document is the introduction and explanation of the Transformer network architecture, which is based solely on attention mechanisms and does not use recurrence or convolutions. This architecture is used in sequence transduction models that involve an encoder and a decoder connected through an attention mechanism.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "SOURCE DOCUMENTS USED:\n",
      "================================================================================\n",
      "\n",
      "Document 1:\n",
      "  Source: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}\n",
      "  Content: Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 2:\n",
      "  Source: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}\n",
      "  Content: Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "sh...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 3:\n",
      "  Source: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}\n",
      "  Content: Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "sh...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 4:\n",
      "  Source: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "  Content: Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example Query 1: General question about the document\n",
    "query1 = \"What is the main topic or subject of this document?\"\n",
    "\n",
    "print(f\"Query: {query1}\")\n",
    "print(\"\\nProcessing...\\n\")\n",
    "\n",
    "# With LangChain 1.0+, we invoke the chain with the question directly\n",
    "answer = rag_chain.invoke(query1)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# To see which documents were retrieved, we can call the retriever separately\n",
    "print(\"\\nSOURCE DOCUMENTS USED:\")\n",
    "print(\"=\" * 80)\n",
    "retrieved_docs = retriever.invoke(query1)\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Source: {doc.metadata}\")\n",
    "    print(f\"  Content: {doc.page_content[:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Can you summarize the key points from this document?\n",
      "\n",
      "Processing...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "The document discusses the imperfections of the law and emphasizes that while the law itself may not be perfect, its application should be just. It also mentions that since 2009, a majority of American governments have passed laws that make the registration or voting process more difficult. Additionally, the document includes technical details about attention mechanisms in neural networks, specifically focusing on how attention heads in layer 5 of a 6-layer encoder model are involved in tasks like anaphora resolution and tracking long-distance dependencies in sentences. The document is part of a larger work related to the Transformer model, a network architecture that relies solely on attention mechanisms, eliminating the need for recurrence and convolutions in sequence transduction models.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example Query 2: Specific information extraction\n",
    "query2 = \"Can you summarize the key points from this document?\"\n",
    "\n",
    "print(f\"Query: {query2}\")\n",
    "print(\"\\nProcessing...\\n\")\n",
    "\n",
    "answer = rag_chain.invoke(query2)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please you Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What specific details are mentioned about attention mechanisms?\n",
      "\n",
      "Processing...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "The provided context mentions several specific details about attention mechanisms:\n",
      "\n",
      "1. **Self-attention**: This is an attention mechanism that relates different positions of a single sequence to compute a representation of the sequence. It has been used successfully in tasks like reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\n",
      "\n",
      "2. **Multi-Head Attention**: This is used to counteract the averaging of attention-weighted positions. It involves multiple attention heads which can attend to different parts of the sequence simultaneously, allowing for a richer representation.\n",
      "\n",
      "3. **End-to-end memory networks**: These use a recurrent attention mechanism instead of sequence-aligned recurrence and have shown good performance in simple-language question answering and language modeling tasks.\n",
      "\n",
      "4. **Transformer model**: This model relies entirely on self-attention to compute representations of its input and output, dispensing with recurrence and convolutions. This is a shift from traditional models that used complex recurrent or convolutional neural networks.\n",
      "\n",
      "These details highlight the versatility and effectiveness of attention mechanisms in various neural network architectures and tasks.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example Query 3: Your custom question\n",
    "# Replace this with your own question!\n",
    "custom_query = \"What specific details are mentioned about attention mechanisms?\"\n",
    "\n",
    "print(f\"Query: {custom_query}\")\n",
    "print(\"\\nProcessing...\\n\")\n",
    "\n",
    "answer = rag_chain.invoke(custom_query)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the applications of Attention Mechanism?\n",
      "\n",
      "Processing...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "The applications of the attention mechanism include reading comprehension, abstractive summarization, textual entailment, learning task-independent sentence representations, simple-language question answering, and language modeling tasks.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example Query 3: Your custom question\n",
    "# Replace this with your own question!\n",
    "custom_query = \"What are the applications of Attention Mechanism?\"\n",
    "\n",
    "print(f\"Query: {custom_query}\")\n",
    "print(\"\\nProcessing...\\n\")\n",
    "\n",
    "response3 = rag_chain.invoke(custom_query)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(response3)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS Kernel Crash ISSUE\n",
    "\n",
    "Testing retrieval with query: 'What is the main topic of this document?'\n",
    "OMP: Error #15: Initializing libomp.dylib, but found libomp.dylib already initialized.\n",
    "OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://openmp.llvm.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
